<?xml version="1.0"?>
<Benchmark xmlns:html="http://www.w3.org/1999/xhtml" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xhtml="http://www.w3.org/1999/xhtml" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" id="product-name" resolved="false" style="SCAP_1.1" xml:lang="en-US" xsi:schemaLocation="http://checklists.nist.gov/xccdf/1.1 xccdf-1.1.4.xsd">
  <status date="2019-11-28">draft</status>
  <title>Guide to the Secure Configuration of Red Hat OpenShift Container Platform 3</title>
  <description>This guide presents a catalog of security-relevant
configuration settings for Red Hat OpenShift Container Platform 3. It is a rendering of
content structured in the eXtensible Configuration Checklist Description Format (XCCDF)
in order to support security automation.  The SCAP content is
is available in the <tt>scap-security-guide</tt> package which is developed at

    <html:a href="https://www.open-scap.org/security-policies/scap-security-guide">https://www.open-scap.org/security-policies/scap-security-guide</html:a>.
<br/><br/>
Providing system administrators with such guidance informs them how to securely
configure systems under their control in a variety of network roles. Policy
makers and baseline creators can use this catalog of settings, with its
associated references to higher-level security control catalogs, in order to
assist them in security baseline creation. This guide is a <em>catalog, not a
checklist</em>, and satisfaction of every item is not likely to be possible or
sensible in many operational scenarios. However, the XCCDF format enables
granular selection and adjustment of settings, and their association with OVAL
and OCIL content provides an automated checking capability. Transformations of
this document, and its associated automated checking content, are capable of
providing baselines that meet a diverse set of policy objectives. Some example
XCCDF <em>Profiles</em>, which are selections of items that form checklists and
can be used as baselines, are available with this guide. They can be
processed, in an automated fashion, with tools that support the Security
Content Automation Protocol (SCAP). The NIST National Checklist Program (NCP),
which provides required settings for the United States Government, is one example
of a baseline created from this guidance.
</description>
  <notice id="terms_of_use">Do not attempt to implement any of the settings in
this guide without first testing them in a non-operational environment. The
creators of this guidance assume no responsibility whatsoever for its use by
other parties, and makes no guarantees, expressed or implied, about its
quality, reliability, or any other characteristic.
</notice>
  <front-matter>The ComplianceAsCode Project<br/>

    <html:a href="https://www.open-scap.org/security-policies/scap-security-guide">https://www.open-scap.org/security-policies/scap-security-guide</html:a>
</front-matter>
  <rear-matter>Red Hat and Red Hat Enterprise Linux are either registered
trademarks or trademarks of Red Hat, Inc. in the United States and other
countries. All other names are registered trademarks or trademarks of their
respective companies.
</rear-matter>
  <platform idref="cpe:/a:redhat:openshift_container_platform:3.10"/>
  <platform idref="cpe:/a:redhat:openshift_container_platform:3.11"/>
  <version>0.9</version>
  <metadata/>
  <Profile extends="opencis-node" id="opencis-master">
    <title override="true">Open Computing Information Security Profile for OpenShift Master Node</title>
    <description override="true">This baseline was inspired by the Center for Internet Security
(CIS) Kubernetes Benchmark, v1.2.0 - 01-31-2017.

For the ComplianceAsCode project to remain in compliance with
CIS' terms and conditions, specifically Restrictions(8), note
there is no representation or claim that the OpenCIS profile will
ensure a system is in compliance or consistency with the CIS
baseline.</description>
    <select idref="file_groupowner_etc_origin" selected="true"/>
    <select idref="file_groupowner_master_admin_conf" selected="true"/>
    <select idref="file_groupowner_master_api_server" selected="true"/>
    <select idref="file_groupowner_master_cni_conf" selected="true"/>
    <select idref="file_groupowner_master_controller_manager" selected="true"/>
    <select idref="file_groupowner_master_etcd" selected="true"/>
    <select idref="file_groupowner_master_openshift_conf" selected="true"/>
    <select idref="file_groupowner_master_openshift_kubeconfig" selected="true"/>
    <select idref="file_groupowner_master_scheduler_conf" selected="true"/>
    <select idref="file_groupowner_var_lib_etcd" selected="true"/>
    <select idref="file_owner_etc_origin" selected="true"/>
    <select idref="file_owner_master_admin_conf" selected="true"/>
    <select idref="file_owner_master_api_server" selected="true"/>
    <select idref="file_owner_master_cni_conf" selected="true"/>
    <select idref="file_owner_master_controller_manager" selected="true"/>
    <select idref="file_owner_master_etcd" selected="true"/>
    <select idref="file_owner_master_openshift_conf" selected="true"/>
    <select idref="file_owner_master_openshift_kubeconfig" selected="true"/>
    <select idref="file_owner_master_scheduler_conf" selected="true"/>
    <select idref="file_owner_var_lib_etcd" selected="true"/>
    <select idref="file_permissions_etc_origin" selected="true"/>
    <select idref="file_permissions_master_admin_conf" selected="true"/>
    <select idref="file_permissions_master_api_server" selected="true"/>
    <select idref="file_permissions_master_cni_conf" selected="true"/>
    <select idref="file_permissions_master_controller_manager" selected="true"/>
    <select idref="file_permissions_master_etcd" selected="true"/>
    <select idref="file_permissions_master_openshift_conf" selected="true"/>
    <select idref="file_permissions_master_openshift_kubeconfig" selected="true"/>
    <select idref="file_permissions_master_scheduler_conf" selected="true"/>
    <select idref="file_permissions_var_lib_etcd" selected="true"/>
    <select idref="file_groupowner_master_openvswitch" selected="true"/>
    <select idref="file_owner_master_openvswitch" selected="true"/>
    <select idref="file_permissions_master_openvswitch" selected="true"/>
    <select idref="scheduler_profiling_argument" selected="true"/>
    <select idref="controller_bind_address" selected="true"/>
    <select idref="controller_disable_profiling" selected="true"/>
    <select idref="controller_rotate_kubelet_server_certs" selected="true"/>
    <select idref="controller_terminated_pod_gc_threshhold" selected="true"/>
    <select idref="controller_use_service_account" selected="true"/>
    <select idref="etcd_auto_tls" selected="true"/>
    <select idref="etcd_cert_file" selected="true"/>
    <select idref="etcd_client_cert_auth" selected="true"/>
    <select idref="etcd_key_file" selected="true"/>
    <select idref="etcd_max_wals" selected="true"/>
    <select idref="etcd_peer_auto_tls" selected="true"/>
    <select idref="etcd_peer_cert_file" selected="true"/>
    <select idref="etcd_peer_client_cert_auth" selected="true"/>
    <select idref="etcd_peer_key_file" selected="true"/>
    <select idref="etcd_unique_ca" selected="true"/>
    <select idref="etcd_wal_dir" selected="true"/>
    <select idref="api_server_admission_control_plugin_AlwaysAdmit" selected="true"/>
    <select idref="api_server_admission_control_plugin_AlwaysPullImages" selected="true"/>
    <select idref="api_server_admission_control_plugin_DenyEscalatingExec" selected="true"/>
    <select idref="api_server_admission_control_plugin_EventRateLimit" selected="true"/>
    <select idref="api_server_admission_control_plugin_NamespaceLifecycle" selected="true"/>
    <select idref="api_server_admission_control_plugin_NodeRestriction" selected="true"/>
    <select idref="api_server_admission_control_plugin_PodSecurityPolicy" selected="true"/>
    <select idref="api_server_admission_control_plugin_SecurityContextDeny" selected="true"/>
    <select idref="api_server_admission_control_plugin_ServiceAccount" selected="true"/>
    <select idref="api_server_advanced_auditing" selected="true"/>
    <select idref="api_server_anonymous_auth" selected="true"/>
    <select idref="api_server_audit_log_maxage" selected="true"/>
    <select idref="api_server_audit_log_maxbackup" selected="true"/>
    <select idref="api_server_audit_log_maxsize" selected="true"/>
    <select idref="api_server_audit_log_path" selected="true"/>
    <select idref="api_server_authorization_mode" selected="true"/>
    <select idref="api_server_basic_auth" selected="true"/>
    <select idref="api_server_client_ca" selected="true"/>
    <select idref="api_server_etcd_ca" selected="true"/>
    <select idref="api_server_etcd_cert" selected="true"/>
    <select idref="api_server_etcd_key" selected="true"/>
    <select idref="api_server_experimental_encryption_provider_cipher" selected="true"/>
    <select idref="api_server_experimental_encryption_provider_config" selected="true"/>
    <select idref="api_server_insecure_allow_any_token" selected="true"/>
    <select idref="api_server_insecure_bind_address" selected="true"/>
    <select idref="api_server_insecure_port" selected="true"/>
    <select idref="api_server_kubelet_certificate_authority" selected="true"/>
    <select idref="api_server_kubelet_client_cert" selected="true"/>
    <select idref="api_server_kubelet_client_key" selected="true"/>
    <select idref="api_server_kubelet_https" selected="true"/>
    <select idref="api_server_request_timeout" selected="true"/>
    <select idref="api_server_secure_port" selected="true"/>
    <select idref="api_server_service_account_ca" selected="true"/>
    <select idref="api_server_service_account_private_key" selected="true"/>
    <select idref="api_server_service_account_public_key" selected="true"/>
    <select idref="api_server_tls_cert" selected="true"/>
    <select idref="api_server_tls_cipher_suites" selected="true"/>
    <select idref="api_server_tls_private_key" selected="true"/>
    <select idref="api_server_token_auth" selected="true"/>
  </Profile>
  <Profile id="opencis-node">
    <title override="true">Open Computing Information Security Profile for OpenShift Node</title>
    <description override="true">This baseline was inspired by the Center for Internet Security
(CIS) Kubernetes Benchmark, v1.2.0 - 01-31-2017.

For the ComplianceAsCode project to remain in compliance with
CIS' terms and conditions, specifically Restrictions(8), note
there is no representation or claim that the OpenCIS profile will
ensure a system is in compliance or consistency with the CIS
baseline.</description>
    <select idref="file_permissions_node_config" selected="true"/>
    <select idref="file_owner_node_config" selected="true"/>
    <select idref="file_groupowner_node_config" selected="true"/>
    <select idref="file_permissions_openshift_node_client_crt" selected="true"/>
    <select idref="file_owner_openshift_node_client_crt" selected="true"/>
    <select idref="file_groupowner_openshift_node_client_crt" selected="true"/>
    <select idref="file_permissions_openshift_node_service" selected="true"/>
    <select idref="file_owner_openshift_node_service" selected="true"/>
    <select idref="file_groupowner_openshift_node_service" selected="true"/>
    <select idref="kubelet_configure_client_ca" selected="true"/>
    <select idref="kubelet_configure_event_creation" selected="true"/>
    <select idref="kubelet_configure_tls_cert" selected="true"/>
    <select idref="kubelet_configure_tls_key" selected="true"/>
    <select idref="kubelet_disable_cadvisor_port" selected="true"/>
    <select idref="kubelet_disable_hostname_override" selected="true"/>
    <select idref="kubelet_disable_readonly_port" selected="true"/>
    <select idref="kubelet_enable_client_cert_rotation" selected="true"/>
    <select idref="kubelet_enable_server_cert_rotation" selected="true"/>
    <select idref="kubelet_enable_iptables_util_chains" selected="true"/>
    <select idref="kubelet_enable_streaming_connections" selected="true"/>
  </Profile>
  <Value id="conditional_clause" type="string">
    <title>A conditional clause for check statements.</title>
    <description>A conditional clause for check statements.</description>
    <value selector="">This is a placeholder</value>
  </Value>
  <Group id="remediation_functions">
    <title>Remediation functions used by the SCAP Security Guide Project</title>
    <description>XCCDF form of the various remediation functions as used by remediation scripts from the SCAP Security Guide Project.</description>
    <Value hidden="true" id="function_create_audit_remediation_unsuccessful_file_modification_detailed" interactive="0" operator="equals" prohibitChanges="true" type="string">
      <title>Remediation function create_audit_remediation_unsuccessful_file_modification_detailed</title>
      <description>Shared bash remediation function. Not intended to be changed by tailoring.</description>
      <value selector="">function create_audit_remediation_unsuccessful_file_modification_detailed {
	mkdir -p "$(dirname "$1")"
	# The - option to mark a here document limit string (&lt;&lt;-EOF) suppresses leading tabs (but not spaces) in the output.
	cat &lt;&lt;-EOF &gt; "$1"
		## This content is a section of an Audit config snapshot recommended for RHEL8 sytems that target OSPP compliance.
		## The following content has been retreived on 2019-03-11 from: https://github.com/linux-audit/audit-userspace/blob/master/rules/30-ospp-v42.rules

		## The purpose of these rules is to meet the requirements for Operating
		## System Protection Profile (OSPP)v4.2. These rules depends on having
		## 10-base-config.rules, 11-loginuid.rules, and 43-module-load.rules installed.

		## Unsuccessful file creation (open with O_CREAT)
		-a always,exit -F arch=b32 -S openat,open_by_handle_at -F a2&amp;0100 -F exit=-EACCES -F auid&gt;=1000 -F auid!=unset -F key=unsuccesful-create
		-a always,exit -F arch=b64 -S openat,open_by_handle_at -F a2&amp;0100 -F exit=-EACCES -F auid&gt;=1000 -F auid!=unset -F key=unsuccesful-create
		-a always,exit -F arch=b32 -S open -F a1&amp;0100 -F exit=-EACCES -F auid&gt;=1000 -F auid!=unset -F key=unsuccesful-create
		-a always,exit -F arch=b64 -S open -F a1&amp;0100 -F exit=-EACCES -F auid&gt;=1000 -F auid!=unset -F key=unsuccesful-create
		-a always,exit -F arch=b32 -S openat,open_by_handle_at -F a2&amp;0100 -F exit=-EPERM -F auid&gt;=1000 -F auid!=unset -F key=unsuccesful-create
		-a always,exit -F arch=b64 -S openat,open_by_handle_at -F a2&amp;0100 -F exit=-EPERM -F auid&gt;=1000 -F auid!=unset -F key=unsuccesful-create
		-a always,exit -F arch=b32 -S open -F a1&amp;0100 -F exit=-EPERM -F auid&gt;=1000 -F auid!=unset -F key=unsuccesful-create
		-a always,exit -F arch=b64 -S open -F a1&amp;0100 -F exit=-EPERM -F auid&gt;=1000 -F auid!=unset -F key=unsuccesful-create
		-a always,exit -F arch=b32 -S creat -F exit=-EACCES -F auid&gt;=1000 -F auid!=unset -F key=unsuccesful-create
		-a always,exit -F arch=b64 -S creat -F exit=-EACCES -F auid&gt;=1000 -F auid!=unset -F key=unsuccesful-create
		-a always,exit -F arch=b32 -S creat -F exit=-EPERM -F auid&gt;=1000 -F auid!=unset -F key=unsuccesful-create
		-a always,exit -F arch=b64 -S creat -F exit=-EPERM -F auid&gt;=1000 -F auid!=unset -F key=unsuccesful-create

		## Unsuccessful file modifications (open for write or truncate)
		-a always,exit -F arch=b32 -S openat,open_by_handle_at -F a2&amp;01003 -F exit=-EACCES -F auid&gt;=1000 -F auid!=unset -F key=unsuccesful-modification
		-a always,exit -F arch=b64 -S openat,open_by_handle_at -F a2&amp;01003 -F exit=-EACCES -F auid&gt;=1000 -F auid!=unset -F key=unsuccesful-modification
		-a always,exit -F arch=b32 -S open -F a1&amp;01003 -F exit=-EACCES -F auid&gt;=1000 -F auid!=unset -F key=unsuccesful-modification
		-a always,exit -F arch=b64 -S open -F a1&amp;01003 -F exit=-EACCES -F auid&gt;=1000 -F auid!=unset -F key=unsuccesful-modification
		-a always,exit -F arch=b32 -S openat,open_by_handle_at -F a2&amp;01003 -F exit=-EPERM -F auid&gt;=1000 -F auid!=unset -F key=unsuccesful-modification
		-a always,exit -F arch=b64 -S openat,open_by_handle_at -F a2&amp;01003 -F exit=-EPERM -F auid&gt;=1000 -F auid!=unset -F key=unsuccesful-modification
		-a always,exit -F arch=b32 -S open -F a1&amp;01003 -F exit=-EPERM -F auid&gt;=1000 -F auid!=unset -F key=unsuccesful-modification
		-a always,exit -F arch=b64 -S open -F a1&amp;01003 -F exit=-EPERM -F auid&gt;=1000 -F auid!=unset -F key=unsuccesful-modification
		-a always,exit -F arch=b32 -S truncate,ftruncate -F exit=-EACCES -F auid&gt;=1000 -F auid!=unset -F key=unsuccesful-modification
		-a always,exit -F arch=b64 -S truncate,ftruncate -F exit=-EACCES -F auid&gt;=1000 -F auid!=unset -F key=unsuccesful-modification
		-a always,exit -F arch=b32 -S truncate,ftruncate -F exit=-EPERM -F auid&gt;=1000 -F auid!=unset -F key=unsuccesful-modification
		-a always,exit -F arch=b64 -S truncate,ftruncate -F exit=-EPERM -F auid&gt;=1000 -F auid!=unset -F key=unsuccesful-modification

		## Unsuccessful file access (any other opens) This has to go last.
		-a always,exit -F arch=b32 -S open,creat,truncate,ftruncate,openat,open_by_handle_at -F exit=-EACCES -F auid&gt;=1000 -F auid!=unset -F key=unsuccesful-access
		-a always,exit -F arch=b64 -S open,creat,truncate,ftruncate,openat,open_by_handle_at -F exit=-EACCES -F auid&gt;=1000 -F auid!=unset -F key=unsuccesful-access
		-a always,exit -F arch=b32 -S open,creat,truncate,ftruncate,openat,open_by_handle_at -F exit=-EPERM -F auid&gt;=1000 -F auid!=unset -F key=unsuccesful-access
		-a always,exit -F arch=b64 -S open,creat,truncate,ftruncate,openat,open_by_handle_at -F exit=-EPERM -F auid&gt;=1000 -F auid!=unset -F key=unsuccesful-access
	EOF
}</value>
    </Value>
    <Value hidden="true" id="function_die" interactive="0" operator="equals" prohibitChanges="true" type="string">
      <title>Remediation function die</title>
      <description>Shared bash remediation function. Not intended to be changed by tailoring.</description>
      <value selector=""># Print a message to stderr and exit the shell
# $1: The message to print.
# $2: The error code (optional, default is 1)
function die {
	local _message="$1" _rc="${2:-1}"
	printf '%s\n' "$_message" &gt;&amp;2
	exit "$_rc"
}</value>
    </Value>
    <Value hidden="true" id="function_ensure_there_are_servers_in_ntp_compatible_config_file" interactive="0" operator="equals" prohibitChanges="true" type="string">
      <title>Remediation function ensure_there_are_servers_in_ntp_compatible_config_file</title>
      <description>Shared bash remediation function. Not intended to be changed by tailoring.</description>
      <value selector=""># Function ensures that the ntp/chrony config file contains valid server entries
# $1: Path to the config file
# $2: Comma-separated list of servers
function ensure_there_are_servers_in_ntp_compatible_config_file {
	# If invoked with no arguments, exit. This is an intentional behavior.
	[ $# -gt 1 ] || return 0
	[ $# = 2 ] || die "$0 requires zero or exactly two arguments"
	local _config_file="$1" _servers_list="$2"
	if ! grep -q '#[[:space:]]*server' "$_config_file"; then
		for server in $(echo "$_servers_list" | tr ',' '\n') ; do
			printf '\nserver %s iburst' "$server" &gt;&gt; "$_config_file"
		done
	else
		sed -i 's/#[ \t]*server/server/g' "$_config_file"
	fi
}</value>
    </Value>
    <Value hidden="true" id="function_fix_audit_syscall_rule" interactive="0" operator="equals" prohibitChanges="true" type="string">
      <title>Remediation function fix_audit_syscall_rule</title>
      <description>Shared bash remediation function. Not intended to be changed by tailoring.</description>
      <value selector=""># Function to fix syscall audit rule for given system call. It is
# based on example audit syscall rule definitions as outlined in
# /usr/share/doc/audit-2.3.7/stig.rules file provided with the audit
# package. It will combine multiple system calls belonging to the same
# syscall group into one audit rule (rather than to create audit rule per
# different system call) to avoid audit infrastructure performance penalty
# in the case of 'one-audit-rule-definition-per-one-system-call'. See:
#
#   https://www.redhat.com/archives/linux-audit/2014-November/msg00009.html
#
# for further details.
#
# Expects five arguments (each of them is required) in the form of:
# * audit tool				tool used to load audit rules,
# 					either 'auditctl', or 'augenrules
# * audit rules' pattern		audit rule skeleton for same syscall
# * syscall group			greatest common string this rule shares
# 					with other rules from the same group
# * architecture			architecture this rule is intended for
# * full form of new rule to add	expected full form of audit rule as to be
# 					added into audit.rules file
#
# Note: The 2-th up to 4-th arguments are used to determine how many existing
# audit rules will be inspected for resemblance with the new audit rule
# (5-th argument) the function is going to add. The rule's similarity check
# is performed to optimize audit.rules definition (merge syscalls of the same
# group into one rule) to avoid the "single-syscall-per-audit-rule" performance
# penalty.
#
# Example call:
#
#	See e.g. 'audit_rules_file_deletion_events.sh' remediation script
#
function fix_audit_syscall_rule {

# Load function arguments into local variables
local tool="$1"
local pattern="$2"
local group="$3"
local arch="$4"
local full_rule="$5"

# Check sanity of the input
if [ $# -ne "5" ]
then
	echo "Usage: fix_audit_syscall_rule 'tool' 'pattern' 'group' 'arch' 'full rule'"
	echo "Aborting."
	exit 1
fi

# Create a list of audit *.rules files that should be inspected for presence and correctness
# of a particular audit rule. The scheme is as follows:
# 
# -----------------------------------------------------------------------------------------
#  Tool used to load audit rules | Rule already defined  |  Audit rules file to inspect    |
# -----------------------------------------------------------------------------------------
#        auditctl                |     Doesn't matter    |  /etc/audit/audit.rules         |
# -----------------------------------------------------------------------------------------
#        augenrules              |          Yes          |  /etc/audit/rules.d/*.rules     |
#        augenrules              |          No           |  /etc/audit/rules.d/$key.rules  |
# -----------------------------------------------------------------------------------------
#
declare -a files_to_inspect

retval=0

# First check sanity of the specified audit tool
if [ "$tool" != 'auditctl' ] &amp;&amp; [ "$tool" != 'augenrules' ]
then
	echo "Unknown audit rules loading tool: $1. Aborting."
	echo "Use either 'auditctl' or 'augenrules'!"
	return 1
# If audit tool is 'auditctl', then add '/etc/audit/audit.rules'
# file to the list of files to be inspected
elif [ "$tool" == 'auditctl' ]
then
	files_to_inspect+=('/etc/audit/audit.rules' )
# If audit tool is 'augenrules', then check if the audit rule is defined
# If rule is defined, add '/etc/audit/rules.d/*.rules' to the list for inspection
# If rule isn't defined yet, add '/etc/audit/rules.d/$key.rules' to the list for inspection
elif [ "$tool" == 'augenrules' ]
then
	# Extract audit $key from audit rule so we can use it later
	key=$(expr "$full_rule" : '.*-k[[:space:]]\([^[:space:]]\+\)' '|' "$full_rule" : '.*-F[[:space:]]key=\([^[:space:]]\+\)')
	readarray -t matches &lt; &lt;(sed -s -n -e "\;${pattern};!d" -e "/${arch}/!d" -e "/${group}/!d;F" /etc/audit/rules.d/*.rules)
	if [ $? -ne 0 ]
	then
		retval=1
	fi
	for match in "${matches[@]}"
	do
		files_to_inspect+=("${match}")
	done
	# Case when particular rule isn't defined in /etc/audit/rules.d/*.rules yet
	if [ ${#files_to_inspect[@]} -eq "0" ]
	then
		file_to_inspect="/etc/audit/rules.d/$key.rules"
		files_to_inspect=("$file_to_inspect")
		if [ ! -e "$file_to_inspect" ]
		then
			touch "$file_to_inspect"
			chmod 0640 "$file_to_inspect"
		fi
	fi
fi

#
# Indicator that we want to append $full_rule into $audit_file by default
local append_expected_rule=0

for audit_file in "${files_to_inspect[@]}"
do
	# Filter existing $audit_file rules' definitions to select those that:
	# * follow the rule pattern, and
	# * meet the hardware architecture requirement, and
	# * are current syscall group specific
	readarray -t existing_rules &lt; &lt;(sed -e "\;${pattern};!d" -e "/${arch}/!d" -e "/${group}/!d"  "$audit_file")
	if [ $? -ne 0 ]
	then
		retval=1
	fi

	# Process rules found case-by-case
	for rule in "${existing_rules[@]}"
	do
		# Found rule is for same arch &amp; key, but differs (e.g. in count of -S arguments)
		if [ "${rule}" != "${full_rule}" ]
		then
			# If so, isolate just '(-S \w)+' substring of that rule
			rule_syscalls=$(echo $rule | grep -o -P '(-S \w+ )+')
			# Check if list of '-S syscall' arguments of that rule is subset
			# of '-S syscall' list of expected $full_rule
			if grep -q -- "$rule_syscalls" &lt;&lt;&lt; "$full_rule"
			then
				# Rule is covered (i.e. the list of -S syscalls for this rule is
				# subset of -S syscalls of $full_rule =&gt; existing rule can be deleted
				# Thus delete the rule from audit.rules &amp; our array
				sed -i -e "\;${rule};d" "$audit_file"
				if [ $? -ne 0 ]
				then
					retval=1
				fi
				existing_rules=("${existing_rules[@]//$rule/}")
			else
				# Rule isn't covered by $full_rule - it besides -S syscall arguments
				# for this group contains also -S syscall arguments for other syscall
				# group. Example: '-S lchown -S fchmod -S fchownat' =&gt; group='chown'
				# since 'lchown' &amp; 'fchownat' share 'chown' substring
				# Therefore:
				# * 1) delete the original rule from audit.rules
				# (original '-S lchown -S fchmod -S fchownat' rule would be deleted)
				# * 2) delete the -S syscall arguments for this syscall group, but
				# keep those not belonging to this syscall group
				# (original '-S lchown -S fchmod -S fchownat' would become '-S fchmod'
				# * 3) append the modified (filtered) rule again into audit.rules
				# if the same rule not already present
				#
				# 1) Delete the original rule
				sed -i -e "\;${rule};d" "$audit_file"
				if [ $? -ne 0 ]
				then
					retval=1
				fi

				# 2) Delete syscalls for this group, but keep those from other groups
				# Convert current rule syscall's string into array splitting by '-S' delimiter
				IFS_BKP="$IFS"
				IFS=$'-S'
				read -a rule_syscalls_as_array &lt;&lt;&lt; "$rule_syscalls"
				# Reset IFS back to default
				IFS="$IFS_BKP"
				# Splitting by "-S" can't be replaced by the readarray functionality easily

				# Declare new empty string to hold '-S syscall' arguments from other groups
				new_syscalls_for_rule=''
				# Walk through existing '-S syscall' arguments
				for syscall_arg in "${rule_syscalls_as_array[@]}"
				do
					# Skip empty $syscall_arg values
					if [ "$syscall_arg" == '' ]
					then
						continue
					fi
					# If the '-S syscall' doesn't belong to current group add it to the new list
					# (together with adding '-S' delimiter back for each of such item found)
					if grep -q -v -- "$group" &lt;&lt;&lt; "$syscall_arg"
					then
						new_syscalls_for_rule="$new_syscalls_for_rule -S $syscall_arg"
					fi
				done
				# Replace original '-S syscall' list with the new one for this rule
				updated_rule=${rule//$rule_syscalls/$new_syscalls_for_rule}
				# Squeeze repeated whitespace characters in rule definition (if any) into one
				updated_rule=$(echo "$updated_rule" | tr -s '[:space:]')
				# 3) Append the modified / filtered rule again into audit.rules
				#    (but only in case it's not present yet to prevent duplicate definitions)
				if ! grep -q -- "$updated_rule" "$audit_file"
				then
					echo "$updated_rule" &gt;&gt; "$audit_file"
				fi
			fi
		else
			# $audit_file already contains the expected rule form for this
			# architecture &amp; key =&gt; don't insert it second time
			append_expected_rule=1
		fi
	done

	# We deleted all rules that were subset of the expected one for this arch &amp; key.
	# Also isolated rules containing system calls not from this system calls group.
	# Now append the expected rule if it's not present in $audit_file yet
	if [[ ${append_expected_rule} -eq "0" ]]
	then
		echo "$full_rule" &gt;&gt; "$audit_file"
	fi
done

return $retval

}</value>
    </Value>
    <Value hidden="true" id="function_fix_audit_watch_rule" interactive="0" operator="equals" prohibitChanges="true" type="string">
      <title>Remediation function fix_audit_watch_rule</title>
      <description>Shared bash remediation function. Not intended to be changed by tailoring.</description>
      <value selector=""># Function to fix audit file system object watch rule for given path:
# * if rule exists, also verifies the -w bits match the requirements
# * if rule doesn't exist yet, appends expected rule form to $files_to_inspect
#   audit rules file, depending on the tool which was used to load audit rules
#
# Expects four arguments (each of them is required) in the form of:
# * audit tool				tool used to load audit rules,
# 					either 'auditctl', or 'augenrules'
# * path                        	value of -w audit rule's argument
# * required access bits        	value of -p audit rule's argument
# * key                         	value of -k audit rule's argument
#
# Example call:
#
#       fix_audit_watch_rule "auditctl" "/etc/localtime" "wa" "audit_time_rules"
#
function fix_audit_watch_rule {

# Load function arguments into local variables
local tool="$1"
local path="$2"
local required_access_bits="$3"
local key="$4"

# Check sanity of the input
if [ $# -ne "4" ]
then
	echo "Usage: fix_audit_watch_rule 'tool' 'path' 'bits' 'key'"
	echo "Aborting."
	exit 1
fi

# Create a list of audit *.rules files that should be inspected for presence and correctness
# of a particular audit rule. The scheme is as follows:
#
# -----------------------------------------------------------------------------------------
# Tool used to load audit rules	| Rule already defined	|  Audit rules file to inspect	  |
# -----------------------------------------------------------------------------------------
#	auditctl		|     Doesn't matter	|  /etc/audit/audit.rules	  |
# -----------------------------------------------------------------------------------------
# 	augenrules		|          Yes		|  /etc/audit/rules.d/*.rules	  |
# 	augenrules		|          No		|  /etc/audit/rules.d/$key.rules  |
# -----------------------------------------------------------------------------------------
declare -a files_to_inspect
files_to_inspect=()

# Check sanity of the specified audit tool
if [ "$tool" != 'auditctl' ] &amp;&amp; [ "$tool" != 'augenrules' ]
then
	echo "Unknown audit rules loading tool: $1. Aborting."
	echo "Use either 'auditctl' or 'augenrules'!"
	exit 1
# If the audit tool is 'auditctl', then add '/etc/audit/audit.rules'
# into the list of files to be inspected
elif [ "$tool" == 'auditctl' ]
then
	files_to_inspect+=('/etc/audit/audit.rules')
# If the audit is 'augenrules', then check if rule is already defined
# If rule is defined, add '/etc/audit/rules.d/*.rules' to list of files for inspection.
# If rule isn't defined, add '/etc/audit/rules.d/$key.rules' to list of files for inspection.
elif [ "$tool" == 'augenrules' ]
then
	readarray -t matches &lt; &lt;(grep -P "[\s]*-w[\s]+$path" /etc/audit/rules.d/*.rules)

	# For each of the matched entries
	for match in "${matches[@]}"
	do
		# Extract filepath from the match
		rulesd_audit_file=$(echo $match | cut -f1 -d ':')
		# Append that path into list of files for inspection
		files_to_inspect+=("$rulesd_audit_file")
	done
	# Case when particular audit rule isn't defined yet
	if [ "${#files_to_inspect[@]}" -eq "0" ]
	then
		# Append '/etc/audit/rules.d/$key.rules' into list of files for inspection
		local key_rule_file="/etc/audit/rules.d/$key.rules"
		# If the $key.rules file doesn't exist yet, create it with correct permissions
		if [ ! -e "$key_rule_file" ]
		then
			touch "$key_rule_file"
			chmod 0640 "$key_rule_file"
		fi

		files_to_inspect+=("$key_rule_file")
	fi
fi

# Finally perform the inspection and possible subsequent audit rule
# correction for each of the files previously identified for inspection
for audit_rules_file in "${files_to_inspect[@]}"
do

	# Check if audit watch file system object rule for given path already present
	if grep -q -P -- "[\s]*-w[\s]+$path" "$audit_rules_file"
	then
		# Rule is found =&gt; verify yet if existing rule definition contains
		# all of the required access type bits

		# Escape slashes in path for use in sed pattern below
		local esc_path=${path//$'/'/$'\/'}
		# Define BRE whitespace class shortcut
		local sp="[[:space:]]"
		# Extract current permission access types (e.g. -p [r|w|x|a] values) from audit rule
		current_access_bits=$(sed -ne "s/$sp*-w$sp\+$esc_path$sp\+-p$sp\+\([rxwa]\{1,4\}\).*/\1/p" "$audit_rules_file")
		# Split required access bits string into characters array
		# (to check bit's presence for one bit at a time)
		for access_bit in $(echo "$required_access_bits" | grep -o .)
		do
			# For each from the required access bits (e.g. 'w', 'a') check
			# if they are already present in current access bits for rule.
			# If not, append that bit at the end
			if ! grep -q "$access_bit" &lt;&lt;&lt; "$current_access_bits"
			then
				# Concatenate the existing mask with the missing bit
				current_access_bits="$current_access_bits$access_bit"
			fi
		done
		# Propagate the updated rule's access bits (original + the required
		# ones) back into the /etc/audit/audit.rules file for that rule
		sed -i "s/\($sp*-w$sp\+$esc_path$sp\+-p$sp\+\)\([rxwa]\{1,4\}\)\(.*\)/\1$current_access_bits\3/" "$audit_rules_file"
	else
		# Rule isn't present yet. Append it at the end of $audit_rules_file file
		# with proper key

		echo "-w $path -p $required_access_bits -k $key" &gt;&gt; "$audit_rules_file"
	fi
done
}</value>
    </Value>
    <Value hidden="true" id="function_include_merge_files_by_lines" interactive="0" operator="equals" prohibitChanges="true" type="string">
      <title>Remediation function include_merge_files_by_lines</title>
      <description>Shared bash remediation function. Not intended to be changed by tailoring.</description>
      <value selector="">function include_merge_files_by_lines {
	:
}

# 1: Filename of the "master" file
# 2: Filename of the newly created file
function create_empty_file_like {
	local lines_count
	lines_count=$(cat "$1" | wc -l)
	for _ in $(seq 1 "$lines_count"); do
		printf '\n' &gt;&gt; "$2"
	done
}


# 1: Filename of the "master" file
# 2: Filename of sample flie
function second_file_is_same_except_newlines {
	local lines_of_master lines_of_sample len_of_master line_number i
	readarray -t lines_of_master &lt; "$1"
	readarray -t lines_of_sample &lt; "$2"

	len_of_master="${#lines_of_master[@]}"
	if test "$len_of_master" != "${#lines_of_sample[@]}"; then
		echo "Files '$1' and '$2' have different number of lines, $len_of_master and ${#lines_of_sample[@]} respectively."
		return 1
	fi

	for line_number in $(seq 1 "$len_of_master"); do
		i=$((line_number - 1))
		test -n "${lines_of_sample[$i]}" || continue
		if test "${lines_of_master[$i]}" != "${lines_of_sample[$i]}"; then
			echo "Line $line_number is different in files '$1' and '$2'."
			return 1
		fi
	done
}


# 1: Filename of the "master" file
# 2: Filename of sample flie
# 3: List of indices (1-based, space-separated string)
function merge_first_lines_to_second_on_indices {
	local lines_of_master lines_of_sample line_number i
	test -f "$2" || create_empty_file_like "$1" "$2"

	readarray -t lines_of_master &lt; "$1"
	readarray -t lines_of_sample &lt; "$2"

	error_msg="$(second_file_is_same_except_newlines "$1" "$2")"
	if test $? != 0; then
		echo "Error merging lines into '$2': $error_msg" &gt;&amp;2
		return 1
	fi

	for line_number in $3; do
		i=$((line_number - 1))
		lines_of_sample[$i]="${lines_of_master[$i]}"
	done

	printf "%s\n" "${lines_of_sample[@]}" &gt; "$2"
}</value>
    </Value>
    <Value hidden="true" id="function_include_mount_options_functions" interactive="0" operator="equals" prohibitChanges="true" type="string">
      <title>Remediation function include_mount_options_functions</title>
      <description>Shared bash remediation function. Not intended to be changed by tailoring.</description>
      <value selector="">function include_mount_options_functions {
	:
}

# $1: type of filesystem
# $2: new mount point option
# $3: filesystem of new mount point (used when adding new entry in fstab)
# $4: mount type of new mount point (used when adding new entry in fstab)
function ensure_mount_option_for_vfstype {
        local _vfstype="$1" _new_opt="$2" _filesystem=$3 _type=$4 _vfstype_points=()
        readarray -t _vfstype_points &lt; &lt;(grep -E "[[:space:]]${_vfstype}[[:space:]]" /etc/fstab | awk '{print $2}')

        for _vfstype_point in "${_vfstype_points[@]}"
        do
                ensure_mount_option_in_fstab "$_vfstype_point" "$_new_opt" "$_filesystem" "$_type"
        done
}

# $1: mount point
# $2: new mount point option
# $3: device or virtual string (used when adding new entry in fstab)
# $4: mount type of mount point (used when adding new entry in fstab)
function ensure_mount_option_in_fstab {
	local _mount_point="$1" _new_opt="$2" _device=$3 _type=$4
	local _mount_point_match_regexp="" _previous_mount_opts=""
	_mount_point_match_regexp="$(get_mount_point_regexp "$_mount_point")"

	if [ "$(grep -c "$_mount_point_match_regexp" /etc/fstab)" -eq 0 ]; then
		# runtime opts without some automatic kernel/userspace-added defaults
		_previous_mount_opts=$(grep "$_mount_point_match_regexp" /etc/mtab | head -1 |  awk '{print $4}' \
					| sed -E "s/(rw|defaults|seclabel|${_new_opt})(,|$)//g;s/,$//")
		[ "$_previous_mount_opts" ] &amp;&amp; _previous_mount_opts+=","
		echo "${_device} ${_mount_point} ${_type} defaults,${_previous_mount_opts}${_new_opt} 0 0" &gt;&gt; /etc/fstab
	elif [ "$(grep "$_mount_point_match_regexp" /etc/fstab | grep -c "$_new_opt")" -eq 0 ]; then
		_previous_mount_opts=$(grep "$_mount_point_match_regexp" /etc/fstab | awk '{print $4}')
		sed -i "s|\(${_mount_point_match_regexp}.*${_previous_mount_opts}\)|\1,${_new_opt}|" /etc/fstab
	fi
}

# $1: mount point
function get_mount_point_regexp {
		printf "[[:space:]]%s[[:space:]]" "$1"
}

# $1: mount point
function assert_mount_point_in_fstab {
	local _mount_point_match_regexp
	_mount_point_match_regexp="$(get_mount_point_regexp "$1")"
	grep "$_mount_point_match_regexp" -q /etc/fstab \
		|| { echo "The mount point '$1' is not even in /etc/fstab, so we can't set up mount options" &gt;&amp;2; return 1; }
}

# $1: mount point
function remove_defaults_from_fstab_if_overriden {
	local _mount_point_match_regexp
	_mount_point_match_regexp="$(get_mount_point_regexp "$1")"
	if grep "$_mount_point_match_regexp" /etc/fstab | grep -q "defaults,"
	then
		sed -i "s|\(${_mount_point_match_regexp}.*\)defaults,|\1|" /etc/fstab
	fi
}

# $1: mount point
function ensure_partition_is_mounted {
	local _mount_point="$1"
	mkdir -p "$_mount_point" || return 1
	if mountpoint -q "$_mount_point"; then
		mount -o remount --target "$_mount_point"
	else
		mount --target "$_mount_point"
	fi
}</value>
    </Value>
    <Value hidden="true" id="function_include_set_faillock_option" interactive="0" operator="equals" prohibitChanges="true" type="string">
      <title>Remediation function include_set_faillock_option</title>
      <description>Shared bash remediation function. Not intended to be changed by tailoring.</description>
      <value selector="">function include_set_faillock_option {
	:
}

function insert_preauth {
	local pam_file="$1"
	local option="$2"
	local value="$3"
	# is auth required pam_faillock.so preauth present?
	if grep -qE "^\s*auth\s+required\s+pam_faillock\.so\s+preauth.*$" "$pam_file" ; then
		# is the option set?
		if grep -qE "^\s*auth\s+required\s+pam_faillock\.so\s+preauth.*$option=([0-9]*).*$" "$pam_file" ; then
			# just change the value of option to a correct value
			sed -i --follow-symlinks "s/\(^auth.*required.*pam_faillock.so.*preauth.*silent.*\)\($option *= *\).*/\1\2$value/" "$pam_file"
		# the option is not set.
		else
			# append the option
			sed -i --follow-symlinks "/^auth.*required.*pam_faillock.so.*preauth.*silent.*/ s/$/ $option=$value/" "$pam_file"
		fi
	# auth required pam_faillock.so preauth is not present, insert the whole line
	else
		sed -i --follow-symlinks "/^auth.*sufficient.*pam_unix.so.*/i auth        required      pam_faillock.so preauth silent $option=$value" "$pam_file"
	fi
}

function insert_authfail {
	local pam_file="$1"
	local option="$2"
	local value="$3"
	# is auth default pam_faillock.so authfail present?
	if grep -qE "^\s*auth\s+(\[default=die\])\s+pam_faillock\.so\s+authfail.*$" "$pam_file" ; then
		# is the option set?
		if grep -qE "^\s*auth\s+(\[default=die\])\s+pam_faillock\.so\s+authfail.*$option=([0-9]*).*$" "$pam_file" ; then
			# just change the value of option to a correct value
			sed -i --follow-symlinks "s/\(^auth.*[default=die].*pam_faillock.so.*authfail.*\)\($option *= *\).*/\1\2$value/" "$pam_file"
		# the option is not set.
		else
			# append the option
			sed -i --follow-symlinks "/^auth.*[default=die].*pam_faillock.so.*authfail.*/ s/$/ $option=$value/" "$pam_file"
		fi
	# auth default pam_faillock.so authfail is not present, insert the whole line
	else
		sed -i --follow-symlinks "/^auth.*sufficient.*pam_unix.so.*/a auth        [default=die] pam_faillock.so authfail $option=$value" "$pam_file"
	fi
}

function insert_account {
	local pam_file="$1"
	if ! grep -qE "^\s*account\s+required\s+pam_faillock\.so.*$" "$pam_file" ; then
		sed -E -i --follow-symlinks "/^\s*account\s*required\s*pam_unix.so/i account     required      pam_faillock.so" "$pam_file"
	fi
}

function set_faillock_option {
	local pam_file="$1"
	local option="$2"
	local value="$3"
	insert_preauth "$pam_file" "$option" "$value"
	insert_authfail "$pam_file" "$option" "$value"
	insert_account "$pam_file"
}</value>
    </Value>
    <Value hidden="true" id="function_perform_audit_adjtimex_settimeofday_stime_remediation" interactive="0" operator="equals" prohibitChanges="true" type="string">
      <title>Remediation function perform_audit_adjtimex_settimeofday_stime_remediation</title>
      <description>Shared bash remediation function. Not intended to be changed by tailoring.</description>
      <value selector=""># Function to fix syscall audit rule for given system call. It is
# based on example audit syscall rule definitions as outlined in
# /usr/share/doc/audit-2.3.7/stig.rules file provided with the audit
# package. It will combine multiple system calls belonging to the same
# syscall group into one audit rule (rather than to create audit rule per
# different system call) to avoid audit infrastructure performance penalty
# in the case of 'one-audit-rule-definition-per-one-system-call'. See:
#
#   https://www.redhat.com/archives/linux-audit/2014-November/msg00009.html
#
# for further details.
#
# Expects five arguments (each of them is required) in the form of:
# * audit tool				tool used to load audit rules,
# 					either 'auditctl', or 'augenrules
# * audit rules' pattern		audit rule skeleton for same syscall
# * syscall group			greatest common string this rule shares
# 					with other rules from the same group
# * architecture			architecture this rule is intended for
# * full form of new rule to add	expected full form of audit rule as to be
# 					added into audit.rules file
#
# Note: The 2-th up to 4-th arguments are used to determine how many existing
# audit rules will be inspected for resemblance with the new audit rule
# (5-th argument) the function is going to add. The rule's similarity check
# is performed to optimize audit.rules definition (merge syscalls of the same
# group into one rule) to avoid the "single-syscall-per-audit-rule" performance
# penalty.
#
# Example call:
#
#	See e.g. 'audit_rules_file_deletion_events.sh' remediation script
#
function fix_audit_syscall_rule {

# Load function arguments into local variables
local tool="$1"
local pattern="$2"
local group="$3"
local arch="$4"
local full_rule="$5"

# Check sanity of the input
if [ $# -ne "5" ]
then
	echo "Usage: fix_audit_syscall_rule 'tool' 'pattern' 'group' 'arch' 'full rule'"
	echo "Aborting."
	exit 1
fi

# Create a list of audit *.rules files that should be inspected for presence and correctness
# of a particular audit rule. The scheme is as follows:
# 
# -----------------------------------------------------------------------------------------
#  Tool used to load audit rules | Rule already defined  |  Audit rules file to inspect    |
# -----------------------------------------------------------------------------------------
#        auditctl                |     Doesn't matter    |  /etc/audit/audit.rules         |
# -----------------------------------------------------------------------------------------
#        augenrules              |          Yes          |  /etc/audit/rules.d/*.rules     |
#        augenrules              |          No           |  /etc/audit/rules.d/$key.rules  |
# -----------------------------------------------------------------------------------------
#
declare -a files_to_inspect

retval=0

# First check sanity of the specified audit tool
if [ "$tool" != 'auditctl' ] &amp;&amp; [ "$tool" != 'augenrules' ]
then
	echo "Unknown audit rules loading tool: $1. Aborting."
	echo "Use either 'auditctl' or 'augenrules'!"
	return 1
# If audit tool is 'auditctl', then add '/etc/audit/audit.rules'
# file to the list of files to be inspected
elif [ "$tool" == 'auditctl' ]
then
	files_to_inspect+=('/etc/audit/audit.rules' )
# If audit tool is 'augenrules', then check if the audit rule is defined
# If rule is defined, add '/etc/audit/rules.d/*.rules' to the list for inspection
# If rule isn't defined yet, add '/etc/audit/rules.d/$key.rules' to the list for inspection
elif [ "$tool" == 'augenrules' ]
then
	# Extract audit $key from audit rule so we can use it later
	key=$(expr "$full_rule" : '.*-k[[:space:]]\([^[:space:]]\+\)' '|' "$full_rule" : '.*-F[[:space:]]key=\([^[:space:]]\+\)')
	readarray -t matches &lt; &lt;(sed -s -n -e "\;${pattern};!d" -e "/${arch}/!d" -e "/${group}/!d;F" /etc/audit/rules.d/*.rules)
	if [ $? -ne 0 ]
	then
		retval=1
	fi
	for match in "${matches[@]}"
	do
		files_to_inspect+=("${match}")
	done
	# Case when particular rule isn't defined in /etc/audit/rules.d/*.rules yet
	if [ ${#files_to_inspect[@]} -eq "0" ]
	then
		file_to_inspect="/etc/audit/rules.d/$key.rules"
		files_to_inspect=("$file_to_inspect")
		if [ ! -e "$file_to_inspect" ]
		then
			touch "$file_to_inspect"
			chmod 0640 "$file_to_inspect"
		fi
	fi
fi

#
# Indicator that we want to append $full_rule into $audit_file by default
local append_expected_rule=0

for audit_file in "${files_to_inspect[@]}"
do
	# Filter existing $audit_file rules' definitions to select those that:
	# * follow the rule pattern, and
	# * meet the hardware architecture requirement, and
	# * are current syscall group specific
	readarray -t existing_rules &lt; &lt;(sed -e "\;${pattern};!d" -e "/${arch}/!d" -e "/${group}/!d"  "$audit_file")
	if [ $? -ne 0 ]
	then
		retval=1
	fi

	# Process rules found case-by-case
	for rule in "${existing_rules[@]}"
	do
		# Found rule is for same arch &amp; key, but differs (e.g. in count of -S arguments)
		if [ "${rule}" != "${full_rule}" ]
		then
			# If so, isolate just '(-S \w)+' substring of that rule
			rule_syscalls=$(echo $rule | grep -o -P '(-S \w+ )+')
			# Check if list of '-S syscall' arguments of that rule is subset
			# of '-S syscall' list of expected $full_rule
			if grep -q -- "$rule_syscalls" &lt;&lt;&lt; "$full_rule"
			then
				# Rule is covered (i.e. the list of -S syscalls for this rule is
				# subset of -S syscalls of $full_rule =&gt; existing rule can be deleted
				# Thus delete the rule from audit.rules &amp; our array
				sed -i -e "\;${rule};d" "$audit_file"
				if [ $? -ne 0 ]
				then
					retval=1
				fi
				existing_rules=("${existing_rules[@]//$rule/}")
			else
				# Rule isn't covered by $full_rule - it besides -S syscall arguments
				# for this group contains also -S syscall arguments for other syscall
				# group. Example: '-S lchown -S fchmod -S fchownat' =&gt; group='chown'
				# since 'lchown' &amp; 'fchownat' share 'chown' substring
				# Therefore:
				# * 1) delete the original rule from audit.rules
				# (original '-S lchown -S fchmod -S fchownat' rule would be deleted)
				# * 2) delete the -S syscall arguments for this syscall group, but
				# keep those not belonging to this syscall group
				# (original '-S lchown -S fchmod -S fchownat' would become '-S fchmod'
				# * 3) append the modified (filtered) rule again into audit.rules
				# if the same rule not already present
				#
				# 1) Delete the original rule
				sed -i -e "\;${rule};d" "$audit_file"
				if [ $? -ne 0 ]
				then
					retval=1
				fi

				# 2) Delete syscalls for this group, but keep those from other groups
				# Convert current rule syscall's string into array splitting by '-S' delimiter
				IFS_BKP="$IFS"
				IFS=$'-S'
				read -a rule_syscalls_as_array &lt;&lt;&lt; "$rule_syscalls"
				# Reset IFS back to default
				IFS="$IFS_BKP"
				# Splitting by "-S" can't be replaced by the readarray functionality easily

				# Declare new empty string to hold '-S syscall' arguments from other groups
				new_syscalls_for_rule=''
				# Walk through existing '-S syscall' arguments
				for syscall_arg in "${rule_syscalls_as_array[@]}"
				do
					# Skip empty $syscall_arg values
					if [ "$syscall_arg" == '' ]
					then
						continue
					fi
					# If the '-S syscall' doesn't belong to current group add it to the new list
					# (together with adding '-S' delimiter back for each of such item found)
					if grep -q -v -- "$group" &lt;&lt;&lt; "$syscall_arg"
					then
						new_syscalls_for_rule="$new_syscalls_for_rule -S $syscall_arg"
					fi
				done
				# Replace original '-S syscall' list with the new one for this rule
				updated_rule=${rule//$rule_syscalls/$new_syscalls_for_rule}
				# Squeeze repeated whitespace characters in rule definition (if any) into one
				updated_rule=$(echo "$updated_rule" | tr -s '[:space:]')
				# 3) Append the modified / filtered rule again into audit.rules
				#    (but only in case it's not present yet to prevent duplicate definitions)
				if ! grep -q -- "$updated_rule" "$audit_file"
				then
					echo "$updated_rule" &gt;&gt; "$audit_file"
				fi
			fi
		else
			# $audit_file already contains the expected rule form for this
			# architecture &amp; key =&gt; don't insert it second time
			append_expected_rule=1
		fi
	done

	# We deleted all rules that were subset of the expected one for this arch &amp; key.
	# Also isolated rules containing system calls not from this system calls group.
	# Now append the expected rule if it's not present in $audit_file yet
	if [[ ${append_expected_rule} -eq "0" ]]
	then
		echo "$full_rule" &gt;&gt; "$audit_file"
	fi
done

return $retval

}


# Function to perform remediation for the 'adjtimex', 'settimeofday', and 'stime' audit
# system calls on RHEL, Fedora or OL systems.
# Remediation performed for both possible tools: 'auditctl' and 'augenrules'.
#
# Note: 'stime' system call isn't known at 64-bit arch (see "$ ausyscall x86_64 stime" 's output)
# therefore excluded from the list of time group system calls to be audited on this arch
#
# Example Call:
#
#      perform_audit_adjtimex_settimeofday_stime_remediation
#
function perform_audit_adjtimex_settimeofday_stime_remediation {

# Retrieve hardware architecture of the underlying system
[ "$(getconf LONG_BIT)" = "32" ] &amp;&amp; RULE_ARCHS=("b32") || RULE_ARCHS=("b32" "b64")

for ARCH in "${RULE_ARCHS[@]}"
do

	PATTERN="-a always,exit -F arch=${ARCH} -S .* -k *"
	# Create expected audit group and audit rule form for particular system call &amp; architecture
	if [ ${ARCH} = "b32" ]
	then
		# stime system call is known at 32-bit arch (see e.g "$ ausyscall i386 stime" 's output)
		# so append it to the list of time group system calls to be audited
		GROUP="\(adjtimex\|settimeofday\|stime\)"
		FULL_RULE="-a always,exit -F arch=${ARCH} -S adjtimex -S settimeofday -S stime -k audit_time_rules"
	elif [ ${ARCH} = "b64" ]
	then
		# stime system call isn't known at 64-bit arch (see "$ ausyscall x86_64 stime" 's output)
		# therefore don't add it to the list of time group system calls to be audited
		GROUP="\(adjtimex\|settimeofday\)"
		FULL_RULE="-a always,exit -F arch=${ARCH} -S adjtimex -S settimeofday -k audit_time_rules"
	fi
	# Perform the remediation for both possible tools: 'auditctl' and 'augenrules'
	fix_audit_syscall_rule "auditctl" "$PATTERN" "$GROUP" "$ARCH" "$FULL_RULE"
	fix_audit_syscall_rule "augenrules" "$PATTERN" "$GROUP" "$ARCH" "$FULL_RULE"
done

}</value>
    </Value>
    <Value hidden="true" id="function_perform_audit_rules_privileged_commands_remediation" interactive="0" operator="equals" prohibitChanges="true" type="string">
      <title>Remediation function perform_audit_rules_privileged_commands_remediation</title>
      <description>Shared bash remediation function. Not intended to be changed by tailoring.</description>
      <value selector=""># Function to perform remediation for 'audit_rules_privileged_commands' rule
#
# Expects two arguments:
#
# audit_tool		tool used to load audit rules
# 			One of 'auditctl' or 'augenrules'
#
# min_auid		Minimum original ID the user logged in with
# 			'500' for RHEL-6 and before, '1000' for RHEL-7 and after.
#
# Example Call(s):
#
#      perform_audit_rules_privileged_commands_remediation "auditctl" "500"
#      perform_audit_rules_privileged_commands_remediation "augenrules"	"1000"
#
function perform_audit_rules_privileged_commands_remediation {
#
# Load function arguments into local variables
local tool="$1"
local min_auid="$2"

# Check sanity of the input
if [ $# -ne "2" ]
then
	echo "Usage: perform_audit_rules_privileged_commands_remediation 'auditctl | augenrules' '500 | 1000'"
	echo "Aborting."
	exit 1
fi

declare -a files_to_inspect=()

# Check sanity of the specified audit tool
if [ "$tool" != 'auditctl' ] &amp;&amp; [ "$tool" != 'augenrules' ]
then
	echo "Unknown audit rules loading tool: $1. Aborting."
	echo "Use either 'auditctl' or 'augenrules'!"
	exit 1
# If the audit tool is 'auditctl', then:
# * add '/etc/audit/audit.rules'to the list of files to be inspected,
# * specify '/etc/audit/audit.rules' as the output audit file, where
#   missing rules should be inserted
elif [ "$tool" == 'auditctl' ]
then
	files_to_inspect=("/etc/audit/audit.rules")
	output_audit_file="/etc/audit/audit.rules"
#
# If the audit tool is 'augenrules', then:
# * add '/etc/audit/rules.d/*.rules' to the list of files to be inspected
#   (split by newline),
# * specify /etc/audit/rules.d/privileged.rules' as the output file, where
#   missing rules should be inserted
elif [ "$tool" == 'augenrules' ]
then
	readarray -t files_to_inspect &lt; &lt;(find /etc/audit/rules.d -maxdepth 1 -type f -name '*.rules' -print)
	output_audit_file="/etc/audit/rules.d/privileged.rules"
fi

# Obtain the list of SUID/SGID binaries on the particular system (split by newline)
# into privileged_binaries array
readarray -t privileged_binaries &lt; &lt;(find / -xdev -type f -perm -4000 -o -type f -perm -2000 2&gt;/dev/null)

# Keep list of SUID/SGID binaries that have been already handled within some previous iteration
declare -a sbinaries_to_skip=()

# For each found sbinary in privileged_binaries list
for sbinary in "${privileged_binaries[@]}"
do

	# Check if this sbinary wasn't already handled in some of the previous sbinary iterations
	# Return match only if whole sbinary definition matched (not in the case just prefix matched!!!)
	if [[ $(sed -ne "\|${sbinary}|p" &lt;&lt;&lt; "${sbinaries_to_skip[*]}") ]]
	then
		# If so, don't process it second time &amp; go to process next sbinary
		continue
	fi

	# Reset the counter of inspected files when starting to check
	# presence of existing audit rule for new sbinary
	local count_of_inspected_files=0

	# Define expected rule form for this binary
	expected_rule="-a always,exit -F path=${sbinary} -F perm=x -F auid&gt;=${min_auid} -F auid!=unset -k privileged"

	# If list of audit rules files to be inspected is empty, just add new rule and move on to next binary
	if [[ ${#files_to_inspect[@]} -eq 0 ]]; then
		echo "$expected_rule" &gt;&gt; "$output_audit_file"
		continue
	fi

	# Replace possible slash '/' character in sbinary definition so we could use it in sed expressions below
	sbinary_esc=${sbinary//$'/'/$'\/'}

	# For each audit rules file from the list of files to be inspected
	for afile in "${files_to_inspect[@]}"
	do

		# Search current audit rules file's content for match. Match criteria:
		# * existing rule is for the same SUID/SGID binary we are currently processing (but
		#   can contain multiple -F path= elements covering multiple SUID/SGID binaries)
		# * existing rule contains all arguments from expected rule form (though can contain
		#   them in arbitrary order)
	
		base_search=$(sed -e '/-a always,exit/!d' -e '/-F path='"${sbinary_esc}"'/!d'		\
				-e '/-F path=[^[:space:]]\+/!d'   -e '/-F perm=.*/!d'						\
				-e '/-F auid&gt;='"${min_auid}"'/!d' -e '/-F auid!=\(4294967295\|unset\)/!d'	\
				-e '/-k \|-F key=/!d' "$afile")

		# Increase the count of inspected files for this sbinary
		count_of_inspected_files=$((count_of_inspected_files + 1))

		# Require execute access type to be set for existing audit rule
		exec_access='x'

		# Search current audit rules file's content for presence of rule pattern for this sbinary
		if [[ $base_search ]]
		then

			# Current audit rules file already contains rule for this binary =&gt;
			# Store the exact form of found rule for this binary for further processing
			concrete_rule=$base_search

			# Select all other SUID/SGID binaries possibly also present in the found rule

			readarray -t handled_sbinaries &lt; &lt;(grep -o -e "-F path=[^[:space:]]\+" &lt;&lt;&lt; "$concrete_rule")
			handled_sbinaries=("${handled_sbinaries[@]//-F path=/}")

			# Merge the list of such SUID/SGID binaries found in this iteration with global list ignoring duplicates
			readarray -t sbinaries_to_skip &lt; &lt;(for i in "${sbinaries_to_skip[@]}" "${handled_sbinaries[@]}"; do echo "$i"; done | sort -du)

			# Separate concrete_rule into three sections using hash '#'
			# sign as a delimiter around rule's permission section borders
			concrete_rule="$(echo "$concrete_rule" | sed -n "s/\(.*\)\+\(-F perm=[rwax]\+\)\+/\1#\2#/p")"

			# Split concrete_rule into head, perm, and tail sections using hash '#' delimiter

			rule_head=$(cut -d '#' -f 1 &lt;&lt;&lt; "$concrete_rule")
			rule_perm=$(cut -d '#' -f 2 &lt;&lt;&lt; "$concrete_rule")
			rule_tail=$(cut -d '#' -f 3 &lt;&lt;&lt; "$concrete_rule")

			# Extract already present exact access type [r|w|x|a] from rule's permission section
			access_type=${rule_perm//-F perm=/}

			# Verify current permission access type(s) for rule contain 'x' (execute) permission
			if ! grep -q "$exec_access" &lt;&lt;&lt; "$access_type"
			then

				# If not, append the 'x' (execute) permission to the existing access type bits
				access_type="$access_type$exec_access"
				# Reconstruct the permissions section for the rule
				new_rule_perm="-F perm=$access_type"
				# Update existing rule in current audit rules file with the new permission section
				sed -i "s#${rule_head}\(.*\)${rule_tail}#${rule_head}${new_rule_perm}${rule_tail}#" "$afile"

			fi

		# If the required audit rule for particular sbinary wasn't found yet, insert it under following conditions:
		#
		# * in the "auditctl" mode of operation insert particular rule each time
		#   (because in this mode there's only one file -- /etc/audit/audit.rules to be inspected for presence of this rule),
		#
		# * in the "augenrules" mode of operation insert particular rule only once and only in case we have already
		#   searched all of the files from /etc/audit/rules.d/*.rules location (since that audit rule can be defined
		#   in any of those files and if not, we want it to be inserted only once into /etc/audit/rules.d/privileged.rules file)
		#
		elif [ "$tool" == "auditctl" ] || [[ "$tool" == "augenrules" &amp;&amp; $count_of_inspected_files -eq "${#files_to_inspect[@]}" ]]
		then

			# Check if this sbinary wasn't already handled in some of the previous afile iterations
			# Return match only if whole sbinary definition matched (not in the case just prefix matched!!!)
			if [[ ! $(sed -ne "\|${sbinary}|p" &lt;&lt;&lt; "${sbinaries_to_skip[*]}") ]]
			then
				# Current audit rules file's content doesn't contain expected rule for this
				# SUID/SGID binary yet =&gt; append it
				echo "$expected_rule" &gt;&gt; "$output_audit_file"
			fi

			continue
		fi

	done

done
}</value>
    </Value>
    <Value hidden="true" id="function_populate" interactive="0" operator="equals" prohibitChanges="true" type="string">
      <title>Remediation function populate</title>
      <description>Shared bash remediation function. Not intended to be changed by tailoring.</description>
      <value selector=""># The populate function isn't directly used by SSG at the moment but it can be 
# used for testing purposes and will be used in SSG Testsuite in the future.

function populate {
# code to populate environment variables needed (for unit testing)
if [ -z "${!1}" ]; then
	echo "$1 is not defined. Exiting."
	exit
fi
}</value>
    </Value>
    <Value hidden="true" id="function_replace_or_append" interactive="0" operator="equals" prohibitChanges="true" type="string">
      <title>Remediation function replace_or_append</title>
      <description>Shared bash remediation function. Not intended to be changed by tailoring.</description>
      <value selector=""># Function to replace configuration setting in config file or add the configuration setting if
# it does not exist.
#
# Expects arguments:
#
# config_file:		Configuration file that will be modified
# key:			Configuration option to change
# value:		Value of the configuration option to change
# cce:			The CCE identifier or '@CCENUM@' if no CCE identifier exists
# format:		The printf-like format string that will be given stripped key and value as arguments,
#			so e.g. '%s=%s' will result in key=value subsitution (i.e. without spaces around =)
#
# Optional arugments:
#
# format:		Optional argument to specify the format of how key/value should be
# 			modified/appended in the configuration file. The default is key = value.
#
# Example Call(s):
#
#     With default format of 'key = value':
#     replace_or_append '/etc/sysctl.conf' '^kernel.randomize_va_space' '2' '@CCENUM@'
#
#     With custom key/value format:
#     replace_or_append '/etc/sysconfig/selinux' '^SELINUX=' 'disabled' '@CCENUM@' '%s=%s'
#
#     With a variable:
#     replace_or_append '/etc/sysconfig/selinux' '^SELINUX=' $var_selinux_state '@CCENUM@' '%s=%s'
#
function replace_or_append {
  local default_format='%s = %s' case_insensitive_mode=yes sed_case_insensitive_option='' grep_case_insensitive_option=''
  local config_file=$1
  local key=$2
  local value=$3
  local cce=$4
  local format=$5

  if [ "$case_insensitive_mode" = yes ]; then
    sed_case_insensitive_option="i"
    grep_case_insensitive_option="-i"
  fi
  [ -n "$format" ] || format="$default_format"
  # Check sanity of the input
  [ $# -ge "3" ] || { echo "Usage: replace_or_append &lt;config_file_location&gt; &lt;key_to_search&gt; &lt;new_value&gt; [&lt;CCE number or literal '@CCENUM@' if unknown&gt;] [printf-like format, default is '$default_format']" &gt;&amp;2; exit 1; }

  # Test if the config_file is a symbolic link. If so, use --follow-symlinks with sed.
  # Otherwise, regular sed command will do.
  sed_command=('sed' '-i')
  if test -L "$config_file"; then
    sed_command+=('--follow-symlinks')
  fi

  # Test that the cce arg is not empty or does not equal @CCENUM@.
  # If @CCENUM@ exists, it means that there is no CCE assigned.
  if [ -n "$cce" ] &amp;&amp; [ "$cce" != '@CCENUM@' ]; then
    cce="${cce}"
  else
    cce="CCE"
  fi

  # Strip any search characters in the key arg so that the key can be replaced without
  # adding any search characters to the config file.
  stripped_key=$(sed 's/[\^=\$,;+]*//g' &lt;&lt;&lt; "$key")

  # shellcheck disable=SC2059
  printf -v formatted_output "$format" "$stripped_key" "$value"

  # If the key exists, change it. Otherwise, add it to the config_file.
  # We search for the key string followed by a word boundary (matched by \&gt;),
  # so if we search for 'setting', 'setting2' won't match.
  if LC_ALL=C grep -q -m 1 $grep_case_insensitive_option -e "${key}\\&gt;" "$config_file"; then
    "${sed_command[@]}" "s/${key}\\&gt;.*/$formatted_output/g$sed_case_insensitive_option" "$config_file"
  else
    # \n is precaution for case where file ends without trailing newline
    printf '\n# Per %s: Set %s in %s\n' "$cce" "$formatted_output" "$config_file" &gt;&gt; "$config_file"
    printf '%s\n' "$formatted_output" &gt;&gt; "$config_file"
  fi
}</value>
    </Value>
    <Value hidden="true" id="function_set_faillock_option_to_value_in_pam_file" interactive="0" operator="equals" prohibitChanges="true" type="string">
      <title>Remediation function set_faillock_option_to_value_in_pam_file</title>
      <description>Shared bash remediation function. Not intended to be changed by tailoring.</description>
      <value selector="">function set_faillock_option_to_value_in_pam_file {
	# If invoked with no arguments, exit. This is an intentional behavior.
	[ $# -gt 1 ] || return 0
	[ $# -ge 3 ] || die "$0 requires exactly zero, three, or four arguments"
	[ $# -le 4 ] || die "$0 requires exactly zero, three, or four arguments"
	local _pamFile="$1" _option="$2" _value="$3" _insert_lines_callback="$4"
	# pam_faillock.so already present?
	if grep -q "^auth.*pam_faillock.so.*" "$_pamFile"; then

		# pam_faillock.so present, is the option present?
		if grep -q "^auth.*[default=die].*pam_faillock.so.*authfail.*$_option=" "$_pamFile"; then

			# both pam_faillock.so &amp; option present, just correct option to the right value
			sed -i --follow-symlinks "s/\(^auth.*required.*pam_faillock.so.*preauth.*silent.*\)\($_option *= *\).*/\1\2$_value/" "$_pamFile"
			sed -i --follow-symlinks "s/\(^auth.*[default=die].*pam_faillock.so.*authfail.*\)\($_option *= *\).*/\1\2$_value/" "$_pamFile"

		# pam_faillock.so present, but the option not yet
		else

			# append correct option value to appropriate places
			sed -i --follow-symlinks "/^auth.*required.*pam_faillock.so.*preauth.*silent.*/ s/$/ $_option=$_value/" "$_pamFile"
			sed -i --follow-symlinks "/^auth.*[default=die].*pam_faillock.so.*authfail.*/ s/$/ $_option=$_value/" "$_pamFile"
		fi

	# pam_faillock.so not present yet
	else
		test -z "$_insert_lines_callback" || "$_insert_lines_callback" "$_option" "$_value" "$_pamFile"
		# insert pam_faillock.so preauth &amp; authfail rows with proper value of the option in question
	fi
}</value>
    </Value>
  </Group>
  <Group id="openstack" prodtype="rhosp13">
    <title>OpenStack</title>
    <description>TODO TODO TODO</description>
    <Group id="neutron_container" prodtype="rhosp13">
      <title>Neutron Configuration Checklist</title>
      <description>Security configuration requirements for Neutron.</description>
    </Group>
    <Group id="nova_container" prodtype="rhosp13">
      <title>Nova Configuration Checklist</title>
      <description>Security configuration items for Nova.</description>
    </Group>
    <Group id="cinder_container" prodtype="rhosp13">
      <title>Cinder Configuration Checklist</title>
      <description>Security configuration settings for Cinder.</description>
    </Group>
    <Group id="horizon_container" prodtype="rhosp13">
      <title>Horizon Configuration Checklist</title>
      <description>Recommended security configuration settings for Horizon.</description>
    </Group>
    <Group id="keystone_container" prodtype="rhosp13">
      <title>Keystone Configuration Checklist</title>
      <description>Security configuration recommendations for Keystone.</description>
      <Rule id="container_keystone_lockout_failure_attempts" severity="medium">
        <title>Set Maximum Number of Failed Authentication Attempts</title>
        <description>The account lockout feature limits the number of incorrect password
attempts. If a user fails to authenticate after the maximum number
of attempts, the service disables the user.
<br/>
The maximum number of failed authentication attempts is set by the
<tt>lockout_failure_attempts</tt> option in under the
<tt>[security_compliance]</tt> section in <tt>keystone.conf</tt>.</description>
        <rationale>Defining a maximum number of failed logon attempts can help
mitigate brute force password attacks.</rationale>
        <ref nist="AC-7"/>
        <oval id="container_keystone_lockout_failure_attempts"/>
        <ocil clause="lockout_failure_attempts is commented out or not configured properly">Run the following command to see what the maximum authentication
attempts is:
<br/>
<pre>$ grep lockout_failure_attempts /var/lib/config-data/puppet-generated/keystone/etc/keystone/keystone.conf</pre>
<br/>
If properly configured, the output should be:
<pre>lockout_failure_attempts=<sub idref="var_keystone_lockout_failure_attempts"/></pre></ocil>
      </Rule>
      <Rule id="container_keystone_lockout_duration" severity="medium">
        <title>Set Account Lockout Duration</title>
        <description>Once a user account is locked out, such as exceeding the
amount of logon attempts as defined by <tt>lockout_failure_attempts</tt>,
Keystone will lockout an account for the time period defined by the
<tt>lockout_duration</tt> configuration option unde the
<tt>[security_compliance]</tt> section in <tt>keystone.conf</tt>.
<br/>
Note that if <tt>lockout_failure_attempts</tt> is enabled and
<tt>lockout_duration</tt> is left undefined, users will be
locked out indefinitely until the user is explicitly re-enabled.</description>
        <rationale>Defining a lockout duration helps mitigate certain attacks,
such as brute force attempts. Additionally defining a lockout
duration, versus indefinately locking an account, lowers
administrative burden of re-enabling accounts of users
who accidentally triggered the maximum failure attempts.</rationale>
        <ref nist="AC-7"/>
        <oval id="container_keystone_lockout_duration"/>
        <ocil clause="lockout_duration is not configured properly">Run the following command to see what the account lockout
duration is:
<br/>
<pre>$ grep lockout_duration /var/lib/config-data/puppet-generated/keystone/etc/keystone/keystone.conf</pre>
<br/>
If properly configured, the output should be:
<pre>lockout_duration=<sub idref="var_keystone_lockout_failure_duration"/></pre></ocil>
      </Rule>
      <Rule id="container_keystone_disable_user_account_days_inactive" severity="medium">
        <title>Set Maximum Inactivity Period</title>
        <description>Keystone can be configured to disable accounts after an
organizationally-defined time period. This is achieved by configuring the
<tt>disable_user_account_days_inactive</tt> setting in the
<tt>[security_compliance]</tt> section in <tt>keystone.conf</tt>.</description>
        <rationale>Automatically disabling accounts ensures that users who have not
authenticated for an organizationally-defined time period are
automatically disabled. This reduces the risk of stale accounts
being used for malicious purposes.</rationale>
        <ref nist="AC-2(3)"/>
        <oval id="container_keystone_disable_user_account_days_inactive"/>
        <ocil clause="disable_user_account_days_inactive is commented out or not configured properly">Run the following command to see what the maximum authentication
attempts is:
<br/>
<pre>$ grep disable_user_account_days_inactive /var/lib/config-data/puppet-generated/keystone/etc/keystone/keystone.conf</pre>
<br/>
If properly configured, the output should be:
<pre>disable_user_account_days_inactive = <sub idref="var_keystone_disable_user_account_days_inactive"/></pre></ocil>
      </Rule>
    </Group>
    <Group id="keystone" prodtype="rhosp13">
      <title>Keystone STIG Checklist</title>
      <description>High level overview of Keystone STIG settings to go here!</description>
      <Value id="var_keystone_lockout_failure_attempts" operator="less than or equal" type="string">
        <title>Maximum Number of Failed Authentication Attempts</title>
        <description>Specifies the maximum number of failed authentication attempts allowed
prior to disabling a Keystone user.</description>
        <value>6</value>
        <value selector="3">3</value>
      </Value>
      <Value id="var_keystone_lockout_duration" operator="greater than or equal" type="string">
        <title>Account Lockout Duration</title>
        <description>Specifies the time period, in seconds, of how long a user
account will be disabled after exceeding the maximum failed
logon attempts.</description>
        <value>1800</value>
        <value selector="15_minutes">900</value>
      </Value>
      <Value id="var_keystone_disable_user_account_days_inactive" operator="less than or equal" type="string">
        <title>Maximum Number of Days Since Last Authentication</title>
        <description>Specifies the maximum number of days a user has not authenticated
prior to automatically disabling the account.</description>
        <value>90</value>
        <value selector="90">90</value>
        <value selector="180">180</value>
        <value selector="45">45</value>
      </Value>
      <Rule id="keystone_disable_user_account_days_inactive" severity="medium">
        <title>Set Maximum Inactivity Period</title>
        <description>Keystone can be configured to disable accounts after an
organizationally-defined time period. This is achieved by configuring the
<tt>disable_user_account_days_inactive</tt> setting in the 
<tt>[security_compliance]</tt> section in <tt>keystone.conf</tt>.</description>
        <rationale>Automatically disabling accounts ensures that users who have not
authenticated for an organizationally-defined time period are
automatically disabled. This reduces the risk of stale accounts
being used for malicious purposes.</rationale>
        <ref nist="AC-2(3)"/>
        <oval id="keystone_disable_user_account_days_inactive"/>
        <ocil clause="disable_user_account_days_inactive is commented out or not configured properly">Run the following command to see what the maximum authentication
attempts is:
<br/>
<pre>$ grep disable_user_account_days_inactive /etc/keystone/keystone.conf</pre>
<br/>
If properly configured, the output should be:
<pre>disable_user_account_days_inactive = <sub idref="var_keystone_disable_user_account_days_inactive"/></pre></ocil>
      </Rule>
      <Rule id="keystone_lockout_duration" severity="medium">
        <title>Set Account Lockout Duration</title>
        <description>Once a user account is locked out, such as exceeding the
amount of logon attempts as defined by <tt>lockout_failure_attempts</tt>,
Keystone will lockout an account for the time period defined by the
<tt>lockout_duration</tt> configuration option unde the
<tt>[security_compliance]</tt> section in <tt>keystone.conf</tt>.
<br/>
Note that if <tt>lockout_failure_attempts</tt> is enabled and
<tt>lockout_duration</tt> is left undefined, users will be
locked out indefinitely until the user is explicitly re-enabled.</description>
        <rationale>Defining a lockout duration helps mitigate certain attacks,
such as brute force attempts. Additionally defining a lockout
duration, versus indefinately locking an account, lowers
administrative burden of re-enabling accounts of users
who accidentally triggered the maximum failure attempts.</rationale>
        <ref nist="AC-7"/>
        <oval id="keystone_lockout_duration"/>
        <ocil clause="lockout_duration is not configured properly">Run the following command to see what the account lockout
duration is:
<br/>
<pre>$ grep lockout_duration /etc/keystone/keystone.conf</pre>
<br/>
If properly configured, the output should be:
<pre>lockout_duration=<sub idref="var_keystone_lockout_failure_duration"/></pre></ocil>
      </Rule>
      <Rule id="keystone_lockout_failure_attempts" severity="medium">
        <title>Set Maximum Number of Failed Authentication Attempts</title>
        <description>The account lockout feature limits the number of incorrect password
attempts. If a user fails to authenticate after the maximum number
of attempts, the service disables the user. 
<br/>
The maximum number of failed authentication attempts is set by the
<tt>lockout_failure_attempts</tt> option in under the
<tt>[security_compliance]</tt> section in <tt>keystone.conf</tt>.</description>
        <rationale>Defining a maximum number of failed logon attempts can help
mitigate brute force password attacks.</rationale>
        <ref nist="AC-7"/>
        <oval id="keystone_lockout_failure_attempts"/>
        <ocil clause="lockout_failure_attempts is commented out or not configured properly">Run the following command to see what the maximum authentication
attempts is:
<br/>
<pre>$ grep lockout_failure_attempts /etc/keystone/keystone.conf</pre>
<br/>
If properly configured, the output should be:
<pre>lockout_failure_attempts=<sub idref="var_keystone_lockout_failure_attempts"/></pre></ocil>
      </Rule>
    </Group>
    <Group id="nova" prodtype="rhosp13">
      <title>Nova STIG Checklist</title>
      <description>High level overview of Nova STIG settings to go here!</description>
    </Group>
    <Group id="horizon" prodtype="rhosp13">
      <title>Horizon STIG Checklist</title>
      <description>High level overview of Horizon STIG settings to go here!</description>
    </Group>
    <Group id="cinder" prodtype="rhosp13">
      <title>Cinder STIG Checklist</title>
      <description>High level overview of Cinder STIG settings to go here!</description>
    </Group>
    <Group id="neutron" prodtype="rhosp13">
      <title>Neutron STIG Checklist</title>
      <description>High level overview of Neutron STIG settings to go here!</description>
    </Group>
  </Group>
  <Group id="intro">
    <title>Introduction</title>
    <description>The purpose of this guidance is to provide security configuration
recommendations and baselines for Red Hat OpenShift Container Platform 3.
The guide is intended for system and/or application administrators. Readers are assumed to
possess basic system administration skills for the application's operating systems, as well
as some familiarity with the product's documentation and administration
conventions. Some instructions within this guide are complex.
All directions should be followed completely and with understanding of
their effects in order to avoid serious adverse effects on the system
and its security.</description>
    <Group id="general-principles">
      <title>General Principles</title>
      <description>The following general principles motivate much of the advice in this
guide and should also influence any configuration decisions that are
not explicitly covered.</description>
      <Group id="principle-least-privilege">
        <title>Least Privilege</title>
        <description>Grant the least privilege necessary for user accounts and software to perform tasks.
For example, <tt>sudo</tt> can be implemented to limit authorization to super user
accounts on the system only to designated personnel. Another example is to limit
logins on server systems to only those administrators who need to log into them in
order to perform administration tasks.</description>
      </Group>
      <Group id="principle-use-security-tools">
        <title>Configure Security Tools to Improve System Robustness</title>
        <description>Several tools exist which can be effectively used to improve a system's
resistance to and detection of unknown attacks. These tools can improve
robustness against attack at the cost of relatively little configuration
effort.</description>
      </Group>
      <Group id="principle-separate-servers">
        <title>Run Different Network Services on Separate Systems</title>
        <description>Whenever possible, a server should be dedicated to serving exactly one
network service. This limits the number of other services that can
be compromised in the event that an attacker is able to successfully
exploit a software flaw in one network service.</description>
      </Group>
      <Group id="principle-encrypt-transmitted-data">
        <title>Encrypt Transmitted Data Whenever Possible</title>
        <description>Data transmitted over a network, whether wired or wireless, is susceptible
to passive monitoring. Whenever practical solutions for encrypting
such data exist, they should be applied. Even if data is expected to
be transmitted only over a local network, it should still be encrypted.
Encrypting authentication data, such as passwords, is particularly
important. Networks of Red Hat OpenShift Container Platform 3 machines can and should be configured
so that no unencrypted authentication data is ever transmitted between
machines.</description>
      </Group>
    </Group>
    <Group id="how-to-use">
      <title>How to Use This Guide</title>
      <description>Readers should heed the following points when using the guide.</description>
      <Group id="intro-formatting-conventions">
        <title>Formatting Conventions</title>
        <description>Commands intended for shell execution, as well as configuration file text,
are featured in a <tt>monospace font</tt>. <i>Italics</i> are used
to indicate instances where the system administrator must substitute
the appropriate information into a command or configuration file.</description>
      </Group>
      <Group id="intro-test-non-production">
        <title>Test in Non-Production Environment</title>
        <description>This guidance should always be tested in a non-production environment
before deployment. This test environment should simulate the setup in
which the system will be deployed as closely as possible.</description>
      </Group>
      <Group id="intro-read-sections-completely">
        <title>Read Sections Completely and in Order</title>
        <description>Each section may build on information and recommendations discussed in
prior sections. Each section should be read and understood completely;
instructions should never be blindly applied. Relevant discussion may
occur after instructions for an action.</description>
      </Group>
      <Group id="intro-root-shell-assumed">
        <title>Root Shell Environment Assumed</title>
        <description>Most of the actions listed in this document are written with the
assumption that they will be executed by the root user running the
<tt>/bin/bash</tt> shell. Commands preceded with a hash mark (#)
assume that the administrator will execute the commands as root, i.e.
apply the command via <tt>sudo</tt> whenever possible, or use
<tt>su</tt> to gain root privileges if <tt>sudo</tt> cannot be
used. Commands which can be executed as a non-root user are are preceded
by a dollar sign ($) prompt.</description>
      </Group>
      <Group id="intro-reboot-required">
        <title>Reboot Required</title>
        <description>A system or service reboot is implicitly required after some actions in order to
complete the reconfiguration of the system. In many cases, the changes
will not take effect until a reboot is performed. In order to ensure
that changes are applied properly and to test functionality, always
reboot the system after applying a set of recommendations from this guide.</description>
      </Group>
    </Group>
  </Group>
  <Group id="openshift" prodtype="ocp3,ocp4">
    <title>OpenShift Settings</title>
    <description>Each section of this configuration guide includes information about the default configuration
of an OpenShift cluster and a set of recommendations for hardening the configuration. For each
hardening recommendation, information on how to implement the control and/or how to verify or audit
the control is provided. In some cases, remediation information is also provided.

Many of the settings in the hardening guide are in place by default. The audit information for these
settings is provided in order to verify that the cluster admininstrator has not made changes that
would be less secure than the OpenShift defaults. A small number of items require configuration.

Finally, there are some recommendations that require decisions by the system operator, such as audit
log size, retention, and related settings.</description>
    <Group id="scheduler" prodtype="ocp3">
      <title>OpenShift - Kubernetes - Scheduler Settings</title>
      <description>Contains evaluations for kube-scheduler configuration settings.</description>
      <Rule id="scheduler_profiling_argument" prodtype="ocp3" severity="low">
        <title>Disable Scheduler Profiling</title>
        <description>Profiling should be disabled if not needed. To disable profiling,
edit the Scheduler pod specification file
<tt>/etc/origin/master/master-config.yaml</tt> file on the master
node and set the below parameter:
<pre>kubernetesMasterConfig:
  schedulerArguments:
    profiling:
    - false</pre></description>
        <rationale>Profiling allows for the identification of specific performance
bottlenecks. It generates a significant amount of program data that could
potentially be exploited to uncover system and program details. If you are
not experiencing any bottlenecks and do not need the profiler for
troubleshooting purposes, it is recommended to turn it off to reduce the
potential attack surface.</rationale>
        <ident cce="80609-1"/>
        <ref cis="1.2.1,1.1.8"/>
        <oval id="scheduler_profiling_argument"/>
        <ocil clause="profiling is not being used and is not disabled">Run the following command on the master node(s):
<pre>$ sudo grep -A2 profiling /etc/origin/master/master-config.yaml</pre>
The output should return <tt>false</tt>.</ocil>
      </Rule>
    </Group>
    <Group id="general" prodtype="ocp3">
      <title>OpenShift - General Security Practices</title>
      <description>Contains evaluations for general security practices for operating an OpenShift environment.</description>
      <Rule id="general_least_privilege_scc" prodtype="ocp3" severity="low">
        <title>Ensure Pods use Least Privileged Security Context Constraints (SCC)</title>
        <description>Security Context Constraints (SCC) control what actions pods can perform and what resources
they can access. SCCs combine a set of security configurations into a single policy object
that can be applied to pods. These security configurations include, but are not limited to,
Linux Capabilities, Seccomp profiles, User and Group ID Ranges, and types of mounts.

OpenShift ships with several SCCs. The most constrained is the <tt>restricted</tt> SCC, and the
least constrained is the <tt>privileged</tt> SCC. The others provide intermediate levels of
constraint for various use cases. The <tt>restricted</tt> SCC is granted to all authenticated
users by default.

If required, a cluster administrator may allow certain pods to run with different SCCs. As
a general principal pods should always be ran with the most restrictive SCC possible.

Pods inherit their SCC from the Service Account used to run the pod. With the default project
template, new projects receive a Service Account named <tt>default</tt> that is used to run pods. This
default service account is only granted the ability to run the <tt>restricted</tt> SCC.</description>
        <rationale>Ensure Pods are ran with the least privileges possible.</rationale>
        <ref cis="1.6.2,1.6.6"/>
        <oval id="general_least_privilege_scc"/>
        <ocil clause="they are not">To show all available SCCs:
<pre>$ oc describe scc</pre>

To audit a single pod:
<pre>$ oc describe <i>POD</i> | grep openshift.io\/scc</pre>

Ensure each Pod on the system is configured to use the least privilege SCC possible.</ocil>
      </Rule>
      <Rule id="general_limit_cluster_admin" prodtype="ocp3" severity="low">
        <title>Limit Use of cluster-admin Role</title>
        <description>Review users, groups, and service accounts, that are bound to the <tt>cluster-admin</tt>
role and decide whether they require such access. Consider creating custom, least-privilege roles
for users and service accounts.</description>
        <rationale>The <tt>cluster-admin</tt> role contains elevated privileges and should only be used
when necessary.</rationale>
        <ref cis="1.6.1"/>
        <oval id="general_limit_cluster_admin"/>
        <ocil clause="they are not">To ensure that <tt>cluster-admin</tt> role is only used as where required, run
the following command to receive a list of all associated bindings:
<pre>$ oc get clusterrolebindings | grep cluster-admin</pre>

Verify all bindings are appropriately bound to <tt>cluster-admin</tt>, which should
reflect the least privilege required.</ocil>
      </Rule>
      <Rule id="general_enable_seccomp" prodtype="ocp3" severity="high">
        <title>Enable seccomp</title>
        <description>Seccomp (secure computing mode) is used to restrict the set of system calls applications can make,
allowing cluster administrators greater control over the security of workloads running in
the OpenShift Container Platform. 

By default OpenShift is delivered with seccomp disabled, allowing all workloads to run
unconfined and issue all system calls.

To configure seccomp profiles, follow the instructions in the OpenShift Cluster
Administration documentation:

https://docs.openshift.com/container-platform/3.11/admin_guide/seccomp.html</description>
        <rationale>Failure to enable seccomp will allow containers to issue any and all system calls
available in the environment.</rationale>
        <ref cis="1.6.5"/>
        <oval id="general_enable_seccomp"/>
        <ocil clause="they are not">Verify that Security Context Constraints (SCCs) have been configured with seccomp:
<pre>$ oc get scc -ocustom-columns=NAME:.metadata.name,SECCOMP-PROFILES:.seccompProfiles</pre></ocil>
      </Rule>
      <Rule id="general_use_openshift_projects" prodtype="ocp3" severity="medium">
        <title>Use OpenShift Projects to Maintain Boundaries Between Resources</title>
        <description>An OpenShift project is a namespace with additional annotations and is the central vehicle
by which access to resources is managed for regular users. Use OpenShift projects
to maintain boundaries between resources.</description>
        <rationale>A project allows a community of users to organize and manage their content in isolation from other
communities. Users must be given access to projects by administrators, or if allowed to create projects,
automatically have access only to their own projects.</rationale>
        <ref cis="1.6.3"/>
        <oval id="general_use_openshift_projects"/>
        <ocil clause="they are not">To display a list of projects configured on a system, run the following command:
<pre>$ oc get projects</pre>

Verify each project represents a unique community.</ocil>
      </Rule>
      <Rule id="general_configure_imagepolicywebhook" prodtype="ocp3" severity="medium">
        <title>Manage Image Provenance Using ImagePolicyWebhook Adminission</title>
        <description>OpenShift administrators can control which images can be imported, tagged, and run in a cluster.
There are two facilities for this purpose: (1) Allowed Registries, allowing administrators to
restrict image origins to known external registries; and (2) ImagePolicy Admission plug-in which lets
administrators specify specific images which are allowed to run on the OpenShift cluster.

Configure Image Provenance using the <tt>ImagePolicyWebhook</tt> admission controller per
the Image Policy chapter in the Cluster Administration documentation:

https://docs.openshift.com/container-platform/3.11/admin_guide/image_policy.html</description>
        <rationale>Image Policy ensures that only approved container images are allowed to be ran on the OpenShift platform.</rationale>
        <ident cce="81001-0"/>
        <ref cis="1.6.7"/>
        <oval id="general_configure_imagepolicywebhook"/>
        <ocil clause="an image provenance policy is not configured">Review <tt>ImagePolicyConfig</tt> in <tt>/etc/origin/master/master-config.yaml</tt>:
<pre>$ grep admissionConfig /etc/origin/master/master-config.yaml</pre>

Look for the following:
<pre>admissionConfig:
  pluginConfig:
    openshift.io/ImagePolicy</pre></ocil>
      </Rule>
      <Rule id="general_create_network_segmentation" prodtype="ocp3" severity="high">
        <title>Ensure Network Segmentation is Configured</title>
        <description>OpenShuft provides multi-tenant network isolation to segregate network traffic between
containers belonging to different tenants (users or applications) while running on a shared
cluster. Red Hat also works with 3rd-party SDN vendors to provide the same level of capabilities
integrated with OpenShift. For stronger security, create network segmentation using Network Policies
as it provides finer-grained control.</description>
        <rationale>Properly configured network policies ensure traffic between OpenShift tenants is
isolated.</rationale>
        <ref cis="1.6.4"/>
        <oval id="general_create_network_segmentation"/>
        <ocil clause="they are not">Verify on OpenShift master nodes the plugin being used:
<pre>$ grep networkPluginName /etc/origin/master/master-config.yaml</pre>

Nodes should be properly configured to create network segmentation using the Multi-tenant plugin or
Network Policies.</ocil>
      </Rule>
      <Rule id="general_scc_for_privileged_containers" prodtype="ocp3" severity="medium">
        <title>Use Security Context Constrains as Compensating Controls for Privileged Containers</title>
        <description>Security Context Constraints (SCCs) and Role-Based Access Control (RBAC) can be used as
compensating controls to mitigate the use of privileged containers. By default,
the <tt>restricted</tt> SCC is granted to all authenticated users by default. The
<tt>restricted</tt> SCC prevents privileged pods from running. Administrators may create
less restrictive SCCs based on individual container needs. For example, if a container requires
running as the root user, the <tt>anysuid</tt> SCC can be used, which will not expose additional
access granted by running privileged containers.

Refer to the <i>Managing Security Context Constraints</i> chapter of the OpenShift Cluster Administration
documentation for implementation guidance:

https://docs.openshift.com/container-platform/3.11/admin_guide/manage_scc.html</description>
        <rationale>Usage of SCCs and RBAC may mitigate the risks of using privileged containers by restricting
workloads to minimal system calls and resource access.</rationale>
        <ref cis="1.6.9"/>
        <oval id="general_scc_for_privileged_containers"/>
        <ocil clause="privileges are not minimized">Determine all SCCs that allow privileged containers:
<pre>$ oc get scc -ocustom-columns=NAME:.metadata.name,ALLOWS_PRIVILEGED:.allowPrivilegedContainer</pre>

Review users and groups assigned to SCCs allowing privileged containers:
<pre>$ oc describe sccs <i>scc-id</i></pre>

Adjust or assign SCCs or create custom SCCs as needed to minimize privileges.</ocil>
      </Rule>
    </Group>
    <Group id="kubelet" prodtype="ocp3">
      <title>Kubernetes Kubelet Settings</title>
      <description>The Kubernetes Kubelet is an agent that runs on each node in the cluster. It
makes sure that containers are running in a pod.

The kubelet takes a set of PodSpecs that are provided through various
mechanisms and ensures that the containers described in those PodSpecs are
running and healthy. The kubelet doesn&#x2019;t manage containers which were not
created by Kubernetes.</description>
      <Value id="var_kube_authorization_mode" type="string">
        <title>kubelet - Authorization Options</title>
        <description>ABAC - Attribute-Based Access Control (ABAC) mode allows you to configure policies using local files.
<br/>RBAC - Role-based access control (RBAC) mode allows you to create and store policies using the Kubernetes API.
<br/>Webhook - WebHook is an HTTP callback mode that allows you to manage authorization using a remote REST endpoint.
<br/>Node Node - authorization is a special-purpose authorization mode that specifically authorizes API requests made by kubelets.
<br/>AlwaysDeny - This flag blocks all requests. Use this flag only for testing.</description>
        <value selector="node">Node</value>
        <value selector="abac">ABAC</value>
        <value selector="rbac">RBAC</value>
        <value>Webhook</value>
        <value selector="webhook">Webhook</value>
        <value selector="alwaysdeny">AlwaysDeny</value>
      </Value>
      <Value id="var_streaming_connection_timeouts" interactive="true" type="string">
        <title>Streaming Connection Timeout Options</title>
        <description>Time until connection timeouts. Use (s) for seconds, (m) for minutes,
and (h) for hours.</description>
        <value selector="10min">10m</value>
        <value>5m</value>
        <value selector="30min">30m</value>
        <value selector="5min">5m</value>
        <value selector="2hours">2h</value>
        <value selector="6hours">6h</value>
        <value selector="4hours">4h</value>
        <value selector="1hour">1h</value>
        <value selector="8hours">8h</value>
      </Value>
      <Rule id="kubelet_configure_client_ca" prodtype="ocp3" severity="medium">
        <title>kubelet - Configure the Client CA Certificate</title>
        <description>By default, the kubelet is not configured with a CA certificate which
can subject the kubelet to man-in-the-middle attacks.

To configure a client CA certificate, edit the kubelet configuration
file <tt>/etc/origin/node/node-config.yaml</tt>
on the kubelet node(s) and set the below parameter:
<pre>servingInfo:
  clientCA: client-ca.crt</pre></description>
        <rationale>Not having a CA certificate for the kubelet will subject the kubelet to possible
man-in-the-middle attacks especially on unsafe or untrusted networks.
Certificate validation for the kubelet allows the API server to validate
the kubelet's identity. </rationale>
        <ident cce="80594-5"/>
        <ref cis="2.1.4"/>
        <oval id="kubelet_configure_client_ca"/>
        <ocil clause="no client CA certificate has been configured">Run the following command on the kubelet node(s):
<pre>$ sudo grep client-ca.crt /etc/origin/node/node-config.yaml</pre>
The output should something similar to <tt>client-ca.crt</tt>.</ocil>
      </Rule>
      <Rule id="kubelet_disable_readonly_port" prodtype="ocp3" severity="medium">
        <title>kubelet - Disable the Read-Only Port</title>
        <description>To disable the read-only port, edit the kubelet configuration
file <tt>/etc/origin/node/node-config.yaml</tt>
on the kubelet node(s) and set the below parameter:
<pre>kubeletArguments:
  read-only-port:
  - '0'</pre></description>
        <rationale>OpenShift disables the read-only port (<tt>10255</tt>) on all nodes by setting the
<tt>read-only-port</tt> kubelet flag to <tt>0</tt>. This ensures only
authenticated connections are able to receive information about the OpenShift
system.</rationale>
        <ident cce="80601-8"/>
        <ref cis="2.1.5"/>
        <oval id="kubelet_disable_readonly_port"/>
        <ocil clause="the read-only port is not disabled">Run the following command on the kubelet node(s):
<pre>$ sudo grep -A1 read-only-port /etc/origin/node/node-config.yaml</pre>
The output should be <tt>0</tt>.</ocil>
      </Rule>
      <Rule id="kubelet_enable_iptables_util_chains" prodtype="ocp3" severity="medium">
        <title>kubelet - Allow Automatic Firewall Configuration</title>
        <description>The kubelet has the ability to automatically configure the firewall to allow
the containers required ports and connections to networking resources and destinations
parameters potentially creating a security incident.
To allow the kubelet to modify the firewall, edit the kubelet configuration
file <tt>/etc/origin/node/node-config.yaml</tt>
on the kubelet node(s) and set the below parameter:
<pre>kubeletArguments:
  make-iptables-util-chains:
  - 'true'</pre></description>
        <rationale>The kubelet should automatically configure the firewall settings to allow access and
networking traffic through. This ensures that when a pod or container is running that
the correct ports are configured as well as removing the ports when a pod or
container is no longer in existence. </rationale>
        <ident cce="80604-2"/>
        <ref cis="2.1.8"/>
        <oval id="kubelet_enable_iptables_util_chains"/>
        <ocil clause="the kubelet cannot modify the firewall settings">Run the following command on the kubelet node(s):
<pre>$ sudo grep -A1 make-iptables-util-chains /etc/origin/node/node-config.yaml</pre>
The output should return <tt>true</tt>.</ocil>
        <warning category="management">IT IS NOT RECOMMENDED FOR ANY REASON to manually configure firewall ports for running
pods and containers as this not only can create conflicts with firewall rules but can
also introduce inconsistences into the allowed/disabled ports on the firewall.</warning>
      </Rule>
      <Rule id="kubelet_enable_server_cert_rotation" prodtype="ocp3" severity="medium">
        <title>kubelet - Enable Server Certificate Rotation</title>
        <description>To enable the kubelet to rotate server certificates, edit the kubelet configuration
file <tt>/etc/origin/node/node-config.yaml</tt>
on the kubelet node(s) and ensure <tt>RotateKubeletServerCertificate</tt> is included
in the <tt>feature-gates</tt> arguments:
<pre>kubeletArguments:
  feature-gates:
    - RotateKubeletClientCertificate=true,RotateKubeletServerCertificate=true</pre></description>
        <rationale>Allowing the kubelet to auto-update the certificates ensure that there is no downtime
in certificate renewal as well as ensures confidentiality and integrity.</rationale>
        <ident cce="80606-7"/>
        <ref cis="1.3.7"/>
        <oval id="kubelet_enable_server_cert_rotation"/>
        <ocil clause="the kubelet cannot rotate server certificate">Run the following command on the kubelet node(s):
<pre>$ sudo grep RotateKubeletServerCertificate /etc/origin/node/node-config.yaml</pre>
The output should return <tt>true</tt>.</ocil>
      </Rule>
      <Rule id="kubelet_configure_tls_cert" prodtype="ocp3" severity="medium">
        <title>Ensure That The kubelet Client Certificate Is Correctly Set</title>
        <description>To ensure the kubelet TLS client certificate is configured, edit the
kubelet configuration file <tt>/etc/origin/node/node-config.yaml</tt>
and configure the <tt>cert-dir</tt> path for the kubelet certificates.
For example:
<pre>  cert-dir:
  - /etc/origin/node/certificates</pre>
A corresponding certificate should exist in the <tt>cert-dir</tt>. For
example:
<pre>/etc/origin/node/certificates/kubelet-client-current.pem</pre></description>
        <rationale>Without cryptographic integrity protections, information can be
altered by unauthorized users without detection.</rationale>
        <ident cce="80596-0"/>
        <ref cis="2.1.12"/>
        <oval id="kubelet_configure_tls_cert"/>
        <ocil clause="the kubelet certificate is not configured">Run the following command on the kubelet node(s):
<pre>$ sudo grep -A1 cert-dir /etc/origin/node/node-config.yaml</pre>
Verify that a certificate path is configured.
<pre>$ sudo ls /etc/origin/node/certificates/kubelet-client-current.pem</pre>
Verify that a client certificate is configured.</ocil>
      </Rule>
      <Rule id="kubelet_enable_streaming_connections" prodtype="ocp3" severity="medium">
        <title>kubelet - Do Not Disable Streaming Timeouts</title>
        <description>Timouts for streaming connections should not be disabled as they help to prevent
denial-of-service attacks.
To configure streaming connection timeouts, edit the kubelet configuration
file <tt>/etc/origin/node/node-config.yaml</tt>
on the kubelet node(s) and set the below parameter:
<pre>kubeletArguments:
  streaming-connection-idle-timeout:
  - '<sub idref="var_streaming_connection_timeouts"/>'</pre></description>
        <rationale>Ensuring connections have timeouts helps to protect against denial-of-service attacks as
well as disconnect inactive connections. In addition, setting connections timeouts helps
to prevent from running out of ephemeral ports. </rationale>
        <ident cce="80607-5"/>
        <ref cis="2.1.6"/>
        <oval id="kubelet_enable_streaming_connections"/>
        <ocil clause="the streaming connection timeouts are not disabled">Run the following command on the kubelet node(s):
<pre>$ sudo grep -A1 streaming-connection-idle-timeout /etc/origin/node/node-config.yaml</pre>
The output should return <tt><sub idref="var_streaming_connection_timeouts"/></tt>.</ocil>
      </Rule>
      <Rule id="kubelet_configure_event_creation" prodtype="ocp3" severity="medium">
        <title>kubelet - Do Not Limit Event Creation</title>
        <description>All events should be captured and not restricted as this helps in
reconstucting the chain-of-events.

To prevent log creation limiting, edit the kubelet configuration
file <tt>/etc/origin/node/node-config.yaml</tt>
on the kubelet node(s) and set the below parameter:
<pre>kubeletArguments:
  event-qps:
  - '0'</pre></description>
        <rationale>All events should be captured and not restricted as this helps in
reconstucting the chain-of-events.</rationale>
        <ident cce="80595-2"/>
        <ref cis="2.1.11"/>
        <oval id="kubelet_configure_event_creation"/>
        <ocil clause="events are limited">Run the following command on the kubelet node(s):
<pre>$ sudo grep -A1 event-qps /etc/origin/node/node-config.yaml</pre>
The output should return <tt>0</tt>.</ocil>
      </Rule>
      <Rule id="kubelet_configure_tls_key" prodtype="ocp3" severity="medium">
        <title>Ensure That The kubelet Server Key Is Correctly Set</title>
        <description>To ensure the kubelet TLS server key certificate is configured, edit the
kubelet configuration file <tt>/etc/origin/node/node-config.yaml</tt>
and configure the <tt>cert-dir</tt> path for the kubelet certificates.
For example:
<pre>  cert-dir:
  - /etc/origin/node/certificates</pre>
A corresponding certificate should exist in the <tt>cert-dir</tt>. For
example:
<pre>/etc/origin/node/certificates/kubelet-server-current.pem</pre></description>
        <rationale>Without cryptographic integrity protections, information can be
altered by unauthorized users without detection.</rationale>
        <ident cce="80597-8"/>
        <ref cis="2.1.12"/>
        <oval id="kubelet_configure_tls_key"/>
        <ocil clause="the kubelet server key certificate is not configured">Run the following command on the kubelet node(s):
<pre>$ sudo grep -A1 cert-dir /etc/origin/node/node-config.yaml</pre>
Verify that a certificate path is configured.
<pre>$ sudo ls /etc/origin/node/certificates/kubelet-server-current.pem</pre>
Verify that a server key certificate is configured.</ocil>
      </Rule>
      <Rule id="kubelet_disable_hostname_override" prodtype="ocp3" severity="medium">
        <title>kubelet - Disable Hostname Override</title>
        <description>To prevent the hostname from being overrided, edit the kubelet configuration file
<tt>/etc/origin/node/node-config.yaml</tt> on the kubelet node(s) and
remove the <tt>hostname-override</tt> option if it exists.</description>
        <rationale>Allowing hostnames to be overrided creates issues around resolving nodes
in addition to TLS configuration, certificate validation, and log correlation
and validation. </rationale>
        <ident cce="80600-0"/>
        <ref cis="2.1.10"/>
        <oval id="kubelet_disable_hostname_override"/>
        <ocil clause="the hostname cannot be overrided">Run the following command on the kubelet node(s):
<pre>$ sudo grep hostname-override /etc/origin/node/node-config.yaml</pre>
The output should return no output.</ocil>
      </Rule>
      <Rule id="kubelet_enable_client_cert_rotation" prodtype="ocp3" severity="medium">
        <title>kubelet - Enable Client Certificate Rotation</title>
        <description>To enable the kubelet to rotate client certificates, edit the kubelet configuration
file <tt>/etc/origin/node/node-config.yaml</tt>
on the kubelet node(s) and set the below parameter:
<pre>kubeletArguments:
  feature-gates:
    - RotateKubeletClientCertificate=true</pre></description>
        <rationale>Allowing the kubelet to auto-update the certificates ensure that there is no downtime
in certificate renewal as well as ensures confidentiality and integrity.</rationale>
        <ident cce="80603-4"/>
        <ref cis="2.1.14"/>
        <oval id="kubelet_enable_client_cert_rotation"/>
        <ocil clause="the kubelet cannot rotate client certificate">Run the following command on the kubelet node(s):
<pre>$ sudo grep RotateKubeletClientCertificate /etc/origin/node/node-config.yaml</pre>
The output should return <tt>true</tt>.</ocil>
      </Rule>
      <Rule id="kubelet_disable_cadvisor_port" prodtype="ocp3" severity="high">
        <title>kubelet - Disable cAdvisor Port</title>
        <description>The <tt>cAdvisor</tt> port should be disabled as it does not require
any authentication to connect to the port.
To disable the <tt>cAdvisor</tt> port, edit the kubelet configuration
file <tt>/etc/origin/node/node-config.yaml</tt>
on the kubelet node(s) and set the below parameter:
<pre>kubeletArguments:
  cadvisor-port:
  - '0'</pre></description>
        <rationale>Any form of authentication to ports anonymously should be disabled. An attacker
could connect to the port and gain cluster information anonymously.</rationale>
        <ident cce="80599-4"/>
        <ref cis="2.1.13"/>
        <oval id="kubelet_disable_cadvisor_port"/>
        <ocil clause="the cAdvisor port is not disabled">Run the following command on the kubelet node(s):
<pre>$ sudo grep -A1 cadvisor-port /etc/origin/node/node-config.yaml</pre>
The output should return <tt>0</tt>.</ocil>
      </Rule>
    </Group>
    <Group id="controller" prodtype="ocp3">
      <title>OpenShift Controller Settings</title>
      <description>This section contains recommendations for the kube-controller-manager configuration</description>
      <Rule id="controller_disable_profiling" prodtype="ocp3" severity="low">
        <title>Verify Controller Profiling is not Exposed to the Web</title>
        <description>Profiling endpoints are exposed at each master port and secured via Role-Based
Access Control (RBAC). By default profiling is accessible only to users bound to
<tt>cluster-admin</tt> or <tt>cluster-debugger</tt> roles, limiting access to authorized
users only.

Should <tt>OPENSHIFT_PROFILE</tt> be set to <tt>web</tt>, reflecting a change from
the secure defaults, this profiling data will be exposed via a web interface on the
systems localhost interface.

To ensure profiling data is not exposed over a web interface, ensure
<tt>OPENSHIFT_PROFILE</tt> is not set to <tt>web</tt> in <tt>/etc/origin/master/master.env</tt>.</description>
        <rationale>Profiling data may include sensitive system information which could be exploited.</rationale>
        <ident cce="83000-0"/>
        <ref cis="1.1.8"/>
        <oval id="controller_disable_profiling"/>
        <ocil clause="&lt;tt&gt;OPENSHIFT_PROFILE&lt;/tt&gt; is configured to &lt;tt&gt;web&lt;/tt&gt;">Verify that <tt>OPENSHIFT_PROFILE</tt> is not set to <tt>web</tt> by running the following command:
<pre>$ grep OPENSHIFT_PROFILE /etc/origin/master/master.env</pre>

Verify that <tt>OPENSHIFT_PROFILE=web</tt> is NOT returned.</ocil>
      </Rule>
      <Rule id="controller_bind_address" prodtype="ocp3" severity="low">
        <title>Ensure Controller bind-address argument is set</title>
        <description>To ensure the Controller Manager service is bound to secure loopback
address, edit the Controller Manager pod specification file
<tt>/etc/origin/master/master-config.yaml</tt> on the
master node(s) and ensure the correct value for the <tt>bind-address</tt>
parameter. For example:
<pre>kubernetesMasterConfig:
  controllerArguments:
    bind-address:
      - '192.168.1.101'</pre></description>
        <rationale>The Controller Manager API service (which runs on a port specified by the
<tt>secure-port</tt> argument) is used for health and metrics
information and is available without authentication or encryption. As such it
should only be bound to a localhost interface, to minimize the cluster's
attack surface.</rationale>
        <ident cce="80587-9"/>
        <ref cis="1.3.7"/>
        <oval id="controller_bind_address"/>
        <ocil clause="&lt;tt&gt;bind-address&lt;/tt&gt; is not configured to a secure IP address">Run the following command on the master node(s):
<pre>$ sudo grep bind-address /etc/origin/master/master-config.yaml</pre>

Verify that the <tt>bind-address</tt> argument is set to a secure
IP address.</ocil>
        <warning category="functionality">The associated value must be reachable by the rest of the cluster, and by
CLI/web clients. If blank all interfaces will be used (<tt>0.0.0.0</tt> for IPv4
and ``::`` for IPv6).</warning>
      </Rule>
      <Rule id="controller_use_service_account" prodtype="ocp3" severity="medium">
        <title>Ensure that the --use-service-account-credentials argument is set</title>
        <description>To ensure individual service account credentials are used,
edit the Controller Manager pod specification file
<tt>/etc/origin/master/master-config.yaml</tt> on the
master node(s) and set the <tt>use-service-account-credentials</tt> option,
under the <tt>controllerArguments</tt> stanza, to <tt>true</tt>. For example:
<pre>kubernetesMasterConfig:
  controllerArguments:
    use-service-account-credentials:
    - true</pre></description>
        <rationale>The controller manager creates a service account per controller in
<tt>kube-system</tt> namespace, generates an API token and credentials for it,
then builds a dedicated API client with that service account credential
for each controller loop to use. Setting the
<tt>use-service-account-credentials</tt> to <tt>true</tt> runs each
control loop within the contoller manager using a separate service
account credential. When used in combination with RBAC, this ensures
that the control loops run with the minimum permissions required to
perform their intended tasks.</rationale>
        <ident cce="80593-7"/>
        <ref cis="1.3.3"/>
        <oval id="controller_use_service_account"/>
        <ocil clause="&lt;tt&gt;use-service-account-credentials&lt;/tt&gt; is set to &lt;tt&gt;false&lt;/tt&gt;">Run the following command on the master node(s):
<pre>$ sudo grep service-account-credentials /etc/origin/master/master-config.yaml</pre> 

Verify that the <tt>controllerArguments</tt> option
<tt>use-service-account-credentials</tt> argument is not set to
<tt>false</tt>.

If the configuration file does not explicitly set 
<tt>use-service-account-credentials</tt>, and the grep command returns no
text, the default value of <tt>true</tt> is being used.</ocil>
      </Rule>
      <Rule id="controller_terminated_pod_gc_threshhold" prodtype="ocp3" severity="low">
        <title>Enable terminated-pod-gc-threshold for the Controller Manager</title>
        <description>To ensure the garbage collector is activated upon pod termination,
edit the Controller Manager pod specification file
<tt>/etc/origin/master/master-config.yaml</tt> on the
master node(s) and set the <tt>terminated-pod-gc-threshold</tt> to
<tt>true</tt>. For example:
<pre>kubernetesMasterConfig:
  controllerArguments:
    terminated-pod-gc-threshold:
      - true</pre></description>
        <rationale>Garbage collection is important to ensure sufficient resource availability
and avoiding degraded performance and availability. In the worst case,
the system might crash or just be unusable for a long period of time. The
default setting for garbage collection is 12,500 terminated pods which
might be to high for your system to sustain. Based on your system resources
and tests, choose an appropriate threshold value to activate garbage
collection.</rationale>
        <ident cce="80592-9"/>
        <ref cis="1.3.1"/>
        <oval id="controller_terminated_pod_gc_threshhold"/>
        <ocil clause="&lt;tt&gt;terminated-pod-gc-threshold&lt;/tt&gt; is not enabled">Run the following command on the master node(s):
<pre>$ sudo grep terminated-pod-gc-threshold /etc/origin/master/master-config.yaml</pre>

Verify that the <tt>terminated-pod-gc-threshold</tt> argument is not set
to <tt>false</tt>.

If the configuration file does not explicitly set
<tt>terminated-pod-gc-threshold</tt>, and the grep command returns no
text, the default value of <tt>true</tt> is being used.</ocil>
      </Rule>
      <Rule id="controller_rotate_kubelet_server_certs" prodtype="ocp3" severity="medium">
        <title>Ensure that the RotateKubeletServerCertificate argument is set</title>
        <description>To enforce kublet server certificate rotation on the Controller Manager,
edit the Controller Manager pod specification file
<tt>/etc/origin/master/master-config.yaml</tt> on the
master node(s) and set the <tt>controllerArguments</tt> parameter to include
<tt>RotateKubeletServerCertificate=true</tt>. For example:
<pre>kubernetesMasterConfig:
  controllerArguments:
    feature-gates:
      - RotateKubeletServerCertificate=true</pre></description>
        <rationale><tt>RotateKubeletServerCertificate</tt> causes the kubelet to both request
a serving certificate after bootstrapping its client credentials and rotate the
certificate as its existing credentials expire. This automated periodic rotation
ensures that there are no downtimes due to expired certificates and thus
addressing the availability in the C/I/A security triad.

NOTE: This recommendation only applies if you let kubelets get their
certificates from the API Server. In case your certificates come from an
outside Certificate Authority/tool (e.g. Vault) then you need to take care
of rotation yourself.</rationale>
        <ident cce="80590-3"/>
        <ref cis="1.3.6"/>
        <oval id="controller_rotate_kubelet_server_certs"/>
        <ocil clause="&lt;tt&gt;RotateKubeletServerCertificate&lt;/tt&gt; argument is set to &lt;tt&gt;false&lt;/tt&gt; in the&#10;&lt;tt&gt;controllerArguments&lt;/tt&gt; options">Run the following command on the master node(s):
<pre>$ grep RotateKubeletServerCertificate</pre>

The output should return <tt>true</tt>.</ocil>
      </Rule>
    </Group>
    <Group id="etcd" prodtype="ocp3">
      <title>OpenShift etcd Settings</title>
      <description>Contains rules that check correct OpenShift etcd settings.</description>
      <Rule id="etcd_wal_dir" prodtype="ocp3" severity="medium">
        <title>Configure etcd Log Storage</title>
        <description>To ensure the <tt>etcd</tt> service is storing logs separate
from data, set
<tt>ETCD_WAL_DIR</tt> to <tt>/var/lib/etcd/member/wal</tt>
in <tt>/etc/etcd/etcd.conf</tt> on the master node:
<pre>ETCD_WAL_DIR=/var/lib/etcd/member/wal</pre></description>
        <rationale>etcd log files should be stored in a separate location from the etcd data.
This not only ensures data integrity but also helps to prevent IO degradation.</rationale>
        <ident cce="80586-1"/>
        <ref cis="1.5.7"/>
        <oval id="etcd_wal_dir"/>
        <ocil clause="the etcd logs are not configured in a separate location">Run the following command on the master node(s):
<pre>$ grep ETCD_WAL_DIR /etc/etcd/etcd.conf</pre>
The output should return <tt>/var/lib/etcd/member/wal</tt>.</ocil>
      </Rule>
      <Rule id="etcd_client_cert_auth" prodtype="ocp3" severity="medium">
        <title>Enable The Client Certificate Authentication</title>
        <description>To ensure the <tt>etcd</tt> service is serving TLS to clients,
edit the <tt>etcd</tt> configuration file
<tt>/etc/etcd/etcd.conf</tt> on the master node and set
<tt>ETCD_CLIENT_CERT_AUTH</tt> to <tt>true</tt>.
<pre>ETCD_CLIENT_CERT_AUTH=true</pre></description>
        <rationale>Without cryptographic integrity protections, information can be
altered by unauthorized users without detection.</rationale>
        <ident cce="80579-6"/>
        <ref cis="1.5.2"/>
        <oval id="etcd_client_cert_auth"/>
        <ocil clause="the etcd client certificate authentication is not configured">Run the following command on the master node(s):
<pre>$ grep ETCD_CLIENT_CERT_AUTH</pre>
The output should return <tt>true</tt>.</ocil>
      </Rule>
      <Rule id="etcd_peer_key_file" prodtype="ocp3" severity="medium">
        <title>Ensure That The etcd Peer Key File Is Correctly Set</title>
        <description>To ensure the <tt>etcd</tt> service is serving TLS to clients,
edit the <tt>etcd</tt> configuration file
<tt>/etc/etcd/etcd.conf</tt> on the master on the master and
adding a key file to <tt>ETCD_PEER_KEY_FILE</tt>:
<pre>ETCD_PEER_KEY_FILE=/etc/etcd/peer.key</pre></description>
        <rationale>Without cryptographic integrity protections, information can be
altered by unauthorized users without detection.</rationale>
        <ident cce="80581-2"/>
        <ref cis="1.5.4"/>
        <oval id="etcd_peer_key_file"/>
        <ocil clause="the etcd client key file is not configured">Run the following command on the master node(s):
<pre>$ grep ETCD_PEER_KEY_FILE=/etc/etcd/etcd.conf</pre>
Verify that there is a key file configured.</ocil>
      </Rule>
      <Rule id="etcd_peer_cert_file" prodtype="ocp3" severity="medium">
        <title>Ensure That The etcd Peer Client Certificate Is Correctly Set</title>
        <description>To ensure the <tt>etcd</tt> service is serving TLS to clients,
edit the <tt>etcd</tt> configuration file
<tt>/etc/etcd/etcd.conf</tt> on the master and adding a certificate
to <tt>ETCD_PEER_CERT_FILE</tt>:
<pre>ETCD_PEER_CERT_FILE=/etc/etcd/peer.crt</pre></description>
        <rationale>Without cryptographic integrity protections, information can be
altered by unauthorized users without detection.</rationale>
        <ident cce="80580-4"/>
        <ref cis="1.5.4"/>
        <oval id="etcd_peer_cert_file"/>
        <ocil clause="the etcd client certificate is not configured">Run the following command on the master node(s):
<pre>$ grep ETCD_PEER_CERT_FILE=/etc/etcd/etcd.conf</pre>
Verify that there is a certificate configured.</ocil>
      </Rule>
      <Rule id="etcd_peer_auto_tls" prodtype="ocp3" severity="medium">
        <title>Disable etcd Peer Self-Signed Certificates</title>
        <description>To ensure the <tt>etcd</tt> service is not using self-signed
certificates, edit the <tt>etcd</tt> configuration file
<tt>/etc/etcd/etcd.conf</tt> from the master node and set
<tt>ETCD_PEER_AUTO_TLS</tt> to <tt>false</tt>:
<pre>ETCD_PEER_AUTO_TLS=false</pre></description>
        <rationale>Without cryptographic integrity protections, information can be
altered by unauthorized users without detection. Using self-signed
certificates ensures that the certificates are never validated
against a certificate authority and could lead to compromised
and invalidated data.</rationale>
        <ident cce="80583-8"/>
        <ref cis="1.5.6"/>
        <oval id="etcd_peer_auto_tls"/>
        <ocil clause="the etcd is using peer self-signed certificates">Run the following command on the master node(s):
<pre>$ grep ETCD_PEER_AUTO_TLS /etc/etcd/etcd.conf</pre>
The output should return <tt>false</tt>.</ocil>
      </Rule>
      <Rule id="etcd_unique_ca" prodtype="ocp3" severity="medium">
        <title>Configure A Unique CA Certificate for etcd</title>
        <description>A unique and different CA certificate should be created for <tt>etcd</tt>.
To ensure the <tt>etcd</tt> service is using a unique certificate,
, set <tt>ETCD_TRUSTED_CA_FILE</tt> to <tt>/etc/etcd/ca.crt</tt>
in <tt>/etc/etcd/etcd.conf</tt> on the master node that does NOT match
the OpenShift CA certificate:
<pre>ETCD_TRUSTED_CA_FILE=/etc/etcd/ca.crt</pre></description>
        <rationale>A unique CA certificate that is utilized by etcd and is different from
OpenShift ensures that the etcd data is still protected in the event that
the OpenShift certificate is compromised.</rationale>
        <ident cce="80585-3"/>
        <ref cis="1.5.9"/>
        <oval id="etcd_unique_ca"/>
        <ocil clause="the etcd CA certificate is not unique">Run the following command on the master node(s):
<pre>$ grep ETCD_TRUSTED_CA_FILE /etc/etcd/etcd.conf</pre>
Inspect the certificate file that is returned and verify that it
does not match the OpenShift CA certificate.</ocil>
      </Rule>
      <Rule id="etcd_max_wals" prodtype="ocp3" severity="medium">
        <title>Disable etcd Auto Log Rotation</title>
        <description>To ensure the <tt>etcd</tt> service is not auto-rotating logs,
edit the <tt>etcd</tt> configuration file 
<tt>/etc/etcd/etcd.conf</tt> on the master node and set
<tt>ETCD_MAX_WALS</tt> to <tt>0</tt>:
<pre>ETCD_MAX_WALS=0</pre></description>
        <rationale>Ensure data integrity by preventing logs from being overwritten which
allows reconstructing events should the data be compromised. </rationale>
        <ident cce="80584-6"/>
        <ref cis="1.5.8"/>
        <oval id="etcd_max_wals"/>
        <ocil clause="the etcd auto log rotation is not disabled">Run the following command on the master node(s):
<pre>$ grep ETCD_MAX_WALS /etc/etcd/etcd.conf</pre>
The output should return <tt>0</tt>.</ocil>
      </Rule>
      <Rule id="etcd_peer_client_cert_auth" prodtype="ocp3" severity="medium">
        <title>Enable The Peer Client Certificate Authentication</title>
        <description>To ensure the <tt>etcd</tt> service is serving TLS to clients,
edit the <tt>etcd</tt> configuration file
<tt>/etc/etcd/etcd.conf</tt> on the master node and set
<tt>ETCD_PEER_CLIENT_CERT_AUTH</tt> to <tt>true</tt>.
<pre>ETCD_PEER_CLIENT_CERT_AUTH=true</pre></description>
        <rationale>Without cryptographic integrity protections, information can be
altered by unauthorized users without detection.</rationale>
        <ident cce="80582-0"/>
        <ref cis="1.5.5"/>
        <oval id="etcd_peer_client_cert_auth"/>
        <ocil clause="the etcd peer client certificate authentication is not configured">Run the following command on the master node(s):
<pre>$ grep ETCD_PEER_CLIENT_CERT_AUTH</pre>
The output should return <tt>true</tt>.</ocil>
      </Rule>
      <Rule id="etcd_cert_file" prodtype="ocp3" severity="medium">
        <title>Ensure That The etcd Client Certificate Is Correctly Set</title>
        <description>To ensure the <tt>etcd</tt> service is serving TLS to clients,
edit the <tt>etcd</tt> configuration file
<tt>/etc/etcd/etcd.conf</tt> on the master and adding a certificate
to <tt>ETCD_CERT_FILE</tt>:
<pre>ETCD_CERT_FILE=/etc/etcd/server.crt</pre></description>
        <rationale>Without cryptographic integrity protections, information can be
altered by unauthorized users without detection.</rationale>
        <ident cce="80577-0"/>
        <ref cis="1.5.1"/>
        <oval id="etcd_cert_file"/>
        <ocil clause="the etcd client certificate is not configured">Run the following command on the master node(s):
<pre>$ grep ETCD_CERT_FILE=/etc/etcd/etcd.conf</pre>
Verify that there is a certificate configured.</ocil>
      </Rule>
      <Rule id="etcd_auto_tls" prodtype="ocp3" severity="medium">
        <title>Disable etcd Self-Signed Certificates</title>
        <description>To ensure the <tt>etcd</tt> service is not using self-signed
certificates, edit the <tt>etcd</tt> configuration file
<tt>/etc/etcd/etcd.conf</tt> from the master node and set
<tt>ETCD_AUTO_TLS</tt> to <tt>false</tt>:
<pre>ETCD_AUTO_TLS=false</pre></description>
        <rationale>Without cryptographic integrity protections, information can be
altered by unauthorized users without detection. Using self-signed
certificates ensures that the certificates are never validated
against a certificate authority and could lead to compromised
and invalidated data.</rationale>
        <ident cce="80573-9"/>
        <ref cis="1.5.3"/>
        <oval id="etcd_auto_tls"/>
        <ocil clause="the etcd is using self-signed certificates">Run the following command on the master node(s):
<pre>$ grep ETCD_AUTO_TLS /etc/etcd/etcd.conf</pre>
The output should not return <tt>true</tt>.</ocil>
      </Rule>
      <Rule id="etcd_key_file" prodtype="ocp3" severity="medium">
        <title>Ensure That The etcd Key File Is Correctly Set</title>
        <description>To ensure the <tt>etcd</tt> service is serving TLS to clients,
edit the <tt>etcd</tt> configuration file
<tt>/etc/etcd/etcd.conf</tt> on the master on the master and
adding a key file to <tt>ETCD_KEY_FILE</tt>:
<pre>ETCD_KEY_FILE=/etc/etcd/server.key</pre></description>
        <rationale>Without cryptographic integrity protections, information can be
altered by unauthorized users without detection.</rationale>
        <ident cce="80578-8"/>
        <ref cis="1.5.1"/>
        <oval id="etcd_key_file"/>
        <ocil clause="the etcd client key file is not configured">Run the following command on the master node(s):
<pre>$ grep ETCD_KEY_FILE=/etc/etcd/etcd.conf</pre>
Verify that there is a key file configured.</ocil>
      </Rule>
    </Group>
    <Group id="ocp-permissions" prodtype="ocp3,ocp4">
      <title>Permissions</title>
      <description>Traditional security relies heavily on file and
directory permissions to prevent unauthorized users from reading or
modifying files to which they should not have access.</description>
      <Group id="ocp-files" prodtype="ocp3,ocp4">
        <title>Verify Permissions on Important Files and
Directories</title>
        <description>Permissions for many files on a system must be set
restrictively to ensure sensitive information is properly protected.
This section discusses important
permission restrictions which can be verified
to ensure that no harmful discrepancies have
arisen.</description>
        <Rule id="file_owner_node_config" prodtype="ocp3" severity="medium">
          <title>Verify User Who Owns The OpenShift Node Configuration File</title>
          <description> To properly set the owner of <code>/etc/origin/node/node-config.yaml</code>, run the command: <pre>$ sudo chown root /etc/origin/node/node-config.yaml </pre></description>
          <rationale>The <tt>/etc/origin/node/node-config.yaml</tt> file contains information about the configuration of the
OpenShift node that is configured on the system. Protection of this file is
critical for OpenShift security.</rationale>
          <ident cce="80629-9"/>
          <ref cis="2.2.2"/>
          <oval id="file_owner_node_config"/>
          <ocil clause="/etc/origin/node/node-config.yaml has owner root">To check the ownership of <code>/etc/origin/node/node-config.yaml</code>, run the command: <pre>$ ls -lL /etc/origin/node/node-config.yaml</pre> If properly configured, the output should indicate the following owner: <code>root</code></ocil>
        </Rule>
        <Rule id="file_groupowner_master_cni_conf" prodtype="ocp3" severity="medium">
          <title>Verify Group Who Owns The OpenShift Container Network Interface Files</title>
          <description> To properly set the group owner of <code>/etc/cni/net.d/*</code>, run the command: <pre>$ sudo chgrp root /etc/cni/net.d/*</pre></description>
          <rationale>CNI (Container Network Interface) files consist of a specification and libraries for
writing plugins to configure network interfaces in Linux containers, along with a number
of supported plugins. Allowing writeable access to the files could allow an attacker to modify
the networking configuration potentially adding a rouge network connection.</rationale>
          <ident cce="80611-7"/>
          <ref cis="1.4.10"/>
          <oval id="file_groupowner_master_cni_conf"/>
          <ocil clause="/etc/cni/net.d/* has group owner root">To check the group ownership of <code>/etc/cni/net.d/*</code>, run the command: <pre>$ ls -lL /etc/cni/net.d/*</pre> If properly configured, the output should indicate the following group-owner: <code>root</code></ocil>
        </Rule>
        <Rule id="file_permissions_etc_origin" prodtype="ocp3" severity="medium">
          <title>The OpenShift Configuration Directory Must Have Mode 0700</title>
          <description>
To properly set the permissions of <code>/etc/origin/</code>, run the command:
<pre>$ sudo chmod 0700 /etc/origin/</pre></description>
          <rationale>If users can modify the OpenShift configurations, the OpenShift cluster can become inoperable or compromised</rationale>
          <oval id="file_permissions_etc_origin"/>
          <ocil clause="/etc/origin/ has unix mode -rwx------">To check the permissions of <code>/etc/origin/</code>, run the command:
<pre>$ ls -l /etc/origin/</pre>
If properly configured, the output should indicate the following permissions:
<code>-rwx------</code></ocil>
        </Rule>
        <Rule id="file_groupowner_master_openvswitch" prodtype="ocp3" severity="medium">
          <title>Verify Group Who Owns The OpenShift Open vSwitch Files</title>
          <description> To properly set the group owner of <code>/etc/origin/openvswitch/*</code>, run the command: <pre>$ sudo chgrp root /etc/origin/openvswitch/*</pre></description>
          <rationale>CNI (Container Network Interface) files consist of a specification and libraries for
writing plugins to configure network interfaces in Linux containers, along with a number
of supported plugins. Allowing writeable access to the files could allow an attacker to modify
the networking configuration potentially adding a rouge network connection.</rationale>
          <ident cce="82172-8"/>
          <ref cis="1.4.10"/>
          <oval id="file_groupowner_master_openvswitch"/>
          <ocil clause="/etc/origin/openvswitch/* has group owner root">To check the group ownership of <code>/etc/origin/openvswitch/*</code>, run the command: <pre>$ ls -lL /etc/origin/openvswitch/*</pre> If properly configured, the output should indicate the following group-owner: <code>root</code></ocil>
        </Rule>
        <Rule id="file_groupowner_master_openshift_kubeconfig" prodtype="ocp3" severity="medium">
          <title>Verify Group Who Owns The OpenShift Master Kubeconfig File</title>
          <description> To properly set the group owner of <code>/etc/origin/master/openshift-master.kubeconfig</code>, run the command: <pre>$ sudo chgrp root /etc/origin/master/openshift-master.kubeconfig</pre></description>
          <rationale>The <tt>/etc/origin/master/openshift-master.kubeconfig</tt> file contains information about the master configuration of the
OpenShift cluster that is configured on the system. Protection of this file is
critical for OpenShift security.</rationale>
          <ident cce="80615-8"/>
          <ref cis="1.4.18"/>
          <oval id="file_groupowner_master_openshift_kubeconfig"/>
          <ocil clause="/etc/origin/master/openshift-master.kubeconfig has group owner root">To check the group ownership of <code>/etc/origin/master/openshift-master.kubeconfig</code>, run the command: <pre>$ ls -lL /etc/origin/master/openshift-master.kubeconfig</pre> If properly configured, the output should indicate the following group-owner: <code>root</code></ocil>
        </Rule>
        <Rule id="file_owner_master_etcd" prodtype="ocp3" severity="medium">
          <title>Verify User Who Owns The OpenShift etcd Specification File</title>
          <description> To properly set the owner of <code>/etc/origin/node/pods/etcd.yaml</code>, run the command: <pre>$ sudo chown root /etc/origin/node/pods/etcd.yaml </pre></description>
          <rationale>The <tt>/etc/origin/node/pods/etcd.yaml</tt> file contains information about the configuration of the
OpenShift etcd Server that is configured on the system. Protection of this file is
critical for OpenShift security.</rationale>
          <ident cce="80625-7"/>
          <ref cis="1.4.8"/>
          <oval id="file_owner_master_etcd"/>
          <ocil clause="/etc/origin/node/pods/etcd.yaml has owner root">To check the ownership of <code>/etc/origin/node/pods/etcd.yaml</code>, run the command: <pre>$ ls -lL /etc/origin/node/pods/etcd.yaml</pre> If properly configured, the output should indicate the following owner: <code>root</code></ocil>
        </Rule>
        <Rule id="file_groupowner_master_admin_conf" prodtype="ocp3" severity="medium">
          <title>Verify Group Who Owns The OpenShift Admin Kubeconfig File</title>
          <description> To properly set the group owner of <code>/etc/origin/master/admin.kubeconfig</code>, run the command: <pre>$ sudo chgrp root /etc/origin/master/admin.kubeconfig</pre></description>
          <rationale>The <tt>/etc/origin/master/admin.kubeconfig</tt> file contains information about the administrative configuration of the
OpenShift cluster that is configured on the system. Protection of this file is
critical for OpenShift security.</rationale>
          <ident cce="80610-9"/>
          <ref cis="1.4.14"/>
          <oval id="file_groupowner_master_admin_conf"/>
          <ocil clause="/etc/origin/master/admin.kubeconfig has group owner root">To check the group ownership of <code>/etc/origin/master/admin.kubeconfig</code>, run the command: <pre>$ ls -lL /etc/origin/master/admin.kubeconfig</pre> If properly configured, the output should indicate the following group-owner: <code>root</code></ocil>
        </Rule>
        <Rule id="file_permissions_openshift_node_service" prodtype="ocp3" severity="medium">
          <title>Verify Permissions on the OpenShift Node Service File</title>
          <description>
To properly set the permissions of <code>/etc/systemd/system/atomic-openshift-node.service</code>, run the command:
<pre>$ sudo chmod 0644 /etc/systemd/system/atomic-openshift-node.service</pre></description>
          <rationale>If the <tt>/etc/systemd/system/atomic-openshift-node.service</tt> file is writable by a group-owner or the
world the risk of its compromise is increased. The file contains the service configuration of the
OpenShift node service that is configured on the system. Protection of this file is
critical for OpenShift security.</rationale>
          <ident cce="80643-0"/>
          <ref cis="2.2.3"/>
          <oval id="file_permissions_openshift_node_service"/>
          <ocil clause="/etc/systemd/system/atomic-openshift-node.service has unix mode -rw-r--r--">To check the permissions of <code>/etc/systemd/system/atomic-openshift-node.service</code>, run the command:
<pre>$ ls -l /etc/systemd/system/atomic-openshift-node.service</pre>
If properly configured, the output should indicate the following permissions:
<code>-rw-r--r--</code></ocil>
        </Rule>
        <Rule id="file_groupowner_etc_origin" prodtype="ocp3" severity="medium">
          <title>Verify Group Who Owns The OpenShift Configuration Directory</title>
          <description> To properly set the group owner of <code>/etc/origin/</code>, run the command: <pre>$ sudo chgrp root /etc/origin/</pre></description>
          <rationale>If users can modify the OpenShift configurations, the OpenShift cluster can become inoperable or compromised</rationale>
          <oval id="file_groupowner_etc_origin"/>
          <ocil clause="/etc/origin/ has group owner root">To check the group ownership of <code>/etc/origin/</code>, run the command: <pre>$ ls -lL /etc/origin/</pre> If properly configured, the output should indicate the following group-owner: <code>root</code></ocil>
        </Rule>
        <Rule id="file_permissions_master_openvswitch" prodtype="ocp3" severity="medium">
          <title>Verify Permissions on the OpenShift Open vSwitch Files</title>
          <description>
To properly set the permissions of <code>/etc/origin/openvswitch/*</code>, run the command:
<pre>$ sudo chmod 0644 /etc/origin/openvswitch/*</pre></description>
          <rationale>CNI (Container Network Interface) files consist of a specification and libraries for
writing plugins to configure network interfaces in Linux containers, along with a number
of supported plugins. Allowing writeable access to the files could allow an attacker to modify
the networking configuration potentially adding a rouge network connection.</rationale>
          <ident cce="82173-6"/>
          <ref cis="1.4.9"/>
          <oval id="file_permissions_master_openvswitch"/>
          <ocil clause="/etc/origin/openvswitch/* has unix mode -rw-r--r--">To check the permissions of <code>/etc/origin/openvswitch/*</code>, run the command:
<pre>$ ls -l /etc/origin/openvswitch/*</pre>
If properly configured, the output should indicate the following permissions:
<code>-rw-r--r--</code></ocil>
        </Rule>
        <Rule id="file_permissions_master_cni_conf" prodtype="ocp3" severity="medium">
          <title>Verify Permissions on the OpenShift Container Network Interface Files</title>
          <description>
To properly set the permissions of <code>/etc/cni/net.d/*</code>, run the command:
<pre>$ sudo chmod 0644 /etc/cni/net.d/*</pre></description>
          <rationale>CNI (Container Network Interface) files consist of a specification and libraries for
writing plugins to configure network interfaces in Linux containers, along with a number
of supported plugins. Allowing writeable access to the files could allow an attacker to modify
the networking configuration potentially adding a rouge network connection.</rationale>
          <ident cce="80634-9"/>
          <ref cis="1.4.9"/>
          <oval id="file_permissions_master_cni_conf"/>
          <ocil clause="/etc/cni/net.d/* has unix mode -rw-r--r--">To check the permissions of <code>/etc/cni/net.d/*</code>, run the command:
<pre>$ ls -l /etc/cni/net.d/*</pre>
If properly configured, the output should indicate the following permissions:
<code>-rw-r--r--</code></ocil>
        </Rule>
        <Rule id="file_groupowner_node_config" prodtype="ocp3" severity="medium">
          <title>Verify Group Who Owns The OpenShift Node Configuration File</title>
          <description> To properly set the group owner of <code>/etc/origin/node/node-config.yaml</code>, run the command: <pre>$ sudo chgrp root /etc/origin/node/node-config.yaml</pre></description>
          <rationale>The <tt>/etc/origin/node/node-config.yaml</tt> file contains information about the configuration of the
OpenShift node that is configured on the system. Protection of this file is
critical for OpenShift security.</rationale>
          <ident cce="80617-4"/>
          <ref cis="2.2.2"/>
          <oval id="file_groupowner_node_config"/>
          <ocil clause="/etc/origin/node/node-config.yaml has group owner root">To check the group ownership of <code>/etc/origin/node/node-config.yaml</code>, run the command: <pre>$ ls -lL /etc/origin/node/node-config.yaml</pre> If properly configured, the output should indicate the following group-owner: <code>root</code></ocil>
        </Rule>
        <Rule id="file_owner_master_openvswitch" prodtype="ocp3" severity="medium">
          <title>Verify User Who Owns The OpenShift Open vSwitch Files</title>
          <description> To properly set the owner of <code>/etc/origin/openvswitch/*</code>, run the command: <pre>$ sudo chown root /etc/origin/openvswitch/* </pre></description>
          <rationale>CNI (Container Network Interface) files consist of a specification and libraries for
writing plugins to configure network interfaces in Linux containers, along with a number
of supported plugins. Allowing writeable access to the files could allow an attacker to modify
the networking configuration potentially adding a rouge network connection.</rationale>
          <ident cce="82171-0"/>
          <ref cis="1.4.10"/>
          <oval id="file_owner_master_openvswitch"/>
          <ocil clause="/etc/origin/openvswitch/* has owner root">To check the ownership of <code>/etc/origin/openvswitch/*</code>, run the command: <pre>$ ls -lL /etc/origin/openvswitch/*</pre> If properly configured, the output should indicate the following owner: <code>root</code></ocil>
        </Rule>
        <Rule id="file_groupowner_openshift_node_client_crt" prodtype="ocp3" severity="medium">
          <title>Verify Group Who Owns OpenShift Node Certificate File</title>
          <description> To properly set the group owner of <code>/etc/origin/node/client-ca.crt</code>, run the command: <pre>$ sudo chgrp root /etc/origin/node/client-ca.crt</pre></description>
          <rationale>The <tt>/etc/origin/node/client-ca.crt</tt> file contains the certificate authority
certificate for an OpenShift node that is configured on the system. Protection of this file is
critical for OpenShift security.</rationale>
          <ident cce="80619-0"/>
          <ref cis="2.2.8"/>
          <oval id="file_groupowner_openshift_node_client_crt"/>
          <ocil clause="/etc/origin/node/client-ca.crt has group owner root">To check the group ownership of <code>/etc/origin/node/client-ca.crt</code>, run the command: <pre>$ ls -lL /etc/origin/node/client-ca.crt</pre> If properly configured, the output should indicate the following group-owner: <code>root</code></ocil>
        </Rule>
        <Rule id="file_owner_master_admin_conf" prodtype="ocp3" severity="medium">
          <title>Verify User Who Owns The OpenShift Admin Kubeconfig File</title>
          <description> To properly set the owner of <code>/etc/origin/master/admin.kubeconfig</code>, run the command: <pre>$ sudo chown root /etc/origin/master/admin.kubeconfig </pre></description>
          <rationale>The <tt>/etc/origin/master/admin.kubeconfig</tt> file contains information about the administrative configuration of the
OpenShift cluster that is configured on the system. Protection of this file is
critical for OpenShift security.</rationale>
          <ident cce="80622-4"/>
          <ref cis="1.4.14"/>
          <oval id="file_owner_master_admin_conf"/>
          <ocil clause="/etc/origin/master/admin.kubeconfig has owner root">To check the ownership of <code>/etc/origin/master/admin.kubeconfig</code>, run the command: <pre>$ ls -lL /etc/origin/master/admin.kubeconfig</pre> If properly configured, the output should indicate the following owner: <code>root</code></ocil>
        </Rule>
        <Rule id="file_owner_node_kubeconfig" prodtype="ocp3" severity="medium">
          <title>Verify User Who Owns The OpenShift Node Kubeconfig File</title>
          <description> To properly set the owner of <code>/etc/origin/node/node.kubeconfig</code>, run the command: <pre>$ sudo chown root /etc/origin/node/node.kubeconfig </pre></description>
          <rationale>The <tt>/etc/origin/node/node.kubeconfig</tt> file contains information about the configuration of the
OpenShift node that is configured on the system. Protection of this file is
critical for OpenShift security.</rationale>
          <ident cce="80630-7"/>
          <ref cis="2.2.2"/>
          <oval id="file_owner_node_kubeconfig"/>
          <ocil clause="/etc/origin/node/node.kubeconfig has owner root">To check the ownership of <code>/etc/origin/node/node.kubeconfig</code>, run the command: <pre>$ ls -lL /etc/origin/node/node.kubeconfig</pre> If properly configured, the output should indicate the following owner: <code>root</code></ocil>
        </Rule>
        <Rule id="file_permissions_master_controller_manager" prodtype="ocp3" severity="medium">
          <title>Verify Permissions on the OpenShift Controller Manager Specification File</title>
          <description>
To properly set the permissions of <code>/etc/origin/node/pods/controller.yaml</code>, run the command:
<pre>$ sudo chmod 0600 /etc/origin/node/pods/controller.yaml</pre></description>
          <rationale>If the <tt>/etc/origin/node/pods/controller.yaml</tt> file is writable by a group-owner or the
world the risk of its compromise is increased. The file contains the configuration of
an OpenShift Controller Manager server that is configured on the system. Protection of this file is
critical for OpenShift security.</rationale>
          <ident cce="80635-6"/>
          <ref cis="1.4.3,1.4.5"/>
          <oval id="file_permissions_master_controller_manager"/>
          <ocil clause="/etc/origin/node/pods/controller.yaml has unix mode -rw-------">To check the permissions of <code>/etc/origin/node/pods/controller.yaml</code>, run the command:
<pre>$ ls -l /etc/origin/node/pods/controller.yaml</pre>
If properly configured, the output should indicate the following permissions:
<code>-rw-------</code></ocil>
        </Rule>
        <Rule id="file_permissions_master_openshift_kubeconfig" prodtype="ocp3" severity="medium">
          <title>Verify Permissions on the OpenShift Master Kubeconfig File</title>
          <description>
To properly set the permissions of <code>/etc/origin/master/openshift-master.kubeconfig</code>, run the command:
<pre>$ sudo chmod 0600 /etc/origin/master/openshift-master.kubeconfig</pre></description>
          <rationale>If the <tt>/etc/origin/master/openshift-master.kubeconfig</tt> file is writable by a group-owner or the
world the risk of its compromise is increased. The file contains the master configuration of
an OpenShift cluster that is configured on the system. Protection of this file is
critical for OpenShift security.</rationale>
          <ident cce="80638-0"/>
          <ref cis="1.4.17"/>
          <oval id="file_permissions_master_openshift_kubeconfig"/>
          <ocil clause="/etc/origin/master/openshift-master.kubeconfig has unix mode -rw-------">To check the permissions of <code>/etc/origin/master/openshift-master.kubeconfig</code>, run the command:
<pre>$ ls -l /etc/origin/master/openshift-master.kubeconfig</pre>
If properly configured, the output should indicate the following permissions:
<code>-rw-------</code></ocil>
        </Rule>
        <Rule id="file_owner_master_cni_conf" prodtype="ocp3" severity="medium">
          <title>Verify User Who Owns The OpenShift Container Network Interface Files</title>
          <description> To properly set the owner of <code>/etc/cni/net.d/*</code>, run the command: <pre>$ sudo chown root /etc/cni/net.d/* </pre></description>
          <rationale>CNI (Container Network Interface) files consist of a specification and libraries for
writing plugins to configure network interfaces in Linux containers, along with a number
of supported plugins. Allowing writeable access to the files could allow an attacker to modify
the networking configuration potentially adding a rouge network connection.</rationale>
          <ident cce="80623-2"/>
          <ref cis="1.4.10"/>
          <oval id="file_owner_master_cni_conf"/>
          <ocil clause="/etc/cni/net.d/* has owner root">To check the ownership of <code>/etc/cni/net.d/*</code>, run the command: <pre>$ ls -lL /etc/cni/net.d/*</pre> If properly configured, the output should indicate the following owner: <code>root</code></ocil>
        </Rule>
        <Rule id="file_groupowner_master_scheduler_conf" prodtype="ocp3" severity="medium">
          <title>Verify Group Who Owns The OpenShift Scheduler Configuration File</title>
          <description> To properly set the group owner of <code>/etc/origin/master/scheduler.json</code>, run the command: <pre>$ sudo chgrp root /etc/origin/master/scheduler.json</pre></description>
          <rationale>The <tt>/etc/origin/master/scheduler.json</tt> file contains information about the configuration of the
OpenShift scheduler that is configured on the system. Protection of this file is
critical for OpenShift security.</rationale>
          <ident cce="80616-6"/>
          <ref cis="1.4.16"/>
          <oval id="file_groupowner_master_scheduler_conf"/>
          <ocil clause="/etc/origin/master/scheduler.json has group owner root">To check the group ownership of <code>/etc/origin/master/scheduler.json</code>, run the command: <pre>$ ls -lL /etc/origin/master/scheduler.json</pre> If properly configured, the output should indicate the following group-owner: <code>root</code></ocil>
        </Rule>
        <Rule id="file_permissions_var_lib_etcd" prodtype="ocp3" severity="medium">
          <title>The OpenShift etcd Data Directory Must Have Mode 0700</title>
          <description>
To properly set the permissions of <code>/var/lib/etcd</code>, run the command:
<pre>$ sudo chmod 0700 /var/lib/etcd</pre></description>
          <rationale>The <tt>/var/lib/etcd</tt> directory contains highly-avaliable distributed key/value data storage
across an OpenShift cluster. Allowing access to users to this directory could compromise OpenShift
data and the cluster.</rationale>
          <ref cis="1.4.11"/>
          <oval id="file_permissions_var_lib_etcd"/>
          <ocil clause="/var/lib/etcd has unix mode -rwx------">To check the permissions of <code>/var/lib/etcd</code>, run the command:
<pre>$ ls -l /var/lib/etcd</pre>
If properly configured, the output should indicate the following permissions:
<code>-rwx------</code></ocil>
        </Rule>
        <Rule id="file_owner_master_api_server" prodtype="ocp3" severity="medium">
          <title>Verify User Who Owns The OpenShift API Specification File</title>
          <description> To properly set the owner of <code>/etc/origin/node/pods/apiserver.yaml</code>, run the command: <pre>$ sudo chown root /etc/origin/node/pods/apiserver.yaml </pre></description>
          <rationale>The <tt>/etc/origin/node/pods/apiserver.yaml</tt> file contains information about the configuration of the
OpenShift API Server that is configured on the system. Protection of this file is
critical for OpenShift security.</rationale>
          <ident cce="82058-9"/>
          <ref cis="1.4.2"/>
          <oval id="file_owner_master_api_server"/>
          <ocil clause="/etc/origin/node/pods/apiserver.yaml has owner root">To check the ownership of <code>/etc/origin/node/pods/apiserver.yaml</code>, run the command: <pre>$ ls -lL /etc/origin/node/pods/apiserver.yaml</pre> If properly configured, the output should indicate the following owner: <code>root</code></ocil>
        </Rule>
        <Rule id="file_groupowner_master_openshift_conf" prodtype="ocp3" severity="medium">
          <title>Verify Group Who Owns The OpenShift Master Configuration File</title>
          <description> To properly set the group owner of <code>/etc/origin/master/master-config.yaml</code>, run the command: <pre>$ sudo chgrp root /etc/origin/master/master-config.yaml</pre></description>
          <rationale>The <tt>/etc/origin/master/master-config.yaml</tt> file contains information about the master configuration of the
OpenShift cluster that is configured on the system. Protection of this file is
critical for OpenShift security.</rationale>
          <ident cce="80614-1"/>
          <ref cis="1.4.18"/>
          <oval id="file_groupowner_master_openshift_conf"/>
          <ocil clause="/etc/origin/master/master-config.yaml has group owner root">To check the group ownership of <code>/etc/origin/master/master-config.yaml</code>, run the command: <pre>$ ls -lL /etc/origin/master/master-config.yaml</pre> If properly configured, the output should indicate the following group-owner: <code>root</code></ocil>
        </Rule>
        <Rule id="file_permissions_node_kubeconfig" prodtype="ocp3" severity="medium">
          <title>Verify Permissions on the OpenShift Node Kubeconfig File</title>
          <description>
To properly set the permissions of <code>/etc/origin/node/node.kubeconfig</code>, run the command:
<pre>$ sudo chmod 0600 /etc/origin/node/node.kubeconfig</pre></description>
          <rationale>If the <tt>/etc/origin/node/node.kubeconfig</tt> file is writable by a group-owner or the
world the risk of its compromise is increased. The file contains the configuration of
an OpenShift node that is configured on the system. Protection of this file is
critical for OpenShift security.</rationale>
          <ident cce="80641-4"/>
          <ref cis="2.2.1"/>
          <oval id="file_permissions_node_kubeconfig"/>
          <ocil clause="/etc/origin/node/node.kubeconfig has unix mode -rw-------">To check the permissions of <code>/etc/origin/node/node.kubeconfig</code>, run the command:
<pre>$ ls -l /etc/origin/node/node.kubeconfig</pre>
If properly configured, the output should indicate the following permissions:
<code>-rw-------</code></ocil>
        </Rule>
        <Rule id="file_owner_openshift_node_client_crt" prodtype="ocp3" severity="medium">
          <title>Verify User Who Owns OpenShift Node Certificate File</title>
          <description> To properly set the owner of <code>/etc/origin/node/client-ca.crt</code>, run the command: <pre>$ sudo chown root /etc/origin/node/client-ca.crt </pre></description>
          <rationale>The <tt>/etc/origin/node/client-ca.crt</tt> file contains the certificate authority
certificate for an OpenShift node that is configured on the system. Protection of this file is
critical for OpenShift security.</rationale>
          <ident cce="80631-5"/>
          <ref cis="2.2.8"/>
          <oval id="file_owner_openshift_node_client_crt"/>
          <ocil clause="/etc/origin/node/client-ca.crt has owner root">To check the ownership of <code>/etc/origin/node/client-ca.crt</code>, run the command: <pre>$ ls -lL /etc/origin/node/client-ca.crt</pre> If properly configured, the output should indicate the following owner: <code>root</code></ocil>
        </Rule>
        <Rule id="file_permissions_master_scheduler_conf" prodtype="ocp3" severity="medium">
          <title>Verify Permissions on the OpenShift Scheduler Configuration File</title>
          <description>
To properly set the permissions of <code>/etc/origin/master/scheduler.json</code>, run the command:
<pre>$ sudo chmod 0600 /etc/origin/master/scheduler.json</pre></description>
          <rationale>If the <tt>/etc/origin/master/scheduler.json</tt> file is writable by a group-owner or the
world the risk of its compromise is increased. The file contains the configuration of
an OpenShift scheduler that is configured on the system. Protection of this file is
critical for OpenShift security.</rationale>
          <ident cce="80639-8"/>
          <ref cis="1.4.15"/>
          <oval id="file_permissions_master_scheduler_conf"/>
          <ocil clause="/etc/origin/master/scheduler.json has unix mode -rw-------">To check the permissions of <code>/etc/origin/master/scheduler.json</code>, run the command:
<pre>$ ls -l /etc/origin/master/scheduler.json</pre>
If properly configured, the output should indicate the following permissions:
<code>-rw-------</code></ocil>
        </Rule>
        <Rule id="file_groupowner_var_lib_etcd" prodtype="ocp3" severity="medium">
          <title>Verify Group Who Owns The OpenShift etcd Data Directory</title>
          <description> To properly set the group owner of <code>/var/lib/etcd</code>, run the command: <pre>$ sudo chgrp root /var/lib/etcd</pre></description>
          <rationale>The <tt>/var/lib/etcd</tt> directory contains highly-avaliable distributed key/value data storage
across an OpenShift cluster. Allowing access to users to this directory could compromise OpenShift
data and the cluster.</rationale>
          <ident cce="80621-6"/>
          <ref cis="1.4.12"/>
          <oval id="file_groupowner_var_lib_etcd"/>
          <ocil clause="/var/lib/etcd has group owner root">To check the group ownership of <code>/var/lib/etcd</code>, run the command: <pre>$ ls -lL /var/lib/etcd</pre> If properly configured, the output should indicate the following group-owner: <code>root</code></ocil>
        </Rule>
        <Rule id="file_groupowner_node_kubeconfig" prodtype="ocp3" severity="medium">
          <title>Verify Group Who Owns The OpenShift Node Kubeconfig File</title>
          <description> To properly set the group owner of <code>/etc/origin/node/node.kubeconfig</code>, run the command: <pre>$ sudo chgrp root /etc/origin/node/node.kubeconfig</pre></description>
          <rationale>The <tt>/etc/origin/node/node.kubeconfig</tt> file contains information about the configuration of the
OpenShift node that is configured on the system. Protection of this file is
critical for OpenShift security.</rationale>
          <ident cce="80618-2"/>
          <ref cis="2.2.2"/>
          <oval id="file_groupowner_node_kubeconfig"/>
          <ocil clause="/etc/origin/node/node.kubeconfig has group owner root">To check the group ownership of <code>/etc/origin/node/node.kubeconfig</code>, run the command: <pre>$ ls -lL /etc/origin/node/node.kubeconfig</pre> If properly configured, the output should indicate the following group-owner: <code>root</code></ocil>
        </Rule>
        <Rule id="file_permissions_master_openshift_conf" prodtype="ocp3" severity="medium">
          <title>Verify Permissions on the OpenShift Master Configuration File</title>
          <description>
To properly set the permissions of <code>/etc/origin/master/master-config.yaml</code>, run the command:
<pre>$ sudo chmod 0600 /etc/origin/master/master-config.yaml</pre></description>
          <rationale>If the <tt>/etc/origin/master/master-config.yaml</tt> file is writable by a group-owner or the
world the risk of its compromise is increased. The file contains the master configuration of
an OpenShift cluster that is configured on the system. Protection of this file is
critical for OpenShift security.</rationale>
          <ident cce="80637-2"/>
          <ref cis="1.4.17"/>
          <oval id="file_permissions_master_openshift_conf"/>
          <ocil clause="/etc/origin/master/master-config.yaml has unix mode -rw-------">To check the permissions of <code>/etc/origin/master/master-config.yaml</code>, run the command:
<pre>$ ls -l /etc/origin/master/master-config.yaml</pre>
If properly configured, the output should indicate the following permissions:
<code>-rw-------</code></ocil>
        </Rule>
        <Rule id="file_owner_master_controller_manager" prodtype="ocp3" severity="medium">
          <title>Verify User Who Owns The OpenShift Controller Manager Specification File</title>
          <description> To properly set the owner of <code>/etc/origin/node/pods/controller.yaml</code>, run the command: <pre>$ sudo chown root /etc/origin/node/pods/controller.yaml </pre></description>
          <rationale>The <tt>/etc/origin/node/pods/controller.yaml</tt> file contains information about the configuration of the
OpenShift Controller Manager Server that is configured on the system. Protection of this file is
critical for OpenShift security.</rationale>
          <ident cce="80624-0"/>
          <ref cis="1.4.4,1.4.6"/>
          <oval id="file_owner_master_controller_manager"/>
          <ocil clause="/etc/origin/node/pods/controller.yaml has owner root">To check the ownership of <code>/etc/origin/node/pods/controller.yaml</code>, run the command: <pre>$ ls -lL /etc/origin/node/pods/controller.yaml</pre> If properly configured, the output should indicate the following owner: <code>root</code></ocil>
        </Rule>
        <Rule id="file_groupowner_master_etcd" prodtype="ocp3" severity="medium">
          <title>Verify Group Who Owns The OpenShift etcd Specification File</title>
          <description> To properly set the group owner of <code>/etc/origin/node/pods/etcd.yaml</code>, run the command: <pre>$ sudo chgrp root /etc/origin/node/pods/etcd.yaml</pre></description>
          <rationale>The <tt>/etc/origin/node/pods/apiserver.yaml</tt> file contains information about the configuration of the
OpenShift etcd Server that is configured on the system. Protection of this file is
critical for OpenShift security.</rationale>
          <ident cce="80613-3"/>
          <ref cis="1.4.2"/>
          <oval id="file_groupowner_master_etcd"/>
          <ocil clause="/etc/origin/node/pods/etcd.yaml has group owner root">To check the group ownership of <code>/etc/origin/node/pods/etcd.yaml</code>, run the command: <pre>$ ls -lL /etc/origin/node/pods/etcd.yaml</pre> If properly configured, the output should indicate the following group-owner: <code>root</code></ocil>
        </Rule>
        <Rule id="file_owner_master_scheduler_conf" prodtype="ocp3" severity="medium">
          <title>Verify User Who Owns The OpenShift Scheduler Configuration File</title>
          <description> To properly set the owner of <code>/etc/origin/master/scheduler.json</code>, run the command: <pre>$ sudo chown root /etc/origin/master/scheduler.json </pre></description>
          <rationale>The <tt>/etc/origin/master/scheduler.json</tt> file contains information about the configuration of the
OpenShift scheduler that is configured on the system. Protection of this file is
critical for OpenShift security.</rationale>
          <ident cce="80628-1"/>
          <ref cis="1.4.16"/>
          <oval id="file_owner_master_scheduler_conf"/>
          <ocil clause="/etc/origin/master/scheduler.json has owner root">To check the ownership of <code>/etc/origin/master/scheduler.json</code>, run the command: <pre>$ ls -lL /etc/origin/master/scheduler.json</pre> If properly configured, the output should indicate the following owner: <code>root</code></ocil>
        </Rule>
        <Rule id="file_owner_master_openshift_conf" prodtype="ocp3" severity="medium">
          <title>Verify User Who Owns The OpenShift Master Configuration File</title>
          <description> To properly set the owner of <code>/etc/origin/master/master-config.yaml</code>, run the command: <pre>$ sudo chown root /etc/origin/master/master-config.yaml </pre></description>
          <rationale>The <tt>/etc/origin/master/master-config.yaml</tt> file contains information about the master configuration of the
OpenShift cluster that is configured on the system. Protection of this file is
critical for OpenShift security.</rationale>
          <ident cce="80626-5"/>
          <ref cis="1.4.18"/>
          <oval id="file_owner_master_openshift_conf"/>
          <ocil clause="/etc/origin/master/master-config.yaml has owner root">To check the ownership of <code>/etc/origin/master/master-config.yaml</code>, run the command: <pre>$ ls -lL /etc/origin/master/master-config.yaml</pre> If properly configured, the output should indicate the following owner: <code>root</code></ocil>
        </Rule>
        <Rule id="file_owner_var_lib_etcd" prodtype="ocp3" severity="medium">
          <title>Verify User Who Owns The OpenShift etcd Data Directory</title>
          <description> To properly set the owner of <code>/var/lib/etcd</code>, run the command: <pre>$ sudo chown root /var/lib/etcd </pre></description>
          <rationale>The <tt>/var/lib/etcd</tt> directory contains highly-avaliable distributed key/value data storage
across an OpenShift cluster. Allowing access to users to this directory could compromise OpenShift
data and the cluster.</rationale>
          <ref cis="1.4.12"/>
          <oval id="file_owner_var_lib_etcd"/>
          <ocil clause="/var/lib/etcd has owner root">To check the ownership of <code>/var/lib/etcd</code>, run the command: <pre>$ ls -lL /var/lib/etcd</pre> If properly configured, the output should indicate the following owner: <code>root</code></ocil>
        </Rule>
        <Rule id="file_owner_etc_origin" prodtype="ocp3" severity="medium">
          <title>Verify User Who Owns The OpenShift Configuration Directory</title>
          <description> To properly set the owner of <code>/etc/origin/</code>, run the command: <pre>$ sudo chown root /etc/origin/ </pre></description>
          <rationale>If users can modify the OpenShift configurations, the OpenShift cluster can become inoperable or compromised</rationale>
          <oval id="file_owner_etc_origin"/>
          <ocil clause="/etc/origin/ has owner root">To check the ownership of <code>/etc/origin/</code>, run the command: <pre>$ ls -lL /etc/origin/</pre> If properly configured, the output should indicate the following owner: <code>root</code></ocil>
        </Rule>
        <Rule id="file_permissions_master_etcd" prodtype="ocp3" severity="medium">
          <title>Verify Permissions on the OpenShift etcd Specification File</title>
          <description>
To properly set the permissions of <code>/etc/origin/node/pods/etcd.yaml</code>, run the command:
<pre>$ sudo chmod 0600 /etc/origin/node/pods/etcd.yaml</pre></description>
          <rationale>If the <tt>/etc/origin/node/pods/etcd.yaml</tt> file is writable by a group-owner or the
world the risk of its compromise is increased. The file contains the configuration of
an OpenShift etcd server that is configured on the system. Protection of this file is
critical for OpenShift security.</rationale>
          <ident cce="80636-4"/>
          <ref cis="1.4.7"/>
          <oval id="file_permissions_master_etcd"/>
          <ocil clause="/etc/origin/node/pods/etcd.yaml has unix mode -rw-------">To check the permissions of <code>/etc/origin/node/pods/etcd.yaml</code>, run the command:
<pre>$ ls -l /etc/origin/node/pods/etcd.yaml</pre>
If properly configured, the output should indicate the following permissions:
<code>-rw-------</code></ocil>
        </Rule>
        <Rule id="file_groupowner_openshift_node_service" prodtype="ocp3" severity="medium">
          <title>Verify Group Who Owns The OpenShift Node Service File</title>
          <description> To properly set the group owner of <code>/etc/systemd/system/atomic-openshift-node.service</code>, run the command: <pre>$ sudo chgrp root /etc/systemd/system/atomic-openshift-node.service</pre></description>
          <rationale>The <tt>/etc/systemd/system/atomic-openshift-node.service</tt> file contains information about the configuration of the
OpenShift node service that is configured on the system. Protection of this file is
critical for OpenShift security.</rationale>
          <ident cce="80620-8"/>
          <ref cis="2.2.4"/>
          <oval id="file_groupowner_openshift_node_service"/>
          <ocil clause="/etc/systemd/system/atomic-openshift-node.service has group owner root">To check the group ownership of <code>/etc/systemd/system/atomic-openshift-node.service</code>, run the command: <pre>$ ls -lL /etc/systemd/system/atomic-openshift-node.service</pre> If properly configured, the output should indicate the following group-owner: <code>root</code></ocil>
        </Rule>
        <Rule id="file_owner_master_openshift_kubeconfig" prodtype="ocp3" severity="medium">
          <title>Verify User Who Owns The OpenShift Master Kubeconfig File</title>
          <description> To properly set the owner of <code>/etc/origin/master/openshift-master.kubeconfig</code>, run the command: <pre>$ sudo chown root /etc/origin/master/openshift-master.kubeconfig </pre></description>
          <rationale>The <tt>/etc/origin/master/openshift-master.kubeconfig</tt> file contains information about the master configuration of the
OpenShift cluster that is configured on the system. Protection of this file is
critical for OpenShift security.</rationale>
          <ident cce="80627-3"/>
          <ref cis="1.4.18"/>
          <oval id="file_owner_master_openshift_kubeconfig"/>
          <ocil clause="/etc/origin/master/openshift-master.kubeconfig has owner root">To check the ownership of <code>/etc/origin/master/openshift-master.kubeconfig</code>, run the command: <pre>$ ls -lL /etc/origin/master/openshift-master.kubeconfig</pre> If properly configured, the output should indicate the following owner: <code>root</code></ocil>
        </Rule>
        <Rule id="file_permissions_node_config" prodtype="ocp3,ocp4" severity="medium">
          <title>Verify Permissions on the OpenShift Node Configuration File</title>
          <description>
To properly set the permissions of <code>/etc/origin/node/node-config.yaml</code>, run the command:
<pre>$ sudo chmod 0600 /etc/origin/node/node-config.yaml</pre></description>
          <rationale>If the <tt>/etc/origin/node/node-config.yaml</tt> file is writable by a group-owner or the
world the risk of its compromise is increased. The file contains the configuration of
an OpenShift node that is configured on the system. Protection of this file is
critical for OpenShift security.</rationale>
          <ident cce="80640-6"/>
          <ref cis="2.2.1"/>
          <oval id="file_permissions_node_config"/>
          <ocil clause="/etc/origin/node/node-config.yaml has unix mode -rw-------">To check the permissions of <code>/etc/origin/node/node-config.yaml</code>, run the command:
<pre>$ ls -l /etc/origin/node/node-config.yaml</pre>
If properly configured, the output should indicate the following permissions:
<code>-rw-------</code></ocil>
        </Rule>
        <Rule id="file_groupowner_master_api_server" prodtype="ocp3" severity="medium">
          <title>Verify Group Who Owns The OpenShift API Specification File</title>
          <description> To properly set the group owner of <code>/etc/origin/node/pods/apiserver.yaml</code>, run the command: <pre>$ sudo chgrp root /etc/origin/node/pods/apiserver.yaml</pre></description>
          <rationale>The <tt>/etc/origin/node/pods/apiserver.yaml</tt> file contains information about the configuration of the
OpenShift API Server that is configured on the system. Protection of this file is
critical for OpenShift security.</rationale>
          <ident cce="80576-2"/>
          <ref cis="1.4.2"/>
          <oval id="file_groupowner_master_api_server"/>
          <ocil clause="/etc/origin/node/pods/apiserver.yaml has group owner root">To check the group ownership of <code>/etc/origin/node/pods/apiserver.yaml</code>, run the command: <pre>$ ls -lL /etc/origin/node/pods/apiserver.yaml</pre> If properly configured, the output should indicate the following group-owner: <code>root</code></ocil>
        </Rule>
        <Rule id="file_groupowner_master_controller_manager" prodtype="ocp3" severity="medium">
          <title>Verify Group Who Owns The OpenShift Controller Manager Specification File</title>
          <description> To properly set the group owner of <code>/etc/origin/node/pods/controller.yaml</code>, run the command: <pre>$ sudo chgrp root /etc/origin/node/pods/controller.yaml</pre></description>
          <rationale>The <tt>/etc/origin/node/pods/controller.yaml</tt> file contains information about the configuration of the
OpenShift Controller Manager Server that is configured on the system. Protection of this file is
critical for OpenShift security.</rationale>
          <ident cce="80612-5"/>
          <ref cis="1.4.4,1.4.6"/>
          <oval id="file_groupowner_master_controller_manager"/>
          <ocil clause="/etc/origin/node/pods/controller.yaml has group owner root">To check the group ownership of <code>/etc/origin/node/pods/controller.yaml</code>, run the command: <pre>$ ls -lL /etc/origin/node/pods/controller.yaml</pre> If properly configured, the output should indicate the following group-owner: <code>root</code></ocil>
        </Rule>
        <Rule id="file_permissions_openshift_node_client_crt" prodtype="ocp3" severity="medium">
          <title>Verify Permissions on OpenShift Node Certificate File</title>
          <description>
To properly set the permissions of <code>/etc/origin/node/client-ca.crt</code>, run the command:
<pre>$ sudo chmod 0644 /etc/origin/node/client-ca.crt</pre></description>
          <rationale>If the <tt>/etc/origin/node/client-ca.crt</tt> file is writable by a group-owner or the
world the risk of its compromise is increased. The file contains the certificate authority
certificate for an OpenShift node that is configured on the system. Protection of this file is
critical for OpenShift security.</rationale>
          <ident cce="80642-2"/>
          <ref cis="2.2."/>
          <oval id="file_permissions_openshift_node_client_crt"/>
          <ocil clause="/etc/origin/node/client-ca.crt has unix mode -rw-r--r--">To check the permissions of <code>/etc/origin/node/client-ca.crt</code>, run the command:
<pre>$ ls -l /etc/origin/node/client-ca.crt</pre>
If properly configured, the output should indicate the following permissions:
<code>-rw-r--r--</code></ocil>
        </Rule>
        <Rule id="file_owner_openshift_node_service" prodtype="ocp3" severity="medium">
          <title>Verify User Who Owns The OpenShift Node Service File</title>
          <description> To properly set the owner of <code>/etc/systemd/system/atomic-openshift-node.service</code>, run the command: <pre>$ sudo chown root /etc/systemd/system/atomic-openshift-node.service </pre></description>
          <rationale>The <tt>/etc/systemd/system/atomic-openshift-node.service</tt> file contains information about the configuration of the
OpenShift node service that is configured on the system. Protection of this file is
critical for OpenShift security.</rationale>
          <ident cce="80632-3"/>
          <ref cis="2.2.4"/>
          <oval id="file_owner_openshift_node_service"/>
          <ocil clause="/etc/systemd/system/atomic-openshift-node.service has owner root">To check the ownership of <code>/etc/systemd/system/atomic-openshift-node.service</code>, run the command: <pre>$ ls -lL /etc/systemd/system/atomic-openshift-node.service</pre> If properly configured, the output should indicate the following owner: <code>root</code></ocil>
        </Rule>
        <Rule id="file_permissions_master_admin_conf" prodtype="ocp3" severity="medium">
          <title>Verify Permissions on the OpenShift Admin Kubeconfig File</title>
          <description>
To properly set the permissions of <code>/etc/origin/master/admin.kubeconfig</code>, run the command:
<pre>$ sudo chmod 0600 /etc/origin/master/admin.kubeconfig</pre></description>
          <rationale>If the <tt>/etc/origin/master/admin.kubeconfig</tt> file is writable by a group-owner or the
world the risk of its compromise is increased. The file contains the administration configuration of the
OpenShift cluster that is configured on the system. Protection of this file is
critical for OpenShift security.</rationale>
          <ident cce="80633-1"/>
          <ref cis="1.4.13"/>
          <oval id="file_permissions_master_admin_conf"/>
          <ocil clause="/etc/origin/master/admin.kubeconfig has unix mode -rw-------">To check the permissions of <code>/etc/origin/master/admin.kubeconfig</code>, run the command:
<pre>$ ls -l /etc/origin/master/admin.kubeconfig</pre>
If properly configured, the output should indicate the following permissions:
<code>-rw-------</code></ocil>
        </Rule>
        <Rule id="file_permissions_master_api_server" prodtype="ocp3" severity="medium">
          <title>Verify Permissions on the OpenShift API Specification File</title>
          <description>
To properly set the permissions of <code>/etc/origin/node/pods/apiserver.yaml</code>, run the command:
<pre>$ sudo chmod 0600 /etc/origin/node/pods/apiserver.yaml</pre></description>
          <rationale>If the <tt>/etc/origin/node/pods/apiserver.yaml</tt> file is writable by a group-owner or the
world the risk of its compromise is increased. The file contains the configuration of
an OpenShift API server that is configured on the system. Protection of this file is
critical for OpenShift security.</rationale>
          <ident cce="80574-7"/>
          <ref cis="1.4.1"/>
          <oval id="file_permissions_master_api_server"/>
          <ocil clause="/etc/origin/node/pods/apiserver.yaml has unix mode -rw-------">To check the permissions of <code>/etc/origin/node/pods/apiserver.yaml</code>, run the command:
<pre>$ ls -l /etc/origin/node/pods/apiserver.yaml</pre>
If properly configured, the output should indicate the following permissions:
<code>-rw-------</code></ocil>
        </Rule>
      </Group>
    </Group>
    <Group id="api-server" prodtype="ocp3">
      <title>OpenShift API Server</title>
      <description>This section contains recommendations for kube-apiserver configuration.</description>
      <Rule id="api_server_kubelet_certificate_authority" prodtype="ocp3" severity="high">
        <title>Configure the kubelet Certificate Authority for the API Server</title>
        <description>To ensure OpenShift verifies kubelet certificates before establishing
connections, follow the OpenShift documentation and setup the TLS connection
between the API Server and kubelets. Then, verify
that <tt>kubeletClientInfo</tt> has the <tt>ca</tt> configured in 
the API Server pod specification file
<tt>/etc/origin/master/master-config.yaml</tt> on the master
node(s) to something similar to:
<pre>kubeletClientInfo:
  ca: ca-bundle.crt
  certFile: master.kubelet-client.crt
  keyFile: master.kubelet-client.key</pre></description>
        <rationale>Connections from the API Server to the kubelet are used for fetching logs
for pods, attaching (through kubectl) to running pods, and using the kubelet
port-forwarding functionality. These connections terminate at the kubelet
HTTPS endpoint. By default, the API Server does not verify the kubelet serving
certificate, which makes the connection subject to man-in-the-middle attacks,
and unsafe to run over untrusted and/or public networks.</rationale>
        <ref cis="1.1.4,1.1.31"/>
        <oval id="api_server_kubelet_certificate_authority"/>
        <ocil clause="&lt;tt&gt;ca&lt;/tt&gt; is not set as appropriate for &lt;tt&gt;kubeletClientInfo&lt;/tt&gt;">Run the following command on the master node(s):
<pre>$ sudo grep -A3 kubeletClientInfo /etc/origin/master/master-config.yaml</pre>
The output should contain something similar to:
<pre>kubeletClientInfo:
  ca: ca-bundle.crt
  certFile: master.kubelet-client.crt
  keyFile: master.kubelet-client.key</pre></ocil>
      </Rule>
      <Rule id="api_server_tls_private_key" prodtype="ocp3" severity="medium">
        <title>Configure the Certificate Key for the API Server</title>
        <description>To ensure the API Server utilizes its own TLS certificates, the
<tt>keyFile</tt> must be configured. To, verify
that <tt>servingInfo</tt> has the <tt>keyFile</tt> configured in 
the API Server pod specification file
<tt>/etc/origin/master/master-config.yaml</tt> on the master
node(s) to something similar to:
<pre>servingInfo:
  clientCA: ca.crt
  certFile: master.server.crt
  keyFile: master.server.key</pre></description>
        <rationale>API Server communication contains sensitive parameters that should remain
encrypted in transit. Configure the API Server to serve only HTTPS
traffic.</rationale>
        <ref cis="1.1.38"/>
        <oval id="api_server_tls_private_key"/>
        <ocil clause="&lt;tt&gt;keyFile&lt;/tt&gt; is not set as appropriate for &lt;tt&gt;servingInfo&lt;/tt&gt;">Run the following command on the master node(s):
<pre>$ sudo grep -A7 servingInfo /etc/origin/master/master-config.yaml</pre>
The output should contain something similar to:
<pre>servingInfo:
  clientCA: ca.crt
  certFile: master.server.crt
  keyFile: master.server.key</pre></ocil>
      </Rule>
      <Rule id="api_server_tls_cert" prodtype="ocp3" severity="medium">
        <title>Configure the Certificate for the API Server</title>
        <description>To ensure the API Server utilizes its own TLS certificates, the
<tt>certFile</tt> must be configured. Verify
that <tt>servingInfo</tt> has the <tt>certFile</tt> configured in 
the API Server pod specification file
<tt>/etc/origin/master/master-config.yaml</tt> on the master
node(s) to something similar to:
<pre>servingInfo:
  clientCA: ca.crt
  certFile: master.server.crt
  keyFile: master.server.key</pre></description>
        <rationale>API Server communication contains sensitive parameters that should remain
encrypted in transit. Configure the API Server to serve only HTTPS
traffic.</rationale>
        <ref cis="1.1.28"/>
        <oval id="api_server_tls_cert"/>
        <ocil clause="&lt;tt&gt;certFile&lt;/tt&gt; is not set as appropriate for &lt;tt&gt;servingInfo&lt;/tt&gt;">Run the following command on the master node(s):
<pre>$ sudo grep -A7 servingInfo /etc/origin/master/master-config.yaml</pre>
The output should contain something similar to:
<pre>servingInfo:
  clientCA: ca.crt
  certFile: master.server.crt
  keyFile: master.server.key</pre></ocil>
      </Rule>
      <Rule id="api_server_client_ca" prodtype="ocp3" severity="medium">
        <title>Configure the Client Certificate Authority for the API Server</title>
        <description>Certificates must be provided to fully setup TLS client certificate
authentication. To ensure the API Server utilizes its own TLS certificates, the
<tt>clientCA</tt> must be configured. Verify
that <tt>servingInfo</tt> has the <tt>clientCA</tt> configured in 
the API Server pod specification file
<tt>/etc/origin/master/master-config.yaml</tt> on the master
node(s) to something similar to:
<pre>servingInfo:
  clientCA: ca.crt
  certFile: master.server.crt
  keyFile: master.server.key</pre></description>
        <rationale>API Server communication contains sensitive parameters that should remain
encrypted in transit. Configure the API Server to serve only HTTPS traffic.
If <tt>-clientCA</tt> is set, any request presenting a client
certificate signed by one of the authorities in the <tt>client-ca-file</tt>
is authenticated with an identity corresponding to the <i>CommonName</i> of
the client certificate.</rationale>
        <ident cce="81152-1"/>
        <ref cis="1.1.29"/>
        <oval id="api_server_client_ca"/>
        <ocil clause="&lt;tt&gt;clientCA&lt;/tt&gt; is not set as appropriate for &lt;tt&gt;servingInfo&lt;/tt&gt;">Run the following command on the master node(s):
<pre>$ sudo grep -A7 servingInfo /etc/origin/master/master-config.yaml</pre>
The output should contain something similar to:
<pre>servingInfo:
  clientCA: ca.crt
  certFile: master.server.crt
  keyFile: master.server.key</pre></ocil>
      </Rule>
      <Rule id="api_server_audit_log_path" prodtype="ocp3" severity="high">
        <title>Configure the Audit Log Path</title>
        <description>To enable auditing on the OpenShift API Server, the audit log path
must be set. Edit the API Server pod specification file
<tt>/etc/origin/master/master-config.yaml</tt> on the master node(s)
and set the <tt>audit-log-path</tt> to a suitable path and file
where you would like audit logs to be written. For example:
<pre>auditConfig:
  auditFilePath: "/etc/origin/master/audit-ocp.log"
  enabled: true
  maximumFileRetentionDays: 30
  maximumFileSizeMegabytes: 10
  maximumRetainedFiles: 10</pre></description>
        <rationale>Auditing of the API Server is not enabled by default. Auditing the API Server
provides a security-relevant chronological set of records documenting the sequence
of activities that have affected the system by users, administrators, or other
system components.</rationale>
        <ref cis="1.1.15"/>
        <oval id="api_server_audit_log_path"/>
        <ocil clause="&lt;tt&gt;audit-log-path&lt;/tt&gt; does not contain a valid audit file path">Run the following command on the master node(s):
<pre>$ sudo grep auditFilePath /etc/origin/master/master-config.yaml</pre>
The output should return a valid audit log path. The default is
<tt>/etc/origin/master/audit-ocp.log</tt>.</ocil>
      </Rule>
      <Rule id="api_server_tls_cipher_suites" prodtype="ocp3" severity="medium">
        <title>Use Strong Cryptographic Ciphers on the API Server</title>
        <description>To ensure that the API Server is configured to only use strong
cryptographic ciphers, edit the API Server pod specification file
<tt>/etc/origin/master/master-config.yaml</tt> on the master
node and set the parameter below:
<pre>servingInfo:
  cipherSuites:
  - TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256
  - TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
  - TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305
  - TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
  - TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305
  - TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384
  - TLS_RSA_WITH_AES_256_GCM_SHA384
  - TLS_RSA_WITH_AES_128_GCM_SHA256</pre></description>
        <rationale>TLS ciphers have had a number of known vulnerabilities and weaknesses,
which can reduce the protection provided. By default OpenShift supports
a number of TLS ciphersuites including some that have security concerns,
weakening the protection provided.</rationale>
        <ref cis="1.1.40"/>
        <oval id="api_server_tls_cipher_suites"/>
        <ocil clause="&lt;tt&gt;cipherSuites&lt;/tt&gt; does not contain TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256 from servingInfo">Run the following command on the master node(s):
<pre>$ sudo grep -A8 cipherSuites /etc/origin/master/master-config.yaml</pre>
Verify that the output is similar to:
<pre>cipherSuites:
  - TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256
  - TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
  - TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305
  - TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
  - TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305
  - TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384
  - TLS_RSA_WITH_AES_256_GCM_SHA384
  - TLS_RSA_WITH_AES_128_GCM_SHA256</pre></ocil>
        <warning category="management">Once configured, API Server clients that cannot support modern
cryptographic ciphers will not be able to make connections to the API
server.</warning>
      </Rule>
      <Rule id="api_server_admission_control_plugin_EventRateLimit" prodtype="ocp3" severity="medium">
        <title>Enable the EventRateLimit Admission Control Plugin</title>
        <description>To limit the rate at which the API Server accepts requests, follow
the OpenShift documentation and set the desired limits in a configuration
file. Then, edit the API Server pod specification file
<tt>/etc/origin/master/master-config.yaml</tt> and
set the <tt>admissionConfig</tt> to include <tt>DenyEscalatingExec</tt>:
<pre>admissionConfig:
  pluginConfig:
    EventRateLimit:
      configuration:
      kind: DefaultAdmissionConfig
      apiVersion: v1
      disable: false</pre></description>
        <rationale>Using <tt>EventRateLimit</tt> admission control enforces a limit on the
number of events that the API Server will accept in a given time slice.
In a large multi-tenant cluster, there might be a small percentage of
misbehaving tenants which could have a significant impact on the
performance of the cluster overall. It is recommended to limit the rate
of events that the API Server will accept.</rationale>
        <ref cis="1.1.35"/>
        <oval id="api_server_admission_control_plugin_EventRateLimit"/>
        <ocil clause="&lt;tt&gt;enable-admission-plugins&lt;/tt&gt; does not include &lt;tt&gt;EventRateLimit&lt;/tt&gt;">Run the following command on the master node(s):
<pre>$ sudo grep -A4 EventRateLimit /etc/origin/master/master-config.yaml</pre>
The output should return <pre>disable: false</pre>.</ocil>
      </Rule>
      <Rule id="api_server_authorization_mode" prodtype="ocp3" severity="medium">
        <title>Ensure API Server authorization-mode is set to Webhook</title>
        <description>By default, unauthenticated/unauthorized users have no access to OpenShift nodes
and the API Server <tt>authorization-mode</tt> is set to <tt>Webhook</tt>.
To ensure that the API server requires authorization for API requests,
validate that <tt>authorization-mode</tt> is configured to <tt>Webhook</tt>
in <tt>/etc/origin/master/master-config.yaml</tt>:
<pre>kubernetesMasterConfig:
  apiServerArguments:
    authorization-mode:
    - Webhook</pre></description>
        <rationale>Ensuring <tt>authorization-mode</tt> is set to <tt>Webhook</tt> helps enforce that
unauthenticated/unauthorized users have no access to OpenShift nodes.</rationale>
        <ref cis="1.1.31"/>
        <oval id="api_server_authorization_mode"/>
        <ocil clause="&lt;tt&gt;authorization-mode&lt;/tt&gt; is not configured to &lt;tt&gt;Webhook&lt;/tt&gt;">Run the following command on the master node(s):
<pre>$ sudo grep -A1 authorization-mode /etc/origin/master/master-config.yaml</pre>
Verify that there is no output, or the output is set to <tt>Webhook</tt>.</ocil>
        <warning category="functionality">If <tt>authorization-mode</tt> is not configured to <tt>Webhook</tt>, the node
systemd service (<tt>atomic-openshift-node</tt>) will not start.</warning>
      </Rule>
      <Rule id="api_server_audit_log_maxage" prodtype="ocp3" severity="medium">
        <title>Configure the Audit Log Maximum Retention Days (maximumFileRetentionDays)</title>
        <description>To configure audit log retention, edit the API Server pod specification file
<tt>/etc/origin/master/master-config.yaml</tt> on the master node(s) and set
the <tt>maximumFileRetentionDays</tt> parameter to 30 or as appropriate number of days:
<pre>auditConfig:
  auditFilePath: "/etc/origin/master/audit-ocp.log"
  enabled: true
  maximumFileRetentionDays: 30
  maximumFileSizeMegabytes: 10
  maximumRetainedFiles: 10</pre></description>
        <rationale>Retaining audit logs for a specified period of time allows OpenShift Operators
to retroactively review and correlate events, such as for investigative purposes.</rationale>
        <ref cis="1.1.16"/>
        <oval id="api_server_audit_log_maxage"/>
        <ocil clause="&lt;tt&gt;maximumFileRetentionDays&lt;/tt&gt; is set less than &lt;tt&gt;30&lt;/tt&gt; or as appropriate">Run the following command on the master node(s):
<pre>$ sudo grep maximumFileRetentionDays /etc/origin/master/master-config.yaml</pre>
The output should return a value of <pre>30</pre> or as appropriate.</ocil>
      </Rule>
      <Rule id="api_server_admission_control_plugin_DenyEscalatingExec" prodtype="ocp3" severity="high">
        <title>Enable the DenyEscalatingExec Admission Control Plugin</title>
        <description>By default, <tt>DenyEscalatingExec</tt> is not enabled, which allows
privileged pods to execute <tt>exec</tt> and <tt>attach</tt> commands.
To disable this capability, edit the API Server pod specification file
<tt>/etc/origin/master/master-config.yaml</tt> on the master node(s) and
set the <tt>admissionConfig</tt> to include <tt>DenyEscalatingExec</tt>:
<pre>admissionConfig:
  pluginConfig:
    DenyEscalatingExec:
      configuration:
      kind: DefaultAdmissionConfig
      apiVersion: v1
      disable: false</pre></description>
        <rationale>Setting admission control policy to include <tt>DenyEscalatingExec</tt>
denies <tt>exec</tt> and <tt>attach</tt> commands to pods that run with
escalated privileges that allow host access. This includes pods that run as
privileged, have access to the host IPC namespace, and have access to the host
PID namespace.</rationale>
        <ref cis="1.1.2"/>
        <oval id="api_server_admission_control_plugin_DenyEscalatingExec"/>
        <ocil clause="&lt;tt&gt;admissionConfig&lt;/tt&gt; does not contain &lt;tt&gt;DenyEscalatingExec&lt;/tt&gt;">Run the following command on the master node(s):
<pre>$ sudo grep -A4 DenyEscalatingExec /etc/origin/master/master-config.yaml</pre>
The output should return <pre>disable: false</pre>.</ocil>
      </Rule>
      <Rule id="api_server_admission_control_plugin_ServiceAccount" prodtype="ocp3" severity="medium">
        <title>Enable the ServiceAccount Admission Control Plugin</title>
        <description>To ensure <tt>ServiceAccount</tt> objects must be created and granted
before pod creation is allowed, follow the documentation and create
<tt>ServiceAccount</tt> objects as per your environment. Then, edit the
API Server pod specification file
<tt>/etc/origin/master/master-config.yaml</tt> on the master node(s)
set the <tt>admissionConfig</tt> to include <tt>ServiceAccount</tt>:
<pre>admissionConfig:
  pluginConfig:
    ServiceAccount:
      configuration:
      kind: DefaultAdmissionConfig
      apiVersion: v1
      disable: false</pre></description>
        <rationale>When a pod is created, if a service account is not specified, the pod
is automatically assigned the <i>default</i> service account in the same
namespace. OpenShift operators should create unique service accounts
and let the API Server manage its security tokens.</rationale>
        <ref cis="1.1.27"/>
        <oval id="api_server_admission_control_plugin_ServiceAccount"/>
        <ocil clause="&lt;tt&gt;admissionConfig&lt;/tt&gt; does not contain &lt;tt&gt;ServiceAccount&lt;/tt&gt;">Run the following command on the master node(s):
<pre>$ sudo grep -A4 ServiceAccount /etc/origin/master/master-config.yaml</pre>
The output should return <pre>disable: false</pre>.</ocil>
      </Rule>
      <Rule id="api_server_admission_control_plugin_AlwaysAdmit" prodtype="ocp3" severity="medium">
        <title>Disable the AlwaysAdmit Admission Control Plugin</title>
        <description>To ensure OpenShift only responses to requests explicitly allowed by the
admissions control plugin, edit the API Server pod specification file
<tt>/etc/origin/master/master-config.yaml</tt> on the master node(s)
and ensure the <tt>admissionConfig</tt> is not configured to include <tt>AlwaysAdmit</tt>.</description>
        <rationale>Enabling the admission control plugin <tt>AlwaysAdmin</tt> allows all
requests and does not provide any filtering.</rationale>
        <ref cis="1.1.10"/>
        <oval id="api_server_admission_control_plugin_AlwaysAdmit"/>
        <ocil clause="&lt;tt&gt;admissionConfig&lt;/tt&gt; is not set">Run the following command on the master node(s):
<pre>$ sudo grep -A4 AlwaysAdmit /etc/origin/master/master-config.yaml</pre>
The output should return no output.</ocil>
      </Rule>
      <Rule id="api_server_token_auth" prodtype="ocp3" severity="high">
        <title>Disable Token-based Authentication</title>
        <description>To ensure OpenShift does not accept token-based authentication,
follow the OpenShift documentation and configure alternate mechanisms for
authentication. Then, edit the API Server pod specification file
<tt>/etc/origin/master/master-config.yaml</tt> on the master
node(s) and remove the <tt>token-auth-file</tt> setting.
<pre>kubernetesMasterConfig:
  apiServerArguments:
    token-auth-file:
    - /path/to/file</pre></description>
        <rationale>The token-based authentication utilizes static tokens to authenticate
requests to the API Server. The tokens are stored in clear-text in a file
on the API Server, and cannot be revoked or rotated without restarting the
API Server.</rationale>
        <ref cis="1.1.20"/>
        <oval id="api_server_token_auth"/>
        <ocil clause="&lt;tt&gt;token-auth-file&lt;/tt&gt; argument is configured">Run the following command on the master node(s):
<pre>$ sudo grep -A2 token-auth-file /etc/origin/master/master-config.yaml</pre>
The output should return no output.</ocil>
      </Rule>
      <Rule id="api_server_anonymous_auth" prodtype="ocp3" severity="medium">
        <title>Disable Anonymous Authentication to the API Server</title>
        <description>By default, anonymous access to the OpenShift API is enabled. This
configuration check ensures that anonymous requests to the OpenShift
API server are disabled. Edit the API server pod specification file
<tt>/etc/origin/master/master-config.yaml</tt> on the master node(s)
and set the below parameter:
<pre>apiServerArguments:
  anonymous-auth:
  - 'false'</pre></description>
        <rationale>When enabled, requests that are not rejected by other configured
authentication methods are treated as anonymous requests. These
requests are then served by the API server. OpenShift Operators should
rely on authentication to authorize access and disallow anonymous
requests.</rationale>
        <ref cis="1.1.1"/>
        <oval id="api_server_anonymous_auth"/>
        <ocil clause="&lt;tt&gt;anonymous-auth&lt;/tt&gt; is not set to &lt;tt&gt;false&lt;/tt&gt;">Run the following command on the master node(s):
<pre>$ sudo grep -A1 anonymous-auth /etc/origin/master/master-config.yaml</pre>
The output should return <pre>false</pre>.</ocil>
      </Rule>
      <Rule id="api_server_profiling" prodtype="ocp3" severity="low">
        <title>Disable Profiling on the API server</title>
        <description>To disable profiling, edit the API Server pod specification file
<tt>/etc/origin/master/master-config.yaml</tt> on the master node(s)
and set <tt>profiling</tt> to <tt>false</tt>:
<pre>kubernetesMasterConfig:
  schedulerArguments:
    profiling:
    - false</pre></description>
        <rationale>Profiling allows for the identification of specific performance bottlenecks. It
generates a significant amount of program data that could potentially be
exploited to uncover system and program details. If the profiler is not
needed for troubleshooting purposes, it is recommended to turn off for
reduction of potential attack surface.</rationale>
        <ref cis="1.1.8"/>
        <oval id="api_server_profiling"/>
        <ocil clause="&lt;tt&gt;profiling&lt;/tt&gt; is enabled (set to value of &lt;tt&gt;true&lt;/tt&gt;)">Run the following command on the master node(s):
<pre>$ sudo grep -A2 profiling /etc/origin/master/master-config.yaml</pre>
The output should return <pre>false</pre>.</ocil>
      </Rule>
      <Rule id="api_server_etcd_ca" prodtype="ocp3" severity="medium">
        <title>Configure the etcd Certificate Authority for the API Server</title>
        <description>To ensure etcd is configured to make use of TLS encryption for client
connections, follow the OpenShift documentation and setup the TLS
connection between the API Server and etcd. Then, verify
that <tt>etcdClientInfo</tt> has the <tt>ca</tt> configured in 
the API Server pod specification file
<tt>/etc/origin/master/master-config.yaml</tt> on the master
node(s) to something similar to:
<pre>etcdClientInfo:
  ca: master.etcd-ca.crt
  certFile: master.etcd-client.crt
  keyFile: master.etcd-client.key</pre></description>
        <rationale>etcd is a highly-available key-value store used by OpenShift deployments
for persistent storage of all REST API objects. These objects are
sensitive in nature and should be protected by client authentication. This
requires the API Server to identify itself to the etcd server using
a SSL Certificate Authority file.</rationale>
        <ref cis="1.1.41"/>
        <oval id="api_server_etcd_ca"/>
        <ocil clause="&lt;tt&gt;ca&lt;/tt&gt; is not set as appropriate for &lt;tt&gt;etcdClientInfo&lt;/tt&gt;">Run the following command on the master node(s):
<pre>$ sudo grep -A3 etcdClientInfo /etc/origin/master/master-config.yaml</pre>
The output should contain something similar to:
<pre>etcdClientInfo:
  ca: master.etcd-ca.crt
  certFile: master.etcd-client.crt
  keyFile: master.etcd-client.key</pre></ocil>
      </Rule>
      <Rule id="api_server_advanced_auditing" prodtype="ocp3" severity="medium">
        <title>Enable Advanced Auditing for the API Server</title>
        <description>To ensure advanced auditing is not disabled, edit the API
server pod specification file
<tt>/etc/origin/master/master-config.yaml</tt> and set the
parameters below:
<pre>kubernetesMasterConfig:
  apiServerArguments:
    feature-gates:
    - AdvancedAuditing=true</pre></description>
        <rationale><tt>AdvancedAuditing</tt> enables a much more general API auditing
pipeline, which includes support for pluggable output backends and an
audit policy specifying how different requests should be audited.
Additionally, this enables auditing of failed authentication,
authorization, and login attempts which could prove crucial for
protecting your production clusters. It is recommended not to disable
advanced auditing.</rationale>
        <ref cis="1.1.36"/>
        <oval id="api_server_advanced_auditing"/>
        <ocil clause="&lt;tt&gt;feature-gates&lt;/tt&gt; is set to a value that includes &lt;tt&gt;AdvancedAuditing=false&lt;/tt&gt;">Run the following command on the master node(s):
<pre>$ sudo grep AdvancedAuditing /etc/origin/master/master-config.yaml</pre>
The output should return <pre>AdvancedAuditing=true</pre>.</ocil>
      </Rule>
      <Rule id="api_server_audit_log_maxbackup" prodtype="ocp3" severity="low">
        <title>Configure the Maximum Retained Audit Logs</title>
        <description>To configure how many rotations of audit logs are retained, edit the API Server
pod specification file <tt>/etc/origin/master/master-config.yaml</tt>
on the master node(s) and set the <tt>maximumRetainedFiles</tt> parameter to
<tt>10</tt> or to an organizationally appropriate value:
<pre>auditConfig:
  auditFilePath: "/etc/origin/master/audit-ocp.log"
  enabled: true
  maximumFileRetentionDays: 30
  maximumFileSizeMegabytes: 10
  maximumRetainedFiles: 10</pre></description>
        <rationale>OpenShift automatically rotates the log files. Retaining old log files ensures
OpenShift Operators will have sufficient log data available for carrying out
any investigation or correlation. For example, if the audit log size is set to
100 MB and the number of retained log files is set to 10, OpenShift Operators
would have approximately 1 GB of log data to use during analysis.</rationale>
        <ref cis="1.1.17"/>
        <oval id="api_server_audit_log_maxbackup"/>
        <ocil clause="&lt;tt&gt;maximumRetainedFiles&lt;/tt&gt; is set to &lt;tt&gt;10&lt;/tt&gt; or as appropriate">Run the following command on the master node(s):
<pre>$ sudo grep maximumRetainedFiles /etc/origin/master/master-config.yaml</pre>
The output should return a value of <pre>10</pre> or as appropriate.</ocil>
      </Rule>
      <Rule id="api_server_audit_log_maxsize" prodtype="ocp3" severity="medium">
        <title>Configure Maximum Audit Log Size</title>
        <description>To rotate audit logs upon reaching a maximum size, edit the API Server pod
specification file <tt>/etc/origin/master/master-config.yaml</tt> on
the master node(s) and set the <tt>maximumFileSizeMegabytes</tt> parameter to
an appropriate size in MB. For example, to set it to 100 MB:
<pre>auditConfig:
  auditFilePath: "/etc/origin/master/audit-ocp.log"
  enabled: true
  maximumFileRetentionDays: 30
  maximumFileSizeMegabytes: 100
  maximumRetainedFiles: 10</pre></description>
        <rationale>OpenShift automatically rotates log files. Retaining old log files ensures that
OpenShift Operators have sufficient log data available for carrying out any
investigation or correlation. If you have set file size of 100 MB and the number of
old log files to keep as 10, there would be approximately 1 GB of log data
available for use in analysis.</rationale>
        <ref cis="1.1.18"/>
        <oval id="api_server_audit_log_maxsize"/>
        <ocil clause="&lt;tt&gt;maximumFileSizeMegabytes&lt;/tt&gt; is set to &lt;tt&gt;100&lt;/tt&gt; or as appropriate">Run the following command on the master node(s):
<pre>$ sudo grep maximumFileSizeMegabytes /etc/origin/master/master-config.yaml</pre>
The output should return a value of <pre>100</pre> or as appropriate.</ocil>
      </Rule>
      <Rule id="api_server_admission_control_plugin_NodeRestriction" prodtype="ocp3" severity="medium">
        <title>Enable the NodeRestriction Admission Control Plugin</title>
        <description>To limit the <tt>Node</tt> and <tt>Pod</tt> objects that a kubelet could
modify, follow the OpenShift documentation and configure
<tt>NodeRestriction</tt> plugin on kubelets. Then, edit the API Server pod
specification file <tt>/etc/origin/master/master-config.yaml</tt>
on the master node(s) and set the <tt>admissionConfig</tt> to include <tt>NodeRestriction</tt>:
<pre>admissionConfig:
  pluginConfig:
    NodeRestriction:
      configuration:
      kind: DefaultAdmissionConfig
      apiVersion: v1
      disable: false</pre></description>
        <rationale>Using the <tt>NodeRestriction</tt> plugin ensures that the kubelet is
restricted to the <tt>Node</tt> and <tt>Pod</tt> objects that it could
modify as defined. Such kubelets will only be allowed to modify their
own <tt>Node</tt> API object, and only modify <tt>Pod</tt> API objects
that are bound to their node.</rationale>
        <ref cis="1.1.32"/>
        <oval id="api_server_admission_control_plugin_NodeRestriction"/>
        <ocil clause="&lt;tt&gt;enable-admission-plugins&lt;/tt&gt; does not contain &lt;tt&gt;NodeRestriction&lt;/tt&gt;">Run the following command on the master node(s):
<pre>$ sudo grep -A4 NodeRestriction /etc/origin/master/master-config.yaml</pre>
The output should return <pre>disable: false</pre>.</ocil>
      </Rule>
      <Rule id="api_server_admission_control_plugin_SecurityContextDeny" prodtype="ocp3" severity="medium">
        <title>Enable the SecurityContextDeny Admission Control Plugin</title>
        <description>Instead of using a customized SecurityContext for pods, a Pod Security
Policy (PSP) should be used. PSP is a cluster-level resource that controls
the actions that a pod can perform and what resource the pod may access.
The <tt>SecurityContextDeny</tt> admission control policy enables PSP. To
configure OpenShift to use PSP, edit the API Server pod specification file
<tt>/etc/origin/master/master-config.yaml</tt> on the master node(s) and
set the <tt>admissionConfig</tt> to include <tt>SecurityContextDeny</tt>:
<pre>admissionConfig:
  pluginConfig:
    SecurityContextDeny:
      configuration:
      kind: DefaultAdmissionConfig
      apiVersion: v1
      disable: false</pre></description>
        <rationale>Setting admission control policy to <tt>SecurityContextDeny</tt> denies the
pod level SecurityContext customization. Any attempts to customize the
SecurityContext that are not explicitly defined in the Pod Security Policy
(PSP) are blocked. This ensures that all pods adhere to the PSP defined
by your organization and you have a uniform pod level security posture.</rationale>
        <ref cis="1.1.17"/>
        <oval id="api_server_admission_control_plugin_SecurityContextDeny"/>
        <ocil clause="&lt;tt&gt;admissionConfig&lt;/tt&gt; does not contain &lt;tt&gt;SecurityContextDeny&lt;/tt&gt;">Run the following command on the master node(s):
<pre>$ sudo grep -A4 SecurityContextDeny /etc/origin/master/master-config.yaml</pre>
The output should return <pre>disable: false</pre>.</ocil>
      </Rule>
      <Rule id="api_server_admission_control_plugin_PodSecurityPolicy" prodtype="ocp3" severity="medium">
        <title>Enable the PodSecurityPolicy Admission Control Plugin</title>
        <description>To reject pods that do not match Pod Security Policies, follow the
OpenShift documentation and create Pod Security Policy objects as per your
environment. Then, edit the API Server pod specification file
<tt>/etc/origin/master/master-config.yaml</tt> on the master node(s)
and set the <tt>admissionConfig</tt> to include <tt>PodSecurityPolicy</tt>:
<pre>admissionConfig:
  pluginConfig:
    PodSecurityPolicy:
      configuration:
      kind: DefaultAdmissionConfig
      apiVersion: v1
      disable: false</pre>

Once configured, the API Server service will need to be restarted.</description>
        <rationale>A Pod Security Policy is a cluster-level resource that controls the actions
which a pod can perform and what the pod may access. The
<tt>PodSecurityPolicy</tt> objects define a set of conditions that a pod
must run with in order to be accepted into the system. Pod Security Policies
are comprised of settings and strategies that control the security features
a pod has access to and hence this must be used to control pod access
permissions.</rationale>
        <ref cis="1.1.24"/>
        <oval id="api_server_admission_control_plugin_PodSecurityPolicy"/>
        <ocil clause="&lt;tt&gt;admissionConfig&lt;/tt&gt; does not contain &lt;tt&gt;PodSecurityPolicy&lt;/tt&gt;">Run the following command on the master node(s):
<pre>$ sudo grep -A4 PodSecurityPolicy /etc/origin/master/master-config.yaml</pre>
The output should return <pre>disable: false</pre>.</ocil>
        <warning category="management">When the <tt>PodSecurityPolicy</tt> admission plugin is in use, there
needs to be at least one <tt>PodSecurityPolicy</tt> in place for ANY pods to
be admitted.</warning>
      </Rule>
      <Rule id="api_server_basic_auth" prodtype="ocp3" severity="medium">
        <title>Disable basic-auth-file for the API Server</title>
        <description>Basic Authentication should not be used. Follow the OpenShift documentation
and configure alternate mechanisms for authentication. Then, edit API
server pod specification file <tt>/etc/origin/master/master-config.yaml</tt>
on the master node and remove the <tt>basic-auth-file</tt> parameter.
<pre>kubernetesMasterConfig:
  apiServerArguments:
    basic-auth-file:
    - /path/to/any/file</pre>

Alternate authentication mechanisms such as tokens and certificates will need to be
used. Username and password for basic authentication will be disabled.</description>
        <rationale>Basic authentication uses plaintext credentials for authentication.
Currently the basic authentication credentials last indefinitely, and
the password cannot be changed without restarting the API Server. The
Basic Authentication is currently supported for convenience and is
not intended for production workloads.</rationale>
        <ref cis="1.1.2"/>
        <oval id="api_server_basic_auth"/>
        <ocil clause="basic-auth-file is configured and enabled on the master node">Run the following command on the master node(s):
<pre>$ sudo grep -A2 basic-auth-file /etc/origin/master/master-config.yaml</pre>
The output should return no output.</ocil>
      </Rule>
      <Rule id="api_server_service_account_private_key" prodtype="ocp3" severity="medium">
        <title>Configure the Service Account Private Key for the API Server</title>
        <description>To ensure the API Server utilizes its own key pair, edit the
API Server pod specification file
<tt>/etc/origin/master/master-config.yaml</tt> on the master node(s)
and set the <tt>privateKeyFile</tt> parameter to the public
key file for service accounts:
<pre>serviceAccountConfig:
...
  privateKeyFile: serviceaccounts.private.key
...</pre></description>
        <rationale>By default, if no <tt>privateKeyFile</tt> is specified to the
API Server, the API Server uses the private key from the TLS serving
certificate to verify service account tokens. To ensure that the keys
for service account tokens could be rotated as needed, a separate
public/private key pair should be used for signing service account
tokens.</rationale>
        <ref cis="1.3.4"/>
        <oval id="api_server_service_account_private_key"/>
        <ocil clause="&lt;tt&gt;privateKeyFile&lt;/tt&gt; is not set as appropriate for &lt;tt&gt;serviceAccountConfig&lt;/tt&gt;">Run the following command on the master node(s):
<pre>$ sudo grep -A9 serviceAccountConfig /etc/origin/master/master-config.yaml</pre>
The output should contain a line similar to:
<pre>privateKeyFile: serviceaccounts.private.key</pre></ocil>
      </Rule>
      <Rule id="api_server_kubelet_client_key" prodtype="ocp3" severity="high">
        <title>Configure the kubelet Certificate Key for the API Server</title>
        <description>To enable certificate based kubelet authentication, follow the OpenShift
documentation and setup the TLS connection between the API Server and
kubelets. Then, verify
that <tt>kubeletClientInfo</tt> has the <tt>keyFile</tt> configured in 
the API Server pod specification file
<tt>/etc/origin/master/master-config.yaml</tt> on the master
node(s) to something similar to:
<pre>kubeletClientInfo:
  ca: ca-bundle.crt
  certFile: master.kubelet-client.crt
  keyFile: master.kubelet-client.key</pre></description>
        <rationale>By default the API Server does not authenticate itself to the kubelet's
HTTPS endpoints. Requests from the API Server are treated anonymously.
Configuring certificate-based kubelet authentication ensures that the
API Server authenticates itself to kubelets when submitting requests.</rationale>
        <ref cis="1.1.4,1.1.22"/>
        <oval id="api_server_kubelet_client_key"/>
        <ocil clause="&lt;tt&gt;keyFile&lt;/tt&gt; is not set as appropriate for &lt;tt&gt;kubeletClientInfo&lt;/tt&gt;">Run the following command on the master node(s):
<pre>$ sudo grep -A3 kubeletClientInfo /etc/origin/master/master-config.yaml</pre>
The output should contain something similar to:
<pre>kubeletClientInfo:
  ca: ca-bundle.crt
  certFile: master.kubelet-client.crt
  keyFile: master.kubelet-client.key</pre></ocil>
      </Rule>
      <Rule id="api_server_service_account_public_key" prodtype="ocp3" severity="medium">
        <title>Configure the Service Account Public Key for the API Server</title>
        <description>To ensure the API Server utilizes its own key pair, edit the
API Server pod specification file
<tt>/etc/origin/master/master-config.yaml</tt> on the master node(s)
and set the <tt>publicKeyFiles</tt> parameter to the public
key file for service accounts:
<pre>serviceAccountConfig:
...
  publicKeyFiles:
  - serviceaccounts.public.key
...</pre></description>
        <rationale>By default, if no <tt>privateKeyFile</tt> is specified to the
API Server, the API Server uses the private key from the TLS serving
certificate to verify service account tokens. To ensure that the keys
for service account tokens could be rotated as needed, a separate
public/private key pair should be used for signing service account
tokens.</rationale>
        <ref cis="1.2.25"/>
        <oval id="api_server_service_account_public_key"/>
        <ocil clause="&lt;tt&gt;publicKeyFiles&lt;/tt&gt; is not set as appropriate for &lt;tt&gt;serviceAccountConfig&lt;/tt&gt;">Run the following command on the master node(s):
<pre>$ sudo grep -A9 serviceAccountConfig /etc/origin/master/master-config.yaml</pre>
The output should contain a line similar to:
<pre>publicKeyFiles:
  - serviceaccounts.public.key</pre></ocil>
      </Rule>
      <Rule id="api_server_etcd_key" prodtype="ocp3" severity="medium">
        <title>Configure the etcd Certificate Key for the API Server</title>
        <description>To ensure etcd is configured to make use of TLS encryption for client
communications, follow the OpenShift documentation and setup the TLS
connection between the API Server and etcd. Then, verify
that <tt>etcdClientInfo</tt> has the <tt>keyFile</tt> configured in 
the API Server pod specification file
<tt>/etc/origin/master/master-config.yaml</tt> on the master
node(s) to something similar to:
<pre>etcdClientInfo:
  ca: master.etcd-ca.crt
  certFile: master.etcd-client.crt
  keyFile: master.etcd-client.key</pre></description>
        <rationale>etcd is a highly-available key-value store used by OpenShift deployments
for persistent storage of all REST API objects. These objects are sensitive
in nature and should be protected by client authentication. This requires the
API Server to identify itself to the etcd server using a client certificate
and key.</rationale>
        <ref cis="1.1.26"/>
        <oval id="api_server_etcd_key"/>
        <ocil clause="&lt;tt&gt;keyFile&lt;/tt&gt; does not exist or is not configured to valid certificates">Run the following command on the master node(s):
<pre>$ sudo grep -A3 etcdClientInfo /etc/origin/master/master-config.yaml</pre>
The output should contain something similar to:
<pre>etcdClientInfo:
  ca: master.etcd-ca.crt
  certFile: master.etcd-client.crt
  keyFile: master.etcd-client.key</pre></ocil>
      </Rule>
      <Rule id="api_server_insecure_allow_any_token" prodtype="ocp3" severity="medium">
        <title>Disable Insecure Tokens</title>
        <description>This check is only applicable on OpenShift 3.10 and earlier deployments.
Insecure tokens should be forbidden. Edit the API server pod specification file
<tt>/etc/origin/master/master-config.yaml</tt> on the master node(s) and
remove any instance of the <tt>insecure-allow-any-token</tt> parameter.</description>
        <rationale>Accepting insecure tokens would allow any token without performing
actual authentication. User information is parsed from the token and
connections are allowed.</rationale>
        <ref cis="1.1.3"/>
        <oval id="api_server_insecure_allow_any_token"/>
        <ocil clause="OpenShift 3.10 or earlier is installed and insecure-allow-any-token is configured and enabled on the master node">If OpenShift 3.10 or earlier is installed, run the following command on the master node(s):
<pre>$ sudo grep insecure-allow-any-token /etc/origin/master/master-config.yaml</pre>
The output should return no output.</ocil>
      </Rule>
      <Rule id="api_server_admission_control_plugin_AlwaysPullImages" prodtype="ocp3" severity="high">
        <title>Enable the AlwaysPullImages Admission Control Plugin</title>
        <description>To ensure credentials are required to pull images, edit the API Server pod
specification file
<tt>/etc/origin/master/master-config.yaml</tt> on the master node(s) and
set the <tt>admissionConfig</tt> to include <tt>AlwaysPullImages</tt>:
<pre>admissionConfig:
  pluginConfig:
    AlwaysPullImages:
      configuration:
      kind: DefaultAdmissionConfig
      apiVersion: v1
      disable: false</pre></description>
        <rationale>Setting admission control policy to include <tt>AlwaysPullImages</tt> forces
every new pod to pull the required images during every build. In a multi-tenant
cluster users can be assured that private images can only be used by those who
have the credentials to pull them. Without this admission control policy, once
an image has been pulled to a node, any pod from any user can use it simply by
knowing the image's name (without any authorization check against the image
access control lists). When this plugin is enabled, images are always pulled
prior to starting containers and forces authorization.</rationale>
        <ref cis="1.1.14"/>
        <oval id="api_server_admission_control_plugin_AlwaysPullImages"/>
        <ocil clause="&lt;tt&gt;admissionConfig&lt;/tt&gt; does not contain &lt;tt&gt;AlwaysPullImages&lt;/tt&gt;">Run the following command on the master node(s):
<pre>$ sudo grep -A4 AlwaysPullImages /etc/origin/master/master-config.yaml</pre>
The output should return <pre>disable: false</pre>.</ocil>
      </Rule>
      <Rule id="api_server_kubelet_client_cert" prodtype="ocp3" severity="high">
        <title>Configure the kubelet Certificate File for the API Server</title>
        <description>To enable certificate based kubelet authentication, follow the OpenShift
documentation and setup the TLS connection between the API Server and
kubelets. Then, verify
that <tt>kubeletClientInfo</tt> has the <tt>certFile</tt> configured in 
the API Server pod specification file
<tt>/etc/origin/master/master-config.yaml</tt> on the master
node(s) to something similar to:
<pre>kubeletClientInfo:
  ca: ca-bundle.crt
  certFile: master.kubelet-client.crt
  keyFile: master.kubelet-client.key</pre></description>
        <rationale>By default the API Server does not authenticate itself to the kublet's
HTTPS endpoints. Requests from the API Server are treated anonymously.
Configuring certificate-based kubelet authentication ensures that the
API Server authenticates itself to kubelets when submitting requests.</rationale>
        <ref cis="1.1.4,1.1.32"/>
        <oval id="api_server_kubelet_client_cert"/>
        <ocil clause="&lt;tt&gt;certFile&lt;/tt&gt; is not set as appropriate for &lt;tt&gt;kubeletClientInfo&lt;/tt&gt;">Run the following command on the master node(s):
<pre>$ sudo grep -A3 kubeletClientInfo /etc/origin/master/master-config.yaml</pre>
The output should contain something similar to:
<pre>kubeletClientInfo:
  ca: ca-bundle.crt
  certFile: master.kubelet-client.crt
  keyFile: master.kubelet-client.key</pre></ocil>
      </Rule>
      <Rule id="api_server_request_timeout" prodtype="ocp3" severity="medium">
        <title>Configure the API Server Request Timeout</title>
        <description>All components that use the API should connect via the secured port,
authenticate themselves, and be authorized to use the API. To ensure
such a configuration, edit the API Server pod specification file
<tt>/etc/origin/master/master-config.yaml</tt> on the master
node(s) and set the <tt>request-timeout</tt> to <tt>300</tt>:
<pre>kubernetesMasterConfig:
  apiServerArguments:
    request-timeout:
    - 300</pre></description>
        <rationale>Setting global request timout allows extending the API Server request
timeout limit to a duration appropriate to the user's connection speed.
By default, it is set to 60 seconds which might be problematic on
slower connections making cluster resources inaccessible once the data
volume for requests exceeds what can be transmitted in 60 seconds. But,
setting this timeout limit to be too large can exhaust the API Server
resources making it prone to Denial-of-Service attack. It is recommended
to set this limit as appropriate and change the default limit of 60
seconds only if needed.</rationale>
        <ref cis="1.1.37"/>
        <oval id="api_server_request_timeout"/>
        <ocil clause="&lt;tt&gt;request-timeout&lt;/tt&gt; is not set or is not set to an appropriate value">Run the following command on the master node(s):
<pre>$ sudo grep -A2 request-timeout /etc/origin/master/master-config.yaml</pre>
The output should return <pre>300</pre>.</ocil>
      </Rule>
      <Rule id="api_server_kubelet_https" prodtype="ocp3" severity="medium">
        <title>Enable kubelet HTTPS connections to the API Server</title>
        <description>HTTPS should be used for connections between the API Server and Kubelets.

Edit the API Server pod specification file <tt>/etc/origin/master/master-config.yaml</tt>
on the master node(s) and remove the <tt>kubelet-https</tt> parameter. This will ensure communications
are encrypted using TLS (the default setting).
<pre>kubernetesMasterConfig:
  apiServerArguments:
    kubelet-https:
    - 'false'</pre></description>
        <rationale>Connections from the API Server to Kubelets could potentially carry
sensitive data such as secrets and keys. It is important to use
in-transit encryption for any communication between the API
Server and the Kubelets.</rationale>
        <ref cis="1.1.4"/>
        <oval id="api_server_kubelet_https"/>
        <ocil clause="kubelet-https is specified it must be set to &lt;tt&gt;true&lt;/tt&gt;">Run the following command on the master node(s):
<pre>$ sudo grep kubelet-https /etc/origin/master/master-config.yaml</pre>
The output should return no output.</ocil>
      </Rule>
      <Rule id="api_server_etcd_cert" prodtype="ocp3" severity="medium">
        <title>Configure the etcd Certificate for the API Server</title>
        <description>To ensure etcd is configured to make use of TLS encryption for client
communications, follow the OpenShift documentation and setup the TLS
connection between the API Server and etcd. Then, verify
that <tt>etcdClientInfo</tt> has the <tt>certFile</tt> configured in 
the API Server pod specification file
<tt>/etc/origin/master/master-config.yaml</tt> on the master
node(s) to something similar to:
<pre>etcdClientInfo:
  ca: master.etcd-ca.crt
  certFile: master.etcd-client.crt
  keyFile: master.etcd-client.key</pre></description>
        <rationale>etcd is a highly-available key-value store used by OpenShift deployments
for persistent storage of all REST API objects. These objects are sensitive
in nature and should be protected by client authentication. This requires the
API Server to identify itself to the etcd server using a client certificate
and key.</rationale>
        <ref cis="1.1.26"/>
        <oval id="api_server_etcd_cert"/>
        <ocil clause="&lt;tt&gt;certFile&lt;/tt&gt; does not exist or is not configured to valid certificates">Run the following command on the master node(s):
<pre>$ sudo grep -A3 etcdClientInfo /etc/origin/master/master-config.yaml</pre>
The output should contain something similar to:
<pre>etcdClientInfo:
  ca: master.etcd-ca.crt
  certFile: master.etcd-client.crt
  keyFile: master.etcd-client.key</pre></ocil>
      </Rule>
      <Rule id="api_server_experimental_encryption_provider_config" prodtype="ocp3" severity="medium">
        <title>Configure the Encryption Provider</title>
        <description>To encrypt the etcd key-value store, follow the OpenShift documentation
and configure a <tt>EncryptionConfig</tt> file. Then, edit the API Server
pod specification file
<tt>/etc/origin/master/master-config.yaml</tt> on the master
node(s) and set <tt>experimental-encryption-provider-config</tt>
to the path of that file:
<pre>kubernetesMasterConfig:
  apiServerArguments:
    experimental-encryption-provider-config:
    - /etc/origin/master/encryption-config.yaml</pre></description>
        <rationale>etcd is a highly available key-value store used by OpenShift deployments
for persistent storage of all REST API objects. These objects are
sensitive in nature and should be encrypted at rest to avoid any
disclosures.</rationale>
        <ref cis="1.1.33"/>
        <oval id="api_server_experimental_encryption_provider_config"/>
        <ocil clause="&lt;tt&gt;experimental-encryption-provider-config&lt;/tt&gt; arguments do not point to the &lt;tt&gt;EncryptionConfig&lt;/tt&gt; file">Run the following command on the master node(s):
<pre>$ sudo grep -A2 experimental-encryption-provider-config /etc/origin/master/master-config.yaml</pre>
The output should return <pre>/etc/origin/master/encryption-config.yaml</pre>.</ocil>
      </Rule>
      <Rule id="api_server_insecure_port" prodtype="ocp3" severity="medium">
        <title>Prevent Insecure Port Access</title>
        <description>By default, traffic for the OpenShift API server is served over
HTTPS with authentication and authorization, and the secure API endpoint
is bound to 0.0.0.0:8443. To ensure that the insecure port configuration
has not been enabled, the <tt>insecure-port</tt> setting should not exist
in <tt>/etc/origin/master/master-config.yaml</tt> on the master node(s).</description>
        <rationale>Configuring the API Server on an insecure port would allow unauthenticated
and unencrypted access to your master node(s). It is assumed firewall rules
will be configured to ensure this port is not reachable from outside
the cluster, however as a defense in depth measure, OpenShift should not
be configured to use insecure ports.</rationale>
        <ref cis="1.1.6"/>
        <oval id="api_server_insecure_port"/>
        <ocil clause="&lt;tt&gt;insecure-port&lt;/tt&gt; setting exists">Run the following command on the master node(s):
<pre>$ sudo grep -A2 insecure-port /etc/origin/master/master-config.yaml</pre>
There should be no output returned.</ocil>
      </Rule>
      <Rule id="api_server_admission_control_plugin_NamespaceLifecycle" prodtype="ocp3" severity="medium">
        <title>Enable the NamespaceLifecyle Admission Control Plugin</title>
        <description>To enable <tt>NamespaceLifecycle</tt>, edit the API Server pod specification
file <tt>/etc/origin/master/master-config.yaml</tt> on the master node(s)
and set the <tt>admissionConfig</tt> to include <tt>NamespaceLifecyle</tt>:
<pre>admissionConfig:
  pluginConfig:
    NamespaceLifecyle:
      configuration:
      kind: DefaultAdmissionConfig
      apiVersion: v1
      disable: false</pre></description>
        <rationale>Setting admission control policy to <tt>NamespaceLifecycle</tt> ensures that
objects cannot be created in non-existent namespaces, and that namespaces
undergoing termination are not used for creating new objects. This
is recommended to enforce the integrity of the namespace termination process
and also for the availability of new objects.</rationale>
        <ref cis="1.1.14"/>
        <oval id="api_server_admission_control_plugin_NamespaceLifecycle"/>
        <ocil clause="&lt;tt&gt;admissionConfig&lt;/tt&gt; does not contain &lt;tt&gt;NamespaceLifecycle&lt;/tt&gt;">Run the following command on the master node(s):
<pre>$ sudo grep -A4 NamespaceLifecycle /etc/origin/master/master-config.yaml</pre>
The output should return <pre>disable: false</pre>.</ocil>
      </Rule>
      <Rule id="api_server_service_account_ca" prodtype="ocp3" severity="medium">
        <title>Configure the Service Account Certificate Authority Key for the API Server</title>
        <description>To ensure the API Server utilizes a certificate authority, edit the
API Server pod specification file
<tt>/etc/origin/master/master-config.yaml</tt> on the master node(s)
and set the <tt>masterCA</tt> parameter to the certificate authority
key file for service accounts:
<pre>serviceAccountConfig:
...
  masterCA: ca-bundle.crt
...</pre></description>
        <rationale>Service accounts authenticate to the API using tokens signed by a private RSA
key. The authentication layer verifies the signature using a matching public RSA key.
Configuring the certificate authority file ensures that the API server's signing
certificates are validated.</rationale>
        <ref cis="1.3.5"/>
        <oval id="api_server_service_account_ca"/>
        <ocil clause="&lt;tt&gt;masterCA&lt;/tt&gt; is not set as appropriate for &lt;tt&gt;serviceAccountConfig&lt;/tt&gt;">Run the following command on the master node(s):
<pre>$ sudo grep -A9 serviceAccountConfig /etc/origin/master/master-config.yaml</pre>
The output should contain a line similar to:
<pre>masterCA: ca-bundle.crt</pre></ocil>
      </Rule>
      <Rule id="api_server_experimental_encryption_provider_cipher" prodtype="ocp3" severity="medium">
        <title>Configure the Encryption Provider Cipher</title>
        <description>To configure OpenShift to use the <tt>aescbc</tt> encryption provider,
follow the OpenShift documentation and configure a
<tt>EncryptionConfig</tt> file <tt>/etc/origin/master/encryption-config.yaml</tt>.
In this file, choose <tt>aescbc</tt> as the encryption provider:
<pre>kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
    - secrets
    providers:
    - aescbc:
        keys:
        - name: key1
          secret: <i>32-byte base64-encoded secret</i></pre></description>
        <rationale><tt>aescbc</tt> is currently the strongest encryption provider, it should
be preferred over other providers.</rationale>
        <ref cis="1.1.34"/>
        <oval id="api_server_experimental_encryption_provider_cipher"/>
        <ocil clause="&lt;tt&gt;aescbc&lt;/tt&gt; is not configured as the encryption provider">Run the following command on the master node(s):
<pre>$ sudo grep -A4 aescbc /etc/origin/master/encryption-config.yaml</pre>
Verify that the <tt>aescbc</tt> encryption provider is used for all the desired
<tt>resources</tt>.</ocil>
      </Rule>
      <Rule id="api_server_secure_port" prodtype="ocp3" severity="medium">
        <title>Enable the Secure Port for the API Server</title>
        <description>To ensure traffic is served over HTTPS, edit the API Server pod specification
file <tt>/etc/origin/master/master-config.yaml</tt> on the master node(s)
and either remove the <tt>secure-port</tt>  or set it to a different
(non-zero) desired port.
<pre>kubernetesMasterConfig:
  apiServerArguments:
    secure-port:
    - 8443</pre></description>
        <rationale>The secure port is used to serve HTTPS with authentication and authorization.
If <tt>secure-port</tt> is disabled, as indicated with a value of <tt>0</tt>,
HTTPS traffic will not be served and all traffic will be unencrypted.</rationale>
        <ref cis="1.1.7"/>
        <oval id="api_server_secure_port"/>
        <ocil clause="&lt;tt&gt;secure-port&lt;/tt&gt; is set with a value greater than &lt;tt&gt;0&lt;/tt&gt;">Run the following command on the master node(s):
<pre>$ sudo grep -A2 secure-port /etc/origin/master/master-config.yaml</pre>
The output should not return <pre>0</pre>.</ocil>
      </Rule>
      <Rule id="api_server_insecure_bind_address" prodtype="ocp3" severity="medium">
        <title>Disable Use of the Insecure Bind Address</title>
        <description>OpenShift should not bind to non-loopback insecure addresses. Edit the API
Server pod specification file <tt>/etc/origin/master/master-config.yaml</tt>
on the master node(s) and remove the <tt>insecure-bind-address</tt>
parameter.
<pre>kubernetesMasterConfig:
  apiServerArguments:
    insecure-bind-address:
    - 127.0.0.1</pre></description>
        <rationale>If the API Server is bound to an insecure address the installation would
be susceptible to unauthented and unencrypted access to the master node(s).
The API Server does not perform authentication checking for insecure
binds and the traffic is generally not encrypted.</rationale>
        <ref cis="1.1.5"/>
        <oval id="api_server_insecure_bind_address"/>
        <ocil clause="insecure-bind-address is specified and it must be set to &lt;tt&gt;127.0.01&lt;/tt&gt;">Run the following command on the master node(s):
<pre>$ sudo grep -A2 insecure-bind-address /etc/origin/master/master-config.yaml</pre>
The output should return <pre>127.0.01</pre>.</ocil>
      </Rule>
    </Group>
  </Group>
</Benchmark>
